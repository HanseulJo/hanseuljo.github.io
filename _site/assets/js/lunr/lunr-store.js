var store = [{
        "title": "[논문읽기] Are Transformers universal approximators of sequence-to-sequence functions?",
        "excerpt":"Abstract Keywords &amp; Definitions 1. Sequence-to-sequence Function 2. Permutation Equivariant 3. Universal Approximation 4. Contextual Mapping Main Text 1. Universal Approximator임을 보이기 힘든 이유 2. 논문에서 본 Transformer 2.1. 기존 Transformer 논문과의 공통점 2.2. 기존 Transformer 논문과의 차이점 2.3. Positional encoding 3. 주요 결과 (2가지) 4. 어떻게 증명하나? 5. 몇...","categories": ["paper review"],
        "tags": ["transformer","sequence-to-sequence","approximation","expressive power","deep learning theory"],
        "url": "/posts/are-transformer-universal-approximators-of-sequence-to-sequence-functions/",
        "teaser": null
      },{
        "title": "SGDA with shuffling: faster convergence for nonconvex-PŁ minimax optimization",
        "excerpt":"Abstract Stochastic gradient descent-ascent (SGDA) is one of the main workhorses for solving finite-sum minimax optimization problems. Most practical implementations of SGDA randomly reshuffle components and sequentially use them (i.e., without-replacement sampling); however, there are few theoretical results on this approach for minimax algorithms, especially outside the easier-to-analyze (strongly-)monotone setups....","categories": ["arXiv","KAIA"],
        "tags": ["minimax optimization","SGDA","without-replacement sampling","shuffling-based"],
        "url": "/publication/sgda-with-shuffling",
        "teaser": null
      },]
