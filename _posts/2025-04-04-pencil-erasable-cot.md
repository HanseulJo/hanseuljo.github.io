---
title: '[Paper Reading] PENCIL: Long Thoughts with Short Memory'
excerpt: 'Presented in OptiML Group Meeting (Spring 2025)'
collections: posts
date: 2025-04-04
permalink: /posts/pencil-erasable-cot/
use_math: true
toc:      true
toc_sticky: true
categories:
    - paper review
    - group meeting
tags:
    - transformers
    - pencil
    - chain of thought
    - reasoning
---

<object data="/files/group_meeting/GroupMeeting250404_HanseulCho_PENCIL.pdf" width="960" height="600" type='application/pdf'></object>
For mobile: [**View PDF**](/files/group_meeting/GroupMeeting250404_HanseulCho_PENCIL.pdf)

## Main References

* Chenxiao Yang, Nathan Srebro, David McAllester, and Zhiyuan Li. [PENCIL: Long Thoughts with Short Memory.](https://arxiv.org/abs/2503.14337) arXiv preprint, 2025.

## Supplementary References:
* Maxwell Nye et al. [Show Your Work: Scratchpads for Intermediate Computation with Language Models.](https://arxiv.org/abs/2112.00114) arXiv preprint, 2021.
* Jason Wei et al. [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.](https://arxiv.org/pdf/2201.11903) NeurIPS 2022.
* William Merrill and Ashish Sabharwal, [The Parallelism Tradeoff: Limitations of Log-Precision Transformers.](https://arxiv.org/abs/2207.00729) TACL 2023.
* William Merrill and Ashish Sabharwal. [The Expressive Power of Transformers with Chain of Thought.](https://arxiv.org/pdf/2310.07923) ICLR 2024.
