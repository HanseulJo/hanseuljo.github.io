---
permalink: /
title: "Hanseul Cho (조한슬) 👋"
excerpt: "About me"
author_profile: true
toc: true
toc_sticky: true
redirect_from: 
  - /about/
  - /about.html
---

<style>
gray { color: gray; font-size: 75%;}
.nobull {
  margin:0px; padding:0px;
  list-style: none;
  padding-left: 2rem;
  text-indent: -1.6rem;
}
.nobull2 {
  line-height:1em;
  padding-left: 1rem;
  text-indent: 0rem;
}
</style>

I am a Ph.D. student in the [Optimization & Machine Learning (OptiML) Laboratory](https://chulheeyun.github.io), where I am fortunate to be advised by [Prof. Chulhee Yun](https://chulheeyun.github.io) at [Kim Jaechul Graduate School of AI](https://gsai.kaist.ac.kr) in [Korea Advanced Institute of Science and Technology](https://www.kaist.ac.kr/en/) (KAIST AI).
Previously, I completed my M.Sc. (in AI) and B.Sc. (in Math, minor in CS, Summa Cum Laude) at KAIST.

<!-- <p style="text-align:center;font-weight:bold">
🚨<span style="color:ForestGreen">I am Looking for Internship Opportunities‼️</span>🚨
</p> -->

I am interested in a broad range of fields in **optimization, machine learning (ML), and deep learning (DL)**, especially focusing on both:

* **Mathematical/theoretical analysis**, and
* **Empirical improvements (usually based on theoretical understanding)**.  

During my journey to Ph.D., I aim to **rigorously understand and practically overcome** the following **three critical obstacles** in ML/DL (see my [Thesis Proposal](/files/PhD_thesis_research_proposal.pdf) for details):

* <span style="color:FireBrick">**[Generalizability]**</span>  **Failures in out-of-distibution generalization of (large) language models** (e.g., length generalization and compositional generalization of Transformer architectures)
* <span style="color:FireBrick">**[Adaptability]**</span>  **Difficulties in training adaptable models under an evolving environment** (e.g., continual learning, maintaining plasticity of neural networks, reinforcement learning)
* <span style="color:FireBrick">**[Multifacetedness]**</span> **Learning with multiple (possibly conflicting & orthogonal) goals** (e.g., minimax optimization, bi‑level optimization, fairness in ML)

## 📰 Publications 📰

Please click the ["Publications"](/publications/) tab above to look up the full list of my publications.  
You can also find my articles on my <a href="{{ site.author.googlescholar }}">Google Scholar</a> profile.

## ‼️News‼️

<ul class="nobull">
  <li>🗞️ [Nov. '24] Our paper on theoretical analysis of continual learning is accepted to <a href="http://aiassociation.kr">JKAIA 2024</a> and won the <b>Best Paper Award</b>! 🎉 (See Publications for more details)</li>
  <li>🗞️ [Nov. '24] I'm selected as one of the <a href="https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers">Top Reviewers (top 8.6%: 1,304 of 15,160 reviewers)</a> at NeurIPS 2024! (+ Free registration! 😎) </li>
  <li>🗞️ [Sep. '24] Two papers got accepted to <b>NeurIPS 2024</b>! 🎉 <a href="https://arxiv.org/abs/2405.20671">One</a> is about length generalization of arithmetic Transfomers, and another is about mitigating loss of plasticity in incremental neural net training. See you in Vancouver🇨🇦!</li>
  <li>🗞️ [Jun. '24] An early version of our <a href="https://arxiv.org/abs/2405.20671">paper</a> on length generalization of Transformers got accepted to the ICML 2024 Workshop on <a href="https://longcontextfm.github.io/">Long-Context Foundation Models</a>!</li>
  <li>🗞️ [May. '24] A <a href="https://openreview.net/forum?id=s6ZAT8MLKU">paper</a> got accepted to <b>ICML 2024</b> as a <b>spotlight paper</b> (top 3.5% among all submissions)! 🎉 We show global convergence of Alt-GDA (which is <i>strictly</i> faster than Sim-GDA) and propose an enhanced algorithm called Alex-GDA for minimax optimization. See you in Vienna🇦🇹!</li>
  <li>🗞️ [Sep. '23] Two papers are accepted to <b>NeurIPS 2023</b>! 🎉 One is about <a href="https://arxiv.org/abs/2310.18593">Fair Streaming PCA</a> and another is about <a href="https://arxiv.org/abs/2306.10711">enhancing plasticity in RL</a>.</li>
  <li>🗞️ [Jan. '23] Our <a href="https://openreview.net/forum?id=6xXtM8bFFJ">paper</a> about shuffling-based stochastic gradient descent-ascent got accepted to <b>ICLR 2023</b>! </li>
  <li>🗞️ [Nov. '22] Our <a href="https://arxiv.org/abs/2210.05995">paper</a> about shuffling-based stochastic gradient descent-ascent is accepted to 2022 <a href="http://aiassociation.kr">Korea AI Association</a> + <a href="https://www.navercorp.com/en">NAVER</a> Autumnal Joint Conference (JKAIA 2022) and selected as the <b>NAVER Outstanding Theory Paper</b>! </li>
  <li>🗞️ [Oct. '22] I am happy to announce that our very first <a href="https://arxiv.org/abs/2210.05995">preprint</a> is now on arXiv!  It is about convergence analysis of shuffling-based stochastic gradient descent-ascent. </li>
  <li>🗞️ [Feb. '22] Now I am part of <a href="https://chulheeyun.github.io">OptiML Lab</a> of KAIST AI. </li>
</ul>

## Education

<ul class="nobull">
  <li>🏫 Ph.D. in Artificial Intelligence  <gray>KAIST, Sept. 2023 – Current</gray></li>
  <li>🏫 M.Sc. in Artificial Intelligence  <gray>KAIST, Mar. 2022 – Aug. 2023</gray></li>
  <li>🏫 B.Sc. in Mathematical Sciences  <gray>KAIST, Mar. 2017 – Feb. 2022</gray></li>
    <ul class="nobull2" style="color:gray">
    <li>Minor in Computing Sciences / <b>Summa Cum Laude</b></li>
    </ul>
</ul>

## Contact & Info

📋 **Curriculum Vitae (CV)**: [Here](/files/Curriculum_Vitae__Hanseul_Cho.pdf)  
📧 Email: jhs4015 at kaist dot ac dot kr
