---
layout: post
title: '[Paper Follow-up] ICML 2022'
categories:
  - papers
tags:
comments: true
use_math: true
---

ICML 2022에 accept된 논문 가운데 "관심있는($\ne$ 잘 아는)" 주제의 논문들을 추려보았습니다. 지극히 개인적인 리스트이며, 정확한 분류가 아닐 수 있습니다. (논문 제목, 1저자.)

## Optimization (Minimax, Federated Learning, Convex/Nonconvex, etc.)

- [On Convergence of Gradient Descent Ascent: A Tight Local Analysis](https://proceedings.mlr.press/v162/li22e/li22e.pdf), Haochuan Li
- [Federated Minimax Optimization: Improved Convergence Analyses and Algorithms](https://proceedings.mlr.press/v162/sharma22c/sharma22c.pdf), Pranay sharma
- [FedNest: Federated Bilevel, Minimax, and Compositional Optimization](https://proceedings.mlr.press/v162/tarzanagh22a/tarzanagh22a.pdf), Davoud Ataee Tarzanagh
- [Proximal and Federated Random Reshuffling](https://proceedings.mlr.press/v162/mishchenko22a/mishchenko22a.pdf), Konstantin Mishchenko
- [Exact Optimal Accelerated Complexity for Fixed-Point Iterations](https://proceedings.mlr.press/v162/park22c/park22c.pdf), Jisun Park
- [Continuous-Time Analysis of Accelerated Gradient Methods via Conservation Laws in Dilated Coordinate Systems](https://proceedings.mlr.press/v162/suh22a/suh22a.pdf), Jaewook Suh
- [AdaGrad Avoids Saddle Points](https://proceedings.mlr.press/v162/antonakopoulos22a/antonakopoulos22a.pdf), Kimon Antonakopoulos
- [Convergence Rates of Non-Convex Stochastic Gradient Descent Under a Generic Lojasiewicz Condition and Local Smoothness](https://proceedings.mlr.press/v162/scaman22a/scaman22a.pdf), Kevin Scaman
- [Adaptive Inertia: Disentangling the Effects of Adaptive Learning Rate and Momentum](https://proceedings.mlr.press/v162/xie22d/xie22d.pdf), Zeke Xie
- [Last Iterate Risk Bounds of SGD with Decaying Stepsize for Overparameterized Linear Regression](https://proceedings.mlr.press/v162/wu22p/wu22p.pdf), Jingfeng Wu

## Unstable convergence / Edge of Stability

- [Understanding Gradient Descent on the Edge of Stability in Deep Learning](https://proceedings.mlr.press/v162/arora22a/arora22a.pdf), Sanjeev Arora
- [Understanding the unstable convergence of gradient descent](https://proceedings.mlr.press/v162/ahn22a/ahn22a.pdf), Kwangjun Ahn
- [Neural Network Weights Do Not Converge to Stationary Points: An Invariant Measure Perspective](https://proceedings.mlr.press/v162/zhang22q/zhang22q.pdf), Jingzhao Zhang

## Contrastive Learning

- [Robustness Verification for Contrastive Learning](https://proceedings.mlr.press/v162/wang22q/wang22q.pdf), Zekai Wang
- [Understanding Contrastive Learning Requires Incorporating Inductive Biases](https://proceedings.mlr.press/v162/saunshi22a/saunshi22a.pdf), Nikunj Umesh Saunshi
- [Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation](https://proceedings.mlr.press/v162/shen22d/shen22d.pdf), Kendrick Shen
- [Do More Negative Samples Necessarily Hurt In Contrastive Learning?](https://proceedings.mlr.press/v162/awasthi22b/awasthi22b.pdf), Pranjal Awasthi

## Robustness / Perturbation / Generalization

- [Robust Training of Neural Networks Using Scale Invariant Architectures](https://proceedings.mlr.press/v162/li22b/li22b.pdf), Zhiyuan Li
- [Anticorrelated Noise Injection for Improved Generalization](https://proceedings.mlr.press/v162/orvieto22a/orvieto22a.pdf), Antonio Orvieto
- [Robustness Implies Generalization via Data-Dependent Generalization Bounds](https://proceedings.mlr.press/v162/kawaguchi22a/kawaguchi22a.pdf), Kenji Kawaguchi
- [Not All Poisons are Created Equal: Robust Training against Data Poisoning](https://proceedings.mlr.press/v162/yang22j/yang22j.pdf), Yu Yang
- [To Smooth or Not? When Label Smoothing Meets Noisy Labels](https://proceedings.mlr.press/v162/wei22b/wei22b.pdf), Jiaheng Wei

## Other Topics(causality, fairness, explainability, differential privacy, approximation, heavy-tailed noise, out-of-distribution detection, etc.)

- Please refer to [this link](https://han-5eu1.notion.site/bdb96ba5c59a4489981d745f01153795?v=d0e4883eb95d4e9382628a9dd5dc4e7c).
