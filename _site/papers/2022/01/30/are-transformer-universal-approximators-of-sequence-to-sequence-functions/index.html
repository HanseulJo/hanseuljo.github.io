<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v6.4.0 (https://qwtel.com/hydejack/)
-->









<head>
  <!-- =============== -->
<!-- META            -->
<!-- =============== -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="format-detection" content="telephone=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="naver-site-verification" content="22c5b4ef3bfda7fc6100671413989219de7a4ac8"/>
<meta property="og:title" content="[논문리뷰] Are Transformers universal approximators of sequence-to-sequence functions?">
<meta property="og:type" content="article">





  <meta property="og:image" content="http://localhost:4000/assets/img/logo.png">


<meta property="og:image:width" content="640" />
<meta property="og:image:height" content="360" />



  <title>[논문리뷰] Are Transformers universal approximators of sequence-to-sequence functions? &middot; Hanseul's Blog</title>



<meta name="description" content="📌 한 줄 요약: Transformer의 expressive power를 이론적으로 보인 첫번째 논문

">
<meta property="og:description" content="📌 한 줄 요약: Transformer의 expressive power를 이론적으로 보인 첫번째 논문

">



<!-- =============== -->
<!-- LINKS           -->
<!-- =============== -->
<link rel="canonical" href="http://localhost:4000/papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions/">
<meta property="og:url" content="http://localhost:4000/papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions/">

<link rel="alternate" type="application/atom+xml" title="Hanseul's Blog Feed" href="http://localhost:4000/feed.xml">


  <link rel="prev" href="http://localhost:4000/papers/2022/01/29/paper-list/">




<link rel="apple-touch-icon" href="http://localhost:4000/apple-touch-icon.png">
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
<!-- Place favicon.ico in the root directory -->

<!-- =============== -->
<!-- SCRIPTS         -->
<!-- =============== -->
<script>
  !function(n,e){function t(n,e){n.onload=function(){this.onerror=this.onload=null,e(null,n)},n.onerror=function(){this.onerror=this.onload=null,e(new Error("Failed to load "+this.src),n)}}function o(n,e){n.onreadystatechange=function(){"complete"!=this.readyState&&"loaded"!=this.readyState||(this.onreadystatechange=null,e(null,n))}}n._loaded=!1,n.loadJSDeferred=function(a,d){function r(){n._loaded=!0;var r=e.createElement("script");r.src=a,d&&(("onload"in r?t:o)(r,d),r.onload||t(r,d));var l=e.getElementsByTagName("script")[0];l.parentNode.insertBefore(r,l)}n._loaded?r():n.addEventListener?n.addEventListener("load",r,!1):n.attachEvent?n.attachEvent("onload",r):n.onload=r}}(window,document);

  !function(e){"use strict";var n=function(n,t,o){function i(e){if(a.body)return e();setTimeout(function(){i(e)})}function r(){l.addEventListener&&l.removeEventListener("load",r),l.media=o||"all"}var d,a=e.document,l=a.createElement("link");if(t)d=t;else{var f=(a.body||a.getElementsByTagName("head")[0]).childNodes;d=f[f.length-1]}var s=a.styleSheets;l.rel="stylesheet",l.href=n,l.media="only x",i(function(){d.parentNode.insertBefore(l,t?d:d.nextSibling)});var u=function(e){for(var n=l.href,t=s.length;t--;)if(s[t].href===n)return e();setTimeout(function(){u(e)})};return l.addEventListener&&l.addEventListener("load",r),l.onloadcssdefined=u,u(r),l};"undefined"!=typeof exports?exports.loadCSS=n:e.loadCSS=n}("undefined"!=typeof global?global:this);

  !function(t){if(t.loadCSS){var e=loadCSS.relpreload={};if(e.support=function(){try{return t.document.createElement("link").relList.supports("preload")}catch(t){return!1}},e.poly=function(){for(var e=t.document.getElementsByTagName("link"),r=0;r<e.length;r++){var n=e[r];"preload"===n.rel&&"style"===n.getAttribute("as")&&(t.loadCSS(n.href,n,n.getAttribute("media")),n.rel=null)}},!e.support()){e.poly();var r=t.setInterval(e.poly,300);t.addEventListener&&t.addEventListener("load",function(){e.poly(),t.clearInterval(r)}),t.attachEvent&&t.attachEvent("onload",function(){t.clearInterval(r)})}}}(this);

  window.disablePushState = false;
  window.disableDrawer = false;
</script>

<!--[if lt IE 9]>
<script src="https://unpkg.com/html5shiv/dist/html5shiv.min.js"></script>
<![endif]-->

<!-- =============== -->
<!-- STYLES          -->
<!-- =============== -->
<!--[if gt IE 8]><!---->
<style>
  
  article,aside,dialog,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}mark{background:#FF0;color:#000}*{box-sizing:border-box}html,body{margin:0;padding:0}html{font-size:16px;line-height:1.75}body{color:#333;background-color:#fff;overflow-y:scroll}a{text-decoration:none}.lead{margin-left:-1rem;margin-right:-1rem}img,.img{display:block;max-width:100%;margin-bottom:1rem;border:none}img.lead,.img.lead{max-width:calc(100% + 2rem);width:calc(100% + 2rem)}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6,.heading{font-weight:bold;text-rendering:optimizeLegibility}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6{margin:1.6rem 0 1rem;line-height:1.6}h1,.h1{font-size:2rem;line-height:1.25}h2,.h2{font-size:1.5rem}h3,.h3{font-size:1.17em}p{margin-top:0;margin-bottom:1rem}p.lead{font-size:1.25rem;font-weight:300;padding:0 1rem}ul,ol,dl{margin-top:0;margin-bottom:1rem}ul,ol{padding-left:1.25rem}hr{position:relative;margin:1.5rem 0;border:0;border-top:1px solid #eee}.hr{padding-bottom:.5rem;border-bottom:1px solid #eee;margin-bottom:1.5rem}h4,h5,h6,.h4,.h5,.h6{margin-bottom:0.5rem;font-size:1rem}table{margin-bottom:1rem;width:100%;width:calc(100% + 2rem);margin-left:-1rem;border:1px solid #e5e5e5;border-collapse:collapse;border-spacing:0}td,th{padding:.25rem .5rem;border:1px solid #e5e5e5}td:first-child,th:first-child{padding-left:1rem}td:last-child,th:last-child{padding-right:1rem}thead+tbody,tbody+tbody,tfoot{border-top:3px double #e5e5e5}tbody tr:nth-child(odd) td,tbody tr:nth-child(odd) th{background-color:#f9f9f9}footer{margin-bottom:2rem}.page,.post{margin-bottom:3em}.page li+li,.post li+li{margin-top:.25rem}.page>header,.post>header{margin-bottom:2rem}.page-title,.post-title{margin-top:0}.post-date{display:block;margin-top:-0.5rem;margin-bottom:1rem;color:#9a9a9a}.related-posts{padding-left:0;list-style:none}.related-posts>li,.related-posts>li+li{margin-top:1rem}.related-posts>li>small,.related-posts>li+li>small{font-size:75%;color:#9a9a9a}.message{margin-bottom:1rem;padding:1rem;color:#787878;background-color:#f9f9f9;margin-left:-1rem;margin-right:-1rem}body,main{position:relative;overflow-x:hidden}@media screen{body::before{content:'';background:#e5e5e5;position:absolute;left:0;top:0;bottom:0}}@media screen and (min-width: 40em){html{font-size:17px}}@media screen and (min-width: 54em){html{font-size:16px}}@media screen and (min-width: 88em){html{font-size:17px}}@media screen and (min-width: 125em){html{font-size:18px}}.sr-only{display:none}.clearfix,.sidebar-social::after,.clearafter::after{content:"";display:table;clear:both}a,.a{position:relative;padding-bottom:.15rem;border-style:hidden}.img{overflow:hidden;background-color:#f9f9f9}.img>img{margin:0;width:100%;height:100%}.sixteen-nine{position:relative}.sixteen-nine::before{display:block;content:"";width:100%;padding-top:56.25%}.sixteen-nine>*{position:absolute;top:0;left:0;right:0;bottom:0}h1+hr,h2+hr,h3+hr,h4+hr,h5+hr,h6+hr{margin-top:0}.fade-in{animation-duration:500ms;animation-name:fade-in;animation-fill-mode:forwards}@keyframes fade-in{from{transform:translateY(-2rem);opacity:0}50%{transform:translateY(-2rem);opacity:0}to{transform:translateY(0);opacity:1}}.mb6{margin-bottom:10rem}.sidebar{color:rgba(255,255,255,0.75);text-align:left}.sidebar::before{content:"";position:absolute;z-index:2;top:0;left:0;bottom:0;right:0;background:linear-gradient(to bottom, rgba(32,32,32,0) 0%, rgba(32,32,32,0.5) 100%)}.sidebar a{color:#fff;border-bottom-color:rgba(255,255,255,0.2)}.right-side{width:100%;margin-left:auto;margin-right:auto}.right-side .ad-first{text-align:center}@media screen{.right-side{max-width:38rem;min-height:100vh}.right-side .ad-second{display:none}}@media screen and (min-width: 54em){.right-side{margin-left:20rem;margin-right:1rem;padding:4rem 1rem 12rem}.right-side .ad-second{text-align:center;display:block}}@media screen and (min-width: 72em){.right-side{margin-left:22rem;max-width:42rem}}@media screen and (min-width: 88em){.right-side{width:162px;margin-left:0rem;margin-right:0rem;padding:0rem;margin-top:10rem;display:block;float:left}}@media screen and (min-width: 96em){.right-side{width:300px;margin-right:0rem}}#_yDrawer{position:relative}@media screen{#_yDrawer{min-height:640px;min-height:100vh}}@media screen and (min-width: 54em){#_yDrawer{width:18rem;margin-left:0}}.sidebar-bg{position:absolute;height:100%;overflow:hidden;top:0;right:0;bottom:0;left:0;background:#202020 center / cover}.sidebar-box{display:flex;justify-content:center}.sidebar-sticky{position:relative;z-index:3}@media screen{.sidebar-sticky{-ms-overflow-style:none;overflow:-moz-scrollbars-none;height:100%;overflow:auto;position:absolute;padding:3rem 0rem;right:2.5rem;left:2.5rem}}.sidebar-sticky::-webkit-scrollbar{display:none}.sidebar-about>h1{color:#fff;font-size:2rem}.sidebar-nav>ul{list-style:none;padding-left:0;margin-bottom:.5rem}a.sidebar-nav-item{width:100%;font-weight:normal;display:block;line-height:1.75;padding:.25rem 0;border-bottom:1px solid rgba(255,255,255,0.2)}a.sidebar-nav-subitem{font-weight:normal;display:block;line-height:1.75;padding:.25rem 0;border-bottom:1px solid rgba(255,255,255,0.2)}@media screen{.y-drawer-scrim{z-index:2}.y-drawer-content{width:18rem;left:-18rem;z-index:3}}.sidebar-social{margin-bottom:.5rem}.sidebar-social>ul{list-style:none;padding-left:0;margin:0 -.25rem}.sidebar-social>ul>li{float:left}.sidebar-social>ul>li>a{display:inline-block;text-align:center;font-size:1.6rem;line-height:3rem;width:3.1249rem;height:4rem;padding:.5rem 0}.sidebar-social>ul li+li{margin-top:0}.fixed-top{position:fixed;top:0;left:0;width:100%;z-index:1}.navbar>.content{padding-top:0;padding-bottom:0;min-height:0;height:0}.menu{display:inline-block;padding:1.75rem 1.5rem;border-bottom:none;margin-left:-1.5rem;color:#9a9a9a !important}.menu::after{content:"\2630"}@media screen and (min-width: 54em){.menu{padding:1.25rem 1.5rem;position:absolute;left:-9999px}.menu:focus{position:static}}.animation-main{pointer-events:none}.loading{display:none}@media print{.menu{display:none}}.animation-main{opacity:0;will-change:opacity}.loading{position:absolute;top:0;right:0;padding:5.25rem 4.5rem;transform-origin:top right;transform:scale(0.33)}.content{position:relative;margin-left:auto;margin-right:auto;padding:5rem 1rem 12rem}@media screen{.content{min-height:100vh}}@media screen and (min-width: 54em){.content{padding:4rem 1rem 12rem;margin-left:19rem;margin-right:3rem}}@media screen and (min-width: 72em){.content{max-width:42rem;margin-left:21rem}}@media screen and (min-width: 88em){.content{float:left;width:100%;margin-left:22rem;margin-right:5rem}}@media screen and (min-width: 96em){.content{max-width:44rem}}@media screen and (min-width: 102em){.content{margin-left:25rem;margin-right:8rem}}.me{width:6.5rem;height:6.5rem;align-self:center;margin-right:20px;border-radius:100%;position:relative}@media screen and (min-width: 40em){.me{width:7rem;height:7rem}}@media screen and (min-width: 54em){.me{width:6.5rem;height:6.5rem}}@media screen and (min-width: 72em){.me{width:7rem;height:7rem}}main>footer{width:100%;position:absolute;bottom:0;left:0;right:0;padding:0 1rem;color:#9a9a9a;font-size:smaller;text-align:center}main>footer>p{margin-bottom:0}html{font-family:'Sans-serif'}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6,.heading{font-family:'Sans-serif'}

</style>


<link rel="preload" href="http://localhost:4000/assets/css/hydejack.css?v=6.4.0" as="style" onload="this.rel='stylesheet'">

<style id="_pageStyle">

.content a{color:#148bba;border-color:rgba(20,139,186,0.2)}.content a:hover{border-color:#148bba}:focus{outline-color:#148bba}::selection{color:#fff;background:#148bba}::-moz-selection{color:#fff;background:#148bba}

</style>


<noscript>
  <link rel="stylesheet" href="http://localhost:4000/assets/css/hydejack.css?v=6.4.0">
  
  
  

  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato">
    <style>
      html { font-family: 'Lato', 'Sans-serif' }
      h1, h2, h3, h4, h5, h6, .h1, .h2, .h3, .h4, .h5, .h6, .heading { font-family: 'Lato', 'Sans-serif' }
    </style>
  

  
  <link rel="stylesheet" href="http://localhost:4000/assets/icomoon/style.css">
</noscript>
<!--<![endif]-->

</head>

<body>
  <!-- =============== -->
<!-- MENU            -->
<!-- =============== -->
<div class="navbar fixed-top">
  <div class="content">
    <span class="sr-only">Jump to:</span>
    <a id="_menu" class="menu no-hover" href="#_title">
      <span class="sr-only">Menu</span>
    </a>
  </div>
</div>

<!-- =============== -->
<!-- CONTENT         -->
<!-- =============== -->
<div id="_yPushState">
  <div class="fade-in">
    <main id="_main" class="content" role="main" data-color="#148bba" data-image="/assets/img/side.jpeg">
      

<article id="post-papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions" class="post" role="article">
  <header>
    <h1 class="post-title">
      
        [논문리뷰] Are Transformers universal approximators of sequence-to-sequence functions?
        
    </h1>

    <p class="post-date heading">
      <time datetime="2022-01-30T00:00:00+09:00">30 Jan 2022</time>
      









in <a href="/category/papers/" data-flip="title">Papers</a>

      









on <a href="/tag/papers-theory/" data-flip="title">Theory</a>

    </p>

    
  <div class="hr" style="padding-bottom:0"></div>


  </header>
  

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  
  <div class="markdown-body">

    <!--  -->
    <style>
      .myAd1190 {
        width: 48%;
        height: 280px;
      }

      .myAd1290 {
        width: 48%;
        height: 280px;
      }

      @media(max-width: 800px) {
        .myAd1190 {
          display: none;
        }

        .myAd1290 {
          width: 98%;
        }
      }

      .row {
        display: flex;
        justify-content: space-between;
        align-items: center;
        height: 300px;
      }

      .row-center {
        display: flex;
        justify-content: center;
        height: 258px;
      }
    </style>
    <div class='row'>
      <ins class="adsbygoogle myAd1190" data-ad-client="ca-pub-9134477021095729" data-ad-slot="6559875097"></ins>
      <ins class="adsbygoogle myAd1290" data-ad-client="ca-pub-9134477021095729" data-ad-slot="6559875097"></ins>
    </div>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
    </script>

    <br />
    <p>📌 한 줄 요약: <strong>Transformer의 expressive power를 이론적으로 보인 첫번째 논문</strong></p>

<p>🤔 참고: 노션으로 작성한 <a href="https://han-5eu1.notion.site/Are-Transformers-universal-approximators-of-sequence-to-sequence-functions-158eac79332a4d81b1b7cccff9b1b0ce">원문</a>을 옮겨온 것입니다.</p>

<ul id="markdown-toc">
  <li><a href="#abstract" id="markdown-toc-abstract">Abstract</a></li>
  <li><a href="#keywords--definitions" id="markdown-toc-keywords--definitions">Keywords &amp; Definitions</a>    <ul>
      <li><a href="#1-sequence-to-sequence-function" id="markdown-toc-1-sequence-to-sequence-function">1. Sequence-to-sequence Function</a></li>
      <li><a href="#2-permutation-equivariant" id="markdown-toc-2-permutation-equivariant">2. Permutation Equivariant</a></li>
      <li><a href="#3-universal-approximation" id="markdown-toc-3-universal-approximation">3. Universal Approximation</a></li>
      <li><a href="#4-contextual-mapping" id="markdown-toc-4-contextual-mapping">4. Contextual Mapping</a></li>
    </ul>
  </li>
  <li><a href="#main-text" id="markdown-toc-main-text">Main Text</a>    <ul>
      <li><a href="#1-universal-approximator임을-보이기-힘든-이유" id="markdown-toc-1-universal-approximator임을-보이기-힘든-이유">1. Universal Approximator임을 보이기 힘든 이유</a></li>
      <li><a href="#2-논문에서-본-transformer" id="markdown-toc-2-논문에서-본-transformer">2. 논문에서 본 Transformer</a>        <ul>
          <li><a href="#21-기존-transformer-논문과의-공통점" id="markdown-toc-21-기존-transformer-논문과의-공통점">2.1. 기존 Transformer 논문과의 공통점</a></li>
          <li><a href="#22-기존-transformer-논문과의-차이점" id="markdown-toc-22-기존-transformer-논문과의-차이점">2.2. 기존 Transformer 논문과의 차이점</a></li>
          <li><a href="#23-positional-encoding" id="markdown-toc-23-positional-encoding">2.3. Positional encoding</a></li>
        </ul>
      </li>
      <li><a href="#3-주요-결과-2가지" id="markdown-toc-3-주요-결과-2가지">3. 주요 결과 (2가지)</a>        <ul>
          <li><a href="#31-theorem-2" id="markdown-toc-31-theorem-2">3.1. Theorem 2</a></li>
          <li><a href="#32-theorem-3" id="markdown-toc-32-theorem-3">3.2. Theorem 3</a></li>
        </ul>
      </li>
      <li><a href="#4-어떻게-증명하나" id="markdown-toc-4-어떻게-증명하나">4. 어떻게 증명하나?</a>        <ul>
          <li><a href="#1-f를-piece-wise-상수함수로-근사하기" id="markdown-toc-1-f를-piece-wise-상수함수로-근사하기">1) $f$를 piece-wise 상수함수로 근사하기</a></li>
          <li><a href="#2-piece-wise-상수함수를-modified-transformer-network로-근사하기" id="markdown-toc-2-piece-wise-상수함수를-modified-transformer-network로-근사하기">2) Piece-wise 상수함수를 ‘modified’ Transformer network로 근사하기</a></li>
          <li><a href="#3-modified-transformer-network를-transformer-network로-근사하기" id="markdown-toc-3-modified-transformer-network를-transformer-network로-근사하기">3) Modified Transformer network를 Transformer network로 근사하기</a></li>
        </ul>
      </li>
      <li><a href="#5-몇-개의-block을-쌓아야-하나" id="markdown-toc-5-몇-개의-block을-쌓아야-하나">5. 몇 개의 block을 쌓아야 하나?</a></li>
    </ul>
  </li>
  <li><a href="#my-comments--questions" id="markdown-toc-my-comments--questions">My Comments &amp; Questions</a></li>
</ul>

<h1 id="abstract">Abstract</h1>

<ul>
  <li><strong>Transformer encoder</strong>는 <strong>‘permutation equivariant</strong>’한 성질을 갖는 <strong>연속</strong>인 ‘<strong>sequence-to-sequence</strong>’ 함수(with compact support)에 대한 universal approximator임을 보인다.</li>
  <li>Transformer encoder에다 <strong>learnable한 positional encodings를 같이 쓰면</strong> <strong>임의의</strong>(permutation equivariant하지 않아도) <strong>연속</strong>인 ‘<strong>sequence-to-sequence</strong>’ 함수(with compact domain)를 universally approximate함을 보인다.</li>
  <li>Contextual mapping이라는 것을 수식적으로 정의했으며, Transformer Encoder의 <strong>multi-head</strong> <strong>self-attention layer들이 입력 sequence에 대한 contextual mapping을 잘 계산함</strong>을 보인다.</li>
  <li>(실험도 진행하였으나 여기서는 생략)</li>
</ul>

<hr />
<hr />

<h1 id="keywords--definitions">Keywords &amp; Definitions</h1>

<h2 id="1-sequence-to-sequence-function">1. Sequence-to-sequence Function</h2>

<p>$\mathbb{R}^{d\times n}$에서 $\mathbb{R}^{d\times n}$로 가는 함수를 <strong>sequence-to-sequence</strong> function이라고 말한다. 정확히는 정의역도 치역도 모두 subset of $\mathbb{R}^{d\times n}$인 함수를 말한다. ($\mathbb{R}^{d\times n}$: the set of all $d\times n$  real matrices)</p>

<p>이때 $d$와 $n$은 각각, <a href="https://arxiv.org/abs/1706.03762">Transformer 논문</a>에서 언급하는 embedding 차원과 입력 sequence 길이로 비유된다. 기존 Transformer 논문에서도 거의 같은 표기를 사용했다($d_{\text{model}} = d$). 한 가지 차이가 있다면, Transformer 논문에서는 $n\times d$ 행렬을 쓰는 반면, 이 논문에서는 그 반대($d\times n$ 행렬)를 이용하기 때문에, 행렬의 각 열(column)이 한 input word embedding(혹은 token)으로 비유된다. 안그래도 이 논문에서 계속해서 $d\times n$ 행렬 $X$를 input sequence라고 칭한다.</p>

<ul>
  <li>
    <p>Sequence-to-sequence 함수의 연속성 정의</p>

    <p>Sequence-to-sequence function이 행렬을 받아 행렬을 내뱉는 함수이다 보니 연속성도 잘 정의되어야 한다. 논문에서는 $\mathbb{R}^{d\times n}$에 entry-wise $\ell^p$ norm($|\cdot|_p$)과 그에 대한 <a href="https://mathworld.wolfram.com/NormTopology.html">norm topology</a>를 주고 그 위에서 연속성을 정의하는 것으로 보인다. 이때 $p$의 값은 $1\le p&lt;\infty$.</p>
  </li>
  <li>
    <p>함수 간의 거리(function metric)</p>

    <p>함수끼리 얼마나 가까운 지를 나타내기 위해 function 사이의 distance를 정의한다. 즉 sequence-to-sequence function space의 metric $d_p$을 쓰자면 다음과 같다.</p>

    <p><img src="/assets/img/papers/metric-formula.png" alt="metric-formula" /></p>

    <p>(Usual한 $\ell^p$  function norm을 이용해서, 논문에 있는 표기와 조금 다르게 적어보았다.)</p>

    <ul>
      <li>
        <p>Note:</p>

        <p>논문에서는 언제나 compact domain, compact support를 가정하기 때문에, $N_p(f)$가 무한대로 발산할 걱정은 하지 않아도 될 것 같다.</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-permutation-equivariant">2. Permutation Equivariant</h2>

<ul>
  <li>
    <p>Permutation matrix란</p>

    <p>Permutation matrix는 각 행과 각 열마다 1이 딱 하나씩 있는 정사각행렬이다. 어떤 행렬 $A\in \mathbb{R}^{m\times n}$에 Permutation matrix $P$를 곱하면 $A$의 행 또는 열의 순서를 뒤죽박죽 섞어 놓은 것과 같다. 좀 더 정확히는, (1) $P\in \mathbb{R}^{n\times n}$이라면 $AP$는 $A$의  열들의 순서를 섞어놓은 행렬이 되고, (2) $P\in \mathbb{R}^{m\times m}$이라면 $PA$는 $A$의  행들의 순서를 섞어놓은 행렬이 된다. 예를 들자면 다음과 같다.</p>

\[\begin{pmatrix} 1&amp;2&amp;3 \\ 4&amp;5&amp;6 \\ 7&amp;8&amp;9\end{pmatrix}\begin{pmatrix} 0&amp;1&amp;0 \\ 0&amp;0&amp;1 \\ 1&amp;0&amp;0\end{pmatrix} = \begin{pmatrix} 3&amp;1&amp;2 \\ 6&amp;4&amp;5 \\ 9&amp;7&amp;8\end{pmatrix}\]

    <p>참고로 이러한 permutation matrix는 언제나 orthogonal하다: $P^TP=PP^T=I$. (P가 행/열의 순서를 어떻게 섞는지 생각해보자.)</p>
  </li>
</ul>

<p>임의의 $X\in \mathbb{R}^{m\times n}$와 임의의 permutation matrix $P\in \mathbb{R}^{n\times n}$에 대해서, Sequence-to-sequence function인 $f$가 $f(XP)=f(X)P$를 만족하면 이러한 함수가 permutation equivariant하다고 말한다.</p>

<p>Sequence의 순서를 뒤섞는 일을 함수에 대입하기 전에 하나 후에 하나 달라지지 않는 함수를 말한다고 보면 된다.</p>

<p>참고로 논문에서는 각각의 <strong>transformer (encoder) block이 permutation equivariant한 sequence-to-sequence function</strong>임을 증명한다. <strong>(Claim 1)</strong></p>

<h2 id="3-universal-approximation">3. Universal Approximation</h2>

<p>딥러닝 이론의 출발점이라고 할 만한 정리로, Neural network의 expressive power에 대해 알려주는 정리인 ‘<strong>universal approximation theorem</strong>’이 있다. 이것의 내용을 요약하자면 다음과 같다.</p>

<blockquote>
  <p>Hidden layer가 1개 있는 neural network만 가지고도 아무런 연속함수(with compact support)를 <strong>임의의 (아주 작은) 오차로 근사</strong>할 수 있다. (단! network의 width에는 제한이 없으며, 중간에 있는 activation function은 다항함수가 아님.)</p>

</blockquote>

<p>이처럼, Universal Approximator는 ‘임의의 정확도로 엄청 많은 함수들을 근사할 수 있’는 모델을 두고 하는 말이다.  이후로도 universal approximation에 대한 다방면의 연구가 이루어졌는데, 이는 여기서 소개하는 논문의 section 1.2 related works &amp; notation에 잘 소개되어 있다.</p>

<h2 id="4-contextual-mapping">4. Contextual Mapping</h2>

<p>논문에 따르면, Transformer가 높은 성능을 보여주는 이유가 보통 ‘contextual mapping’을 잘 계산하기 때문이라고 평가된다고 한다. 즉, 각각의 문맥을 서로 잘 구분하는 능력이 탁월하다고 보는 것이다.</p>

<p>논문에서는 Trasformer의 이런저런 universal approximation 능력을 증명하려 하는데, 그 과정 중에 ‘(multi-head) self-attention layers가 contextual mapping을 잘 계산한다’는 것을 증명하는 게 정말 중요한 중간 과정이라고 한다. 이를 위해 논문에서는 contextual mapping의 개념을 아예 수식적으로 정의해버린 뒤에 이를 증명에 이용한다. 논문에서 주어진 정의는 다음과 같다.</p>

<p><img src="/assets/img/papers/trasformer-formula.jpeg" alt="transformer-formula.jpeg" /></p>

<p>즉 contextual mapping은 길이 $n$인 input sequence를 받아 $n$개의 값 (혹은 $n$차원 열벡터)를 내놓는 함수로 정의된다. 이때 한 문장(sequence) 안의 단어들은 서로 다른 역할을 하므로 각각 다른 context값(contextual mapping의 entry)이 매겨진다(1번 조건). 게다가, 같은 단어라도 다른 문장에서는 다른 의미로 해석된다는 의미에서, 서로 다른 두 input sequence(L, L’)에 대한 contextual mapping에 있는 모든 (총 2n개의) entry들은 전부 다르게 매겨진다(2번 조건).</p>

<ul>
  <li>
    <p>집합 $\mathbb{L}$이 유한집합으로 설정된 이유는 (내 생각에는)</p>

    <p>Vocabulary의 크기도 유한하고 sequence 길이도 유한하므로 만들 수 있는 input sequence의 개수는 유한하다. Sequence들의 집합과 대응되는 집합이 $\mathbb{L}$과 비슷한 것이라면, $\mathbb{L}$을 유한집합이라고 놓아도 괜찮을 것이다. (이 조건이 필수인지는 증명을 더 들여다봐야..)</p>
  </li>
</ul>

<hr />
<hr />

<h1 id="main-text">Main Text</h1>

<h2 id="1-universal-approximator임을-보이기-힘든-이유">1. Universal Approximator임을 보이기 힘든 이유</h2>

<ul>
  <li>너무 많아 보이는 Parameter sharing. Self-attention layer와 feed-forward layer 모두, token끼리 공유하는 parameter의 수가 매우 많다.</li>
  <li>너무 적어 보이는 token-wise interaction. Self-attention layer의 특성상 pairwise dot-product로만 token 사이의 interaction을 잡아낸다.</li>
</ul>

<p>(둘째 이유는 그럴 만하다고 보이는데, 첫째 이유는 아직 잘 이해하지 못했다.)</p>

<p>논문에서는 위의 두 이유로 인해 transformer encoder 자체가 나타낼 수 있는 sequence-to-sequence 함수의 종류에 제한이 있다고 보며, 이를 trainable한 positional encoding으로 해결한다.</p>

<p>❓ 일반적으로, Parameter sharing이 많을수록 universal approximator가 되기 어려운 이유는 무엇일까?</p>

<hr />

<h2 id="2-논문에서-본-transformer">2. 논문에서 본 Transformer</h2>

<p>아래는 논문에서 사용한 transformer block에 대한 식이다.</p>

<p><img src="/assets/img/papers/contextual-mapping.jpeg" alt="contextual-mapping.jpeg" /></p>

<p>잘 알려져 있듯, transformer encoder block은 multi-head self-attention layer(’Attn’)와 token-wise feed-forward layer(’FF’)라는 두 (sub-)layer로 나뉜다.</p>

<h3 id="21-기존-transformer-논문과의-공통점">2.1. 기존 Transformer 논문과의 공통점</h3>

<ul>
  <li>수식에서 확인할 수 있듯 residual connection은 그대로 살려두었다.</li>
</ul>

<h3 id="22-기존-transformer-논문과의-차이점">2.2. 기존 Transformer 논문과의 차이점</h3>

<ul>
  <li>해석을 간단히 하기 위해 layer normalization은 뺐다고 한다.</li>
  <li>Self-attention layer 식을 보면 기존 논문에서는 볼 수 없던 시그마($\sum$) 기호가 보인다. 원래 transformer 논문에서는 attention head들을 concatenate하는데, 이러한 concatenation을 수식적으로는 저렇게 표현할 수 있다고 한다. 즉 의미가 다른 식이 아니다.</li>
  <li>Self-attention layer의 소문자 시그마 함수($\sigma(\cdot)$)는 (column-wise) softmax를 가리킨다. 그런데 기존 논문에서는 scaled dot-product attention을 사용하는 반면 여기서는 그냥 dot-product attention을 쓰는 것처럼 보인다. 사실 $\boldsymbol{W}_K$나 $\boldsymbol{W}_Q$같은 parameter들이 그 scaling factor($\frac{1}{\sqrt{d_k}}$)를 학습하면 그만이다.</li>
</ul>

<p>❓ Layer normalization을 빼도 괜찮은 이유는 무엇일까?</p>

<h3 id="23-positional-encoding">2.3. Positional encoding</h3>

<ul>
  <li>Trainable한 positional encoding이 없는 순수한 transformer block은 오직 ‘permutation equivariant’한 종류의 함수만을 잘 근사할 뿐이다. 그러나 positional encoding을 도입함으로써 이러한 함수 종류의 제한 없이 아무런 sequence-to-sequence 함수(with compact domain)을 잘 근사할 수 있게 된다.</li>
  <li>Positional encoding $\boldsymbol{E}$ 역시 $d\times n$ 크기의 real matrix로 정의된다. Transformer block을 함수 $g$로 쓴다면, positional encoding이 도입된 transformer block은 input sequence $\boldsymbol{X}$에 대해 $g(\boldsymbol{X}+\boldsymbol{E})$라고 쓸 수 있다.</li>
  <li>논문에서는 이 $\boldsymbol{E}$가 trainable하다고 가정하므로 아무렇게나 설정할 수 있다. 실제로, 함수들의 domain이 compact함을 가정해서 input sequence가 $\boldsymbol{X}\in [0,1]^{d\times n}$ 가 되도록 한 뒤, positional encoding을 나타내는 행렬을 임의로 다음과 같이 정의한다. (Appendix C 참고)</li>
</ul>

\[\boldsymbol{E} = \begin{pmatrix} 0&amp;1&amp;2&amp;\cdots&amp;n-1\\0&amp;1&amp;2&amp;\cdots&amp;n-1\\\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots\\0&amp;1&amp;2&amp;\cdots&amp;n-1\end{pmatrix}\]

<hr />

<h2 id="3-주요-결과-2가지">3. 주요 결과 (2가지)</h2>

<p>논문이 주장하는 두 가지 중요한 결과는 Abstract에서 소개한 처음 두 줄과 같다. 여기서는 더 자세한 서술을 소개한다.</p>

<h3 id="31-theorem-2">3.1. Theorem 2</h3>

<hr />

<p>(임의의  $\epsilon&gt;0$와 $1\le p &lt; \infty$에 대해) 함수 $f$가 다음의 조건을 만족한다고 하자.</p>
<ol>
  <li>$f$는 sequence-to-sequence 함수.</li>
  <li>$f$의 support는 compact.</li>
  <li>$f$는 연속(w.r.t. entry-wise $\ell^p$ norm).</li>
  <li>$f$는 <strong>permutation equivariant</strong>.</li>
</ol>

<p>그러면 다음 조건을 만족하는 Transformer network $g$가 존재한다.</p>
<ol>
  <li>$g$는 $(h,m,r)=(2,1,4)$를 만족.</li>
  <li>$d_p (f,g ) \le \epsilon$.</li>
</ol>

<hr />

<ul>
  <li>참고: Transformer network란, 같은 Transformer block을 여러 개 쌓은 것이다. 또 위에서 쓰인 h, m, r은 각각 다음과 같은 것을 나타내는 기호다.</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">문자</th>
      <th style="text-align: center">뜻</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$h$</td>
      <td style="text-align: center">attention head의 개수</td>
    </tr>
    <tr>
      <td style="text-align: center">$m$</td>
      <td style="text-align: center">attention head의 크기</td>
    </tr>
    <tr>
      <td style="text-align: center">$r$</td>
      <td style="text-align: center">feed-forward layer의 hidden 차원 (=$d_{ff}$)</td>
    </tr>
  </tbody>
</table>

<h3 id="32-theorem-3">3.2. Theorem 3</h3>

<hr />
<p>(임의의  $\epsilon&gt;0$와 $1\le p &lt; \infty$에 대해) 함수 $f$가 다음의 조건을 만족한다고 하자.</p>
<ol>
  <li>$f$는 sequence-to-sequence 함수.</li>
  <li>$f$의 domain은 compact.</li>
  <li>$f$는 연속(w.r.t. entry-wise $\ell^p$ norm).</li>
</ol>

<p>그러면 다음 조건을 만족하는 <strong>Transformer network $g$ with (trainable) positional encoding $\boldsymbol{E}$</strong>가 존재한다.</p>
<ol>
  <li>$g$는 $(h,m,r)=(2,1,4)$를 만족.</li>
  <li>$d_p (f,g ) \le \epsilon$.</li>
</ol>

<hr />

<p>거의 모든 것이 Theorem 2와 동일하지만, Transformer network에는 positional encoding이 추가됐고, 대신 근사하려는 sequence-to-sequence 함수의 permutation equivariant 조건이 사라졌다.</p>

<ul>
  <li>
    <p>$(h,m,r)=(2,1,4)$를 쓰는 이유? (너무 작은 block 아닌가?)</p>

    <p>Attention head가 2개밖에 없고, 그 크기도 겨우 1이고, 심지어 feed-forward layer의 hidden 차원이 4밖에 안 되는 작은 Transformer block은 실질적으로 쓰이지 않는다. 그러나 이러한 Transformer block을 이용한 이유는 단지 단순화가 증명을 쉽게 해주기 때문만은 아니다.</p>

    <p>더 큰 모델은 자명하게 expressive power가 더 크기 때문이다. 실질적으로 쓰이는 transformer block은 훨씬 더 많은 parameter를 쓸 텐데, 그런 model은 논문에서 쓰이는 매우 작은 transformer block에 비하면 당연히 더욱더 많은 함수들을 표현할 수 있을 것이다. 그러니 이렇게 작은 스케일로 문제를 축소시켜서 문제를 풀어도 충분하다.</p>
  </li>
</ul>

<p>❓ 위의 두 정리는 universal approximation의 측면에서 매우 유의미한 결과를 내고 있다. 그러나 모두 존재성 정리인 탓에, 훈련 과정에서 transformer가 ‘우리가 원하는 함수’를 실제로 잘 근사할 수 있는지는 말해주지 않는 게 분명하다. 이것이 가능한지는 어떻게 연구해야 할까?/ 어떻게 연구되고 있을까?</p>

<hr />

<h2 id="4-어떻게-증명하나">4. 어떻게 증명하나?</h2>

<p>Theorem 2와 Theorem 3의 증명은 매우 유사하며, 본문에서는 Theorem 2의 증명과정을 요약하여 설명한다. 세 단계로 나누어 임의의 continuous, permutation equivariant, sequence-to-sequence function $f$ with compact support를 적절한 Transformer network로 근사한다. 그 로드맵은 다음과 같다.</p>

<h3 id="1-f를-piece-wise-상수함수로-근사하기">1) $f$를 piece-wise 상수함수로 근사하기</h3>

<p>상수함수라고 해서 f가 갑자기 real-valued가 되는 것이 아니다. 여기서의 상수함수 역시 행렬을 받아 행렬을 내뱉는 함수인데, 함숫값으로서의 행렬이 고정되어 있으면 상수함수인 것이다.</p>

<h3 id="2-piece-wise-상수함수를-modified-transformer-network로-근사하기">2) Piece-wise 상수함수를 ‘modified’ Transformer network로 근사하기</h3>

<p>‘Modified’ Transformer란, 기존의 Transformer에서 쓰이던 (column-wise) softmax 함수($\sigma$)는 column-wise hardmax($\sigma_H$)로 대체하고, FF의 activation function으로 쓰이던 ReLU는 또다른 특이한 함수($\phi \in \Phi$, 자세한 정의는 아래에)로 대체한 것이다.</p>

<ul>
  <li>
    <p>$\Phi$의 정의</p>

    <p>The set of all piece-wise linear functions with at most three pieces, where at least one piece is constant. (p.9)</p>
  </li>
</ul>

<p>이 부분을 증명하기 위해, 논문에서는 modified Transformer의 layer 순서를 뜯어고치는 일을 하는 것으로 보인다. Residual connection을 이용하면, self-attention과 feed-forward layer를 번갈아 적용하는게 아니라 self-attention만 쭉, 혹은 feed-forward layer만 쭉 이어 합성한 것을 활용할 수 있다고 한다.</p>

<blockquote>
  <p>(….) we note that even though a Transformer network stacks self-attention and feed-forward layers in an alternate manner, <strong>the skip connections enable these networks to employ a composition of multiple self-attention or feed-forward layers.</strong> (중략) self-attention and feed-forward layers play in realizing the ability to universally approximate sequence-to-sequence functions: 1) self-attention layers compute precise contextual maps; and 2) feed-forward layers then assign the results of these contextual maps to the desired output values. (p.6)</p>

</blockquote>

<p>❓ Modified Transformer network의 layer 순서를 뒤바꾸어 같은 종류의 layer만 이어붙일 수 있는 이유가 구체적으로 무엇일까? 여기에 skip connection은 어떤 역할을 할까?</p>

<h3 id="3-modified-transformer-network를-transformer-network로-근사하기">3) Modified Transformer network를 Transformer network로 근사하기</h3>

<p>앞에서 대체했던 softmax와 ReLU를 원래대로 돌려놓는 작업이라고 보면 될 것 같다.</p>

<hr />

<h2 id="5-몇-개의-block을-쌓아야-하나">5. 몇 개의 block을 쌓아야 하나?</h2>

<p>Theorem 2는 결과적으로 몇개의 Transformer block을 쌓아야 하는지 보여준다. 논문에서 제시하는, permutation equivariant 함수를 잘 근사하기 위해 필요한 (h,m,r)=(2,1,4) Transformer block은 총 $O(n(1/\delta)^{dn}/n!)$개다. 또한, positional encoding까지 더해 좀 더 광범위한 sequence-to-sequence 함수를 잘 근사하기 위해서 필요한 block은 $O(n(1/\delta)^{dn})$개다.</p>

<p>이때 $\delta$는 Theorem 2/3의 증명 1~2단계에서 쓰인 piecewise constant function의 domain을 구분하는 grid를 이루는 (hyper-)cube의 한 변의 길이이며, 충분히 작음을 가정해야 한다. (증명과정에 따르면, $O(\delta^{d/p} ) \le \epsilon/3)$</p>

<p>❓ 논문에서는 증명을 위해 아주 작은 transformer block을 이용하고 있다. 만약 이 transformer block의 크기를 키운다면 필요한 block의 수는 줄어들까? (아마 $d$와 $n$에 따른 complexity에는 크게 차이가 있지 않을 것 같다. $h$, $m$, $r$ 등의 값은 $d$나 $n$의 값과는 관련이 없으므로.)</p>

<hr />
<hr />

<h1 id="my-comments--questions">My Comments &amp; Questions</h1>

<ul>
  <li>선형대수학을 꽤나 쓰는 논문이지만 실상은 엄청나게 해석학스러운 논문이었다. 해석학1때 Weierstrass Approximation Theorem(compact domain에서 연속함수를 다항식으로 임의의 정확도로 근사하기) 배웠던 것이 새록새록…</li>
  <li>위에서 던졌던 질문들은 내가 논문을 읽으면서도 끝까지 이해하지 못했던, 혹은 스스로 만족스럽게 대답하지 못했던 대표적인 질문들이다. 한 번 더 모아보자면 아래와 같다.</li>
</ul>

<p>❓ 일반적으로, Parameter sharing이 많을수록 universal approximator가 되기 어려운 이유는 무엇일까?</p>

<p>❓ Layer normalization을 빼도 괜찮은 이유는 무엇일까?</p>

<p>❓ (Paraphrased:) 훈련 과정에서 transformer가 ‘우리가 원하는 함수’를 실제로 잘 근사할 수 있는지는 어떻게 알 수 있을까?</p>

<p>❓ Modified Transformer network의 layer 순서를 뒤바꾸어 같은 종류의 layer만 이어붙일 수 있는 이유가 구체적으로 무엇일까? 여기에 skip connection은 어떤 역할을 할까?</p>

<p>❓ 논문에서는 증명을 위해 아주 작은 transformer block을 이용하고 있다. 만약 이 transformer block의 크기를 키운다면 필요한 block의 수는 줄어들까?</p>

    <br />
    <br />
  </div>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
  

</article>

<hr class="dingbat" />

<div class="share">
  <h2>Share this post</h2>
  <div class="share-body">
    <a href="http://twitter.com/share?text=[논문리뷰] Are Transformers universal approximators of sequence-to-sequence functions?&amp;url=http://localhost:4000/papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions/"
      onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
      <span class="icon-twitter">
      </span>
    </a>
    <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions/"
      onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
      <span class="icon-facebook">
      </span>
    </a>
  </div>
</div>
<br />





  <aside class="author" role="complementary">
    <div class="author">
  <h2 class="page-title hr">
    About
  </h2>
<div class="author-body">
  
    
  

  

  <img
    src="/assets/img/me.jpeg"
    class="me"
    alt="Hanseul cho"
    
    
  />


  
  <div class="author-body-description">
    <p>I am an M.Sc. student in OptiML Lab @ <a href="http://gsai.kaist.ac.kr"><strong>KAIST AI</strong></a>, advised by Prof. <a href="https://chulheeyun.github.io"><strong>Chulhee Yun</strong></a>.
Recently, I focus on deep learning theory.</p>

  </div>
</div>
</div>

  </aside>





<aside class="related" role="complementary">
  <h2 class="hr">Related Posts</h2>

  <ul class="related-posts">
    
      
      
      
        
          
          
        
        
          


<li class="h4">
  <a href="/papers/2022/01/29/paper-list/" data-flip="title">
    <span>읽고 싶은 논문 리스트!</span>
  </a>
  <small><time datetime="2022-01-29T00:00:00+09:00">
    29 Jan 2022
  </time></small>
</li>

        
      
    
  </ul>
</aside>



      
        <aside class="comments" role="complementary">
  <h2>Comments</h2>
  <hr/>

  <div id="disqus_thread"></div>

  <script>
    !function(s,i){function e(e){var t=s.pageYOffset||i.body.scrollTop;s.DISQUS&&!s._disqusThis&&!s._disqusFirst&&t+s.innerHeight>=s._disqusThreadOffsetTop&&(s._disqusThis=!0,s.DISQUS.reset({reload:!0,config:d}))}var d=function(){this.page.title="[논문리뷰] Are Transformers universal approximators of sequence-to-sequence functions?",this.page.identifier="/papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions",this.page.url="http://localhost:4000/papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions/"};s._disqusFirst=void 0===s._disqusFirst||s._disqusFirst,s._disqusLoading=void 0!==s._disqusLoading&&s._disqusLoading,s._disqusThis=!1,s._disqusThreadOffsetTop=i.getElementById("disqus_thread").offsetTop,s._disqusLoading?s._disqusFirst=!1:(s._disqusLoading=!0,loadJSDeferred("//hanseul.disqus.com/embed.js"),s.addEventListener?s.addEventListener("scroll",e,{passive:!0}):s.attachEvent?s.attachEvent("onscroll",e):s.onscroll=e)}(window,document);

  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</aside>

      

      <footer>
  <hr/>
  
    <p>© 2022. by hanseul</p>

  
  <p>
    <code>Powered by <a href="https://qwtel.com/hydejack/">Hydejack v6.4.0</a></code>
  </p>
</footer>

    </main>
    <div class="right-side">
  <div class="ad-first">
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <!-- 블로그-상단-모바일 -->
    <ins class="adsbygoogle"
         style="display:inline-block;width:100%;"
         data-ad-client="ca-pub-9134477021095729"
         data-ad-slot="6559875097"
         data-ad-format="auto"></ins>
    <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
  </div>
<br/>
<br/>
  <div class="ad-second">
    <!-- 블로그-스카이스크래퍼 -->
    <ins class="adsbygoogle"
         style="display:inline-block;max-width:320px;width:100%;height:600px"
         data-ad-client="ca-pub-9134477021095729"
         data-ad-slot="6826803092"></ins>
    <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
  </div>
</div>

  </div>
  <div id="_yDrawer">
  <div id="_sidebar" class="sidebar">
    <div class="sidebar-bg" style="background-color:#148bba;background-image:url(/assets/img/side.jpeg)"></div>
    <header class="sidebar-sticky" role="banner">
      <br/>
      <div class="sidebar-about">
        <h1><a id="_title" href="/">Hanseul's Blog</a></h1>
        <p>ML/AI</p>

      </div>

      <br/>
      <br/>
      <nav class="sidebar-nav heading" role="navigation">
        <span class="sr-only">Navigation:</span>
<ul>
  

  

  
  
  
  
  
    <li>
      <input type="checkbox" id="list-item-1"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/category/papers/">Papers</a>
       <label class="folder" for="list-item-1">▾</label>
    </div>
     <ul class="list-body">
       
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/papers-theory/">Theory</a>
             </li>
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/papers-experiment/">Experiment</a>
             </li>
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/papers-observation/">Observation</a>
             </li>
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/papers-openproblem/">OpenProblem</a>
             </li>
           
         
     </ul>
    </li>

  
  
    <li>
      <input type="checkbox" id="list-item-2"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/category/courseworks/">CourseWorks</a>
       <label class="folder" for="list-item-2">▾</label>
    </div>
     <ul class="list-body">
       
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/courseworks-ai502/">심층학습(AI502)</a>
             </li>
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/courseworks-ai506/">데이터마이닝및검색(AI506)</a>
             </li>
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/courseworks-ai616/">심층학습이론(AI616)</a>
             </li>
           
         
           
         
           
         
     </ul>
    </li>

  
  
    <li>
      <input type="checkbox" id="list-item-3"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/about/">About: Hanseul Cho</a>
       
    </div>
     <ul class="list-body">
       
           
         
           
         
           
         
           
         
           
         
           
         
           
         
     </ul>
    </li>

  
</ul>

      </nav>
    <br/>
    <br/>
      <div class="sidebar-box">
        
          
  

  

  <img
    src="/assets/img/me.jpeg"
    class="me"
    alt="Hanseul cho"
    
    
  />


        
      </div>
      <p>M.Sc. Student in OptiML Lab @ KAIST AI.</p>

      
      
        <div class="sidebar-social">
          <span class="sr-only">Social:</span>
<ul>
  
    









<li>
  <a href="https://github.com/HanseulJo">
    <span class="icon-github" title="GitHub"></span>
    <span class="sr-only">GitHub</span>
  </a>
</li>

  
    









<li>
  <a href="mailto:jhs4015@kaist.ac.kr">
    <span class="icon-mail" title="Email"></span>
    <span class="sr-only">Email</span>
  </a>
</li>

  
</ul>

        </div>
      
    </header>
  </div>
</div>

</div>

<!-- =============== -->
<!-- SCRIPTS         -->
<!-- =============== -->

<script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', '{"tracking_id"=>"G-3K595H6QML"}', 'auto');
  ga('send', 'pageview');
  loadJSDeferred('https://www.google-analytics.com/analytics.js');
</script>





<!--[if gt IE 8]><!---->
<script src="//ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"></script>
<script>
  WebFont.load({
    
    google: {
      families: 'Lato'.split('|')
    },
    

    custom: {
      families: ['icomoon'],
      urls: ['/assets/icomoon/style.css']
    }
  });
</script>
<!--<![endif]-->


  <!--[if gt IE 9]><!---->
  
  <script>loadJSDeferred('/assets/js/hydejack.js?v=6.4.0');</script>

  
  <!--<![endif]-->



<script type="text/javascript" async
	src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		processEscapes: true
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
});
</script>

</body>

</html>
