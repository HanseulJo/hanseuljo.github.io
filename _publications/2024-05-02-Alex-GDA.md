---
title: "Fundamental Benefit of Alternating Updates in Minimax Optimization"
collection: publications
permalink: /publication/Alex-GDA
date: 2024-05-02
toc: true
toc_sticky: true
authors:
    - JwL*
    - me*
    - CY
venue: <a href="https://icml.cc/Conferences/2024"><b>ICML 2024</b></a> (Short version at ICLR 2024 Workshop on <a href="https://sites.google.com/view/bgpt-iclr24">Bridging the Gap Between Practice and Theory in Deep Learning (BGPT)</a>)
award: <a href="https://icml.cc/virtual/2024/events/2024SpotlightPosters"><b>Spotlight</b></a> at the 41st Int'l Conf. on Machine Learning (ICML 2024; Top 3.5% among total submissions)
paperurl: https://proceedings.mlr.press/v235/lee24e.html
arxiv: https://arxiv.org/abs/2402.10475
pdf: https://openreview.net/pdf?id=s6ZAT8MLKU
code: HanseulJo/Alex-GDA
x:
linkedin: https://www.linkedin.com/posts/hanseul-cho_take-a-look-at-our-spotlight-paper-top-activity-7219619439174901762-y3jr
doi: "10.48550/arXiv.2402.10475"
scholar: "2osOgNQ5qMEC"
categories: 
    - ICLR Workshop
    - ICML
tags:
    - minimax optimization
    - Gradient Descent-Ascent (GDA)
    - Alex-GDA
---
<!-- markdownlint-disable MD033 -->

![alex_gda_thumbnail](/assets/img/alex-gda/thumbnail_icml2024_hr.png)

## Poster

![alex_gda_poster](/assets/img/alex-gda/poster_icml2024.png)

## Abstract

The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax optimization problems, takes the descent and ascent steps either simultaneously (Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to converge faster, the performance gap between the two is not yet well understood theoretically, especially in terms of global convergence rates. To address this theory-practice gap, we present fine-grained convergence analyses of both algorithms for strongly-convex-strongly-concave and Lipschitz-gradient objectives. Our new iteration complexity upper bound of Alt-GDA is strictly smaller than the lower bound of Sim-GDA; i.e., Alt-GDA is provably faster. Moreover, we propose Alternating-Extrapolation GDA (Alex-GDA), a general algorithmic framework that subsumes Sim-GDA and Alt-GDA, for which the main idea is to alternately take gradients from extrapolations of the iterates. We show that Alex-GDA satisfies a smaller iteration complexity bound, identical to that of the Extra-gradient method, while requiring less gradient computations. We also prove that Alex-GDA enjoys linear convergence for bilinear problems, for which both Sim-GDA and Alt-GDA fail to converge at all.

## Read the Full Paper

<object data="{{ page.pdf }}" width="960" height="1000" type='application/pdf'></object>
