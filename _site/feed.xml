<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hanseul&apos;s Blog</title>
    <description>ìˆ˜í•™ ì „ê³µí–ˆì§€ë§Œ ì´ì œëŠ” AI ê³µë¶€í• ë˜. ê·¸ ì™¸ì— ìŒì•…ë„ ì¢‹ì•„í•©ë‹ˆë‹¤.
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 20 Feb 2022 01:13:07 +0900</pubDate>
    <lastBuildDate>Sun, 20 Feb 2022 01:13:07 +0900</lastBuildDate>
    <generator>Jekyll v3.9.1</generator>
    
      <item>
        <title>[ë…¼ë¬¸ë¦¬ë·°] Are Transformers universal approximators of sequence-to-sequence functions?</title>
        <description>&lt;p&gt;ğŸ“Œ í•œ ì¤„ ìš”ì•½: &lt;strong&gt;Transformerì˜ expressive powerë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë³´ì¸ ì²«ë²ˆì§¸ ë…¼ë¬¸&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ğŸ¤” ì°¸ê³ : ë…¸ì…˜ìœ¼ë¡œ ì‘ì„±í•œ &lt;a href=&quot;https://han-5eu1.notion.site/Are-Transformers-universal-approximators-of-sequence-to-sequence-functions-158eac79332a4d81b1b7cccff9b1b0ce&quot;&gt;ì›ë¬¸&lt;/a&gt;ì„ ì˜®ê²¨ì˜¨ ê²ƒì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#abstract&quot; id=&quot;markdown-toc-abstract&quot;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#keywords--definitions&quot; id=&quot;markdown-toc-keywords--definitions&quot;&gt;Keywords &amp;amp; Definitions&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1-sequence-to-sequence-function&quot; id=&quot;markdown-toc-1-sequence-to-sequence-function&quot;&gt;1. Sequence-to-sequence Function&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-permutation-equivariant&quot; id=&quot;markdown-toc-2-permutation-equivariant&quot;&gt;2. Permutation Equivariant&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3-universal-approximation&quot; id=&quot;markdown-toc-3-universal-approximation&quot;&gt;3. Universal Approximation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4-contextual-mapping&quot; id=&quot;markdown-toc-4-contextual-mapping&quot;&gt;4. Contextual Mapping&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#main-text&quot; id=&quot;markdown-toc-main-text&quot;&gt;Main Text&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1-universal-approximatorì„ì„-ë³´ì´ê¸°-í˜ë“ -ì´ìœ &quot; id=&quot;markdown-toc-1-universal-approximatorì„ì„-ë³´ì´ê¸°-í˜ë“ -ì´ìœ &quot;&gt;1. Universal Approximatorì„ì„ ë³´ì´ê¸° í˜ë“  ì´ìœ &lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-ë…¼ë¬¸ì—ì„œ-ë³¸-transformer&quot; id=&quot;markdown-toc-2-ë…¼ë¬¸ì—ì„œ-ë³¸-transformer&quot;&gt;2. ë…¼ë¬¸ì—ì„œ ë³¸ Transformer&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#21-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ê³µí†µì &quot; id=&quot;markdown-toc-21-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ê³µí†µì &quot;&gt;2.1. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜ ê³µí†µì &lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#22-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ì°¨ì´ì &quot; id=&quot;markdown-toc-22-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ì°¨ì´ì &quot;&gt;2.2. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜ ì°¨ì´ì &lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#23-positional-encoding&quot; id=&quot;markdown-toc-23-positional-encoding&quot;&gt;2.3. Positional encoding&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3-ì£¼ìš”-ê²°ê³¼-2ê°€ì§€&quot; id=&quot;markdown-toc-3-ì£¼ìš”-ê²°ê³¼-2ê°€ì§€&quot;&gt;3. ì£¼ìš” ê²°ê³¼ (2ê°€ì§€)&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#31-theorem-2&quot; id=&quot;markdown-toc-31-theorem-2&quot;&gt;3.1. Theorem 2&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#32-theorem-3&quot; id=&quot;markdown-toc-32-theorem-3&quot;&gt;3.2. Theorem 3&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4-ì–´ë–»ê²Œ-ì¦ëª…í•˜ë‚˜&quot; id=&quot;markdown-toc-4-ì–´ë–»ê²Œ-ì¦ëª…í•˜ë‚˜&quot;&gt;4. ì–´ë–»ê²Œ ì¦ëª…í•˜ë‚˜?&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#1-fë¥¼-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¡œ-ê·¼ì‚¬í•˜ê¸°&quot; id=&quot;markdown-toc-1-fë¥¼-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¡œ-ê·¼ì‚¬í•˜ê¸°&quot;&gt;1) $f$ë¥¼ piece-wise ìƒìˆ˜í•¨ìˆ˜ë¡œ ê·¼ì‚¬í•˜ê¸°&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#2-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¥¼-modified-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°&quot; id=&quot;markdown-toc-2-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¥¼-modified-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°&quot;&gt;2) Piece-wise ìƒìˆ˜í•¨ìˆ˜ë¥¼ â€˜modifiedâ€™ Transformer networkë¡œ ê·¼ì‚¬í•˜ê¸°&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#3-modified-transformer-networkë¥¼-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°&quot; id=&quot;markdown-toc-3-modified-transformer-networkë¥¼-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°&quot;&gt;3) Modified Transformer networkë¥¼ Transformer networkë¡œ ê·¼ì‚¬í•˜ê¸°&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5-ëª‡-ê°œì˜-blockì„-ìŒ“ì•„ì•¼-í•˜ë‚˜&quot; id=&quot;markdown-toc-5-ëª‡-ê°œì˜-blockì„-ìŒ“ì•„ì•¼-í•˜ë‚˜&quot;&gt;5. ëª‡ ê°œì˜ blockì„ ìŒ“ì•„ì•¼ í•˜ë‚˜?&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#my-comments--questions&quot; id=&quot;markdown-toc-my-comments--questions&quot;&gt;My Comments &amp;amp; Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Transformer encoder&lt;/strong&gt;ëŠ” &lt;strong&gt;â€˜permutation equivariant&lt;/strong&gt;â€™í•œ ì„±ì§ˆì„ ê°–ëŠ” &lt;strong&gt;ì—°ì†&lt;/strong&gt;ì¸ â€˜&lt;strong&gt;sequence-to-sequence&lt;/strong&gt;â€™ í•¨ìˆ˜(with compact support)ì— ëŒ€í•œ universal approximatorì„ì„ ë³´ì¸ë‹¤.&lt;/li&gt;
  &lt;li&gt;Transformer encoderì—ë‹¤ &lt;strong&gt;learnableí•œ positional encodingsë¥¼ ê°™ì´ ì“°ë©´&lt;/strong&gt; &lt;strong&gt;ì„ì˜ì˜&lt;/strong&gt;(permutation equivariantí•˜ì§€ ì•Šì•„ë„) &lt;strong&gt;ì—°ì†&lt;/strong&gt;ì¸ â€˜&lt;strong&gt;sequence-to-sequence&lt;/strong&gt;â€™ í•¨ìˆ˜(with compact domain)ë¥¼ universally approximateí•¨ì„ ë³´ì¸ë‹¤.&lt;/li&gt;
  &lt;li&gt;Contextual mappingì´ë¼ëŠ” ê²ƒì„ ìˆ˜ì‹ì ìœ¼ë¡œ ì •ì˜í–ˆìœ¼ë©°, Transformer Encoderì˜ &lt;strong&gt;multi-head&lt;/strong&gt; &lt;strong&gt;self-attention layerë“¤ì´ ì…ë ¥ sequenceì— ëŒ€í•œ contextual mappingì„ ì˜ ê³„ì‚°í•¨&lt;/strong&gt;ì„ ë³´ì¸ë‹¤.&lt;/li&gt;
  &lt;li&gt;(ì‹¤í—˜ë„ ì§„í–‰í•˜ì˜€ìœ¼ë‚˜ ì—¬ê¸°ì„œëŠ” ìƒëµ)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h1 id=&quot;keywords--definitions&quot;&gt;Keywords &amp;amp; Definitions&lt;/h1&gt;

&lt;h2 id=&quot;1-sequence-to-sequence-function&quot;&gt;1. Sequence-to-sequence Function&lt;/h2&gt;

&lt;p&gt;$\mathbb{R}^{d\times n}$ì—ì„œ $\mathbb{R}^{d\times n}$ë¡œ ê°€ëŠ” í•¨ìˆ˜ë¥¼ &lt;strong&gt;sequence-to-sequence&lt;/strong&gt; functionì´ë¼ê³  ë§í•œë‹¤. ì •í™•íˆëŠ” ì •ì˜ì—­ë„ ì¹˜ì—­ë„ ëª¨ë‘ subset of $\mathbb{R}^{d\times n}$ì¸ í•¨ìˆ˜ë¥¼ ë§í•œë‹¤. ($\mathbb{R}^{d\times n}$: the set of all $d\times n$  real matrices)&lt;/p&gt;

&lt;p&gt;ì´ë•Œ $d$ì™€ $n$ì€ ê°ê°, &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Transformer ë…¼ë¬¸&lt;/a&gt;ì—ì„œ ì–¸ê¸‰í•˜ëŠ” embedding ì°¨ì›ê³¼ ì…ë ¥ sequence ê¸¸ì´ë¡œ ë¹„ìœ ëœë‹¤. ê¸°ì¡´ Transformer ë…¼ë¬¸ì—ì„œë„ ê±°ì˜ ê°™ì€ í‘œê¸°ë¥¼ ì‚¬ìš©í–ˆë‹¤($d_{\text{model}} = d$). í•œ ê°€ì§€ ì°¨ì´ê°€ ìˆë‹¤ë©´, Transformer ë…¼ë¬¸ì—ì„œëŠ” $n\times d$ í–‰ë ¬ì„ ì“°ëŠ” ë°˜ë©´, ì´ ë…¼ë¬¸ì—ì„œëŠ” ê·¸ ë°˜ëŒ€($d\times n$ í–‰ë ¬)ë¥¼ ì´ìš©í•˜ê¸° ë•Œë¬¸ì—, í–‰ë ¬ì˜ ê° ì—´(column)ì´ í•œ input word embedding(í˜¹ì€ token)ìœ¼ë¡œ ë¹„ìœ ëœë‹¤. ì•ˆê·¸ë˜ë„ ì´ ë…¼ë¬¸ì—ì„œ ê³„ì†í•´ì„œ $d\times n$ í–‰ë ¬ $X$ë¥¼ input sequenceë¼ê³  ì¹­í•œë‹¤.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Sequence-to-sequence í•¨ìˆ˜ì˜ ì—°ì†ì„± ì •ì˜&lt;/p&gt;

    &lt;p&gt;Sequence-to-sequence functionì´ í–‰ë ¬ì„ ë°›ì•„ í–‰ë ¬ì„ ë‚´ë±‰ëŠ” í•¨ìˆ˜ì´ë‹¤ ë³´ë‹ˆ ì—°ì†ì„±ë„ ì˜ ì •ì˜ë˜ì–´ì•¼ í•œë‹¤. ë…¼ë¬¸ì—ì„œëŠ” $\mathbb{R}^{d\times n}$ì— entry-wise $\ell^p$ norm($|\cdot|_p$)ê³¼ ê·¸ì— ëŒ€í•œ &lt;a href=&quot;https://mathworld.wolfram.com/NormTopology.html&quot;&gt;norm topology&lt;/a&gt;ë¥¼ ì£¼ê³  ê·¸ ìœ„ì—ì„œ ì—°ì†ì„±ì„ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ì´ë•Œ $p$ì˜ ê°’ì€ $1\le p&amp;lt;\infty$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;í•¨ìˆ˜ ê°„ì˜ ê±°ë¦¬(function metric)&lt;/p&gt;

    &lt;p&gt;í•¨ìˆ˜ë¼ë¦¬ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ ì§€ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ function ì‚¬ì´ì˜ distanceë¥¼ ì •ì˜í•œë‹¤. ì¦‰ sequence-to-sequence function spaceì˜ metric $d_p$ì„ ì“°ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/papers/metric-formula.png&quot; alt=&quot;metric-formula&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;(Usualí•œ $\ell^p$  function normì„ ì´ìš©í•´ì„œ, ë…¼ë¬¸ì— ìˆëŠ” í‘œê¸°ì™€ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ ì ì–´ë³´ì•˜ë‹¤.)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Note:&lt;/p&gt;

        &lt;p&gt;ë…¼ë¬¸ì—ì„œëŠ” ì–¸ì œë‚˜ compact domain, compact supportë¥¼ ê°€ì •í•˜ê¸° ë•Œë¬¸ì—, $N_p(f)$ê°€ ë¬´í•œëŒ€ë¡œ ë°œì‚°í•  ê±±ì •ì€ í•˜ì§€ ì•Šì•„ë„ ë  ê²ƒ ê°™ë‹¤.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-permutation-equivariant&quot;&gt;2. Permutation Equivariant&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Permutation matrixë€&lt;/p&gt;

    &lt;p&gt;Permutation matrixëŠ” ê° í–‰ê³¼ ê° ì—´ë§ˆë‹¤ 1ì´ ë”± í•˜ë‚˜ì”© ìˆëŠ” ì •ì‚¬ê°í–‰ë ¬ì´ë‹¤. ì–´ë–¤ í–‰ë ¬ $A\in \mathbb{R}^{m\times n}$ì— Permutation matrix $P$ë¥¼ ê³±í•˜ë©´ $A$ì˜ í–‰ ë˜ëŠ” ì—´ì˜ ìˆœì„œë¥¼ ë’¤ì£½ë°•ì£½ ì„ì–´ ë†“ì€ ê²ƒê³¼ ê°™ë‹¤. ì¢€ ë” ì •í™•íˆëŠ”, (1) $P\in \mathbb{R}^{n\times n}$ì´ë¼ë©´ $AP$ëŠ” $A$ì˜  ì—´ë“¤ì˜ ìˆœì„œë¥¼ ì„ì–´ë†“ì€ í–‰ë ¬ì´ ë˜ê³ , (2) $P\in \mathbb{R}^{m\times m}$ì´ë¼ë©´ $PA$ëŠ” $A$ì˜  í–‰ë“¤ì˜ ìˆœì„œë¥¼ ì„ì–´ë†“ì€ í–‰ë ¬ì´ ëœë‹¤. ì˜ˆë¥¼ ë“¤ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

\[\begin{pmatrix} 1&amp;amp;2&amp;amp;3 \\ 4&amp;amp;5&amp;amp;6 \\ 7&amp;amp;8&amp;amp;9\end{pmatrix}\begin{pmatrix} 0&amp;amp;1&amp;amp;0 \\ 0&amp;amp;0&amp;amp;1 \\ 1&amp;amp;0&amp;amp;0\end{pmatrix} = \begin{pmatrix} 3&amp;amp;1&amp;amp;2 \\ 6&amp;amp;4&amp;amp;5 \\ 9&amp;amp;7&amp;amp;8\end{pmatrix}\]

    &lt;p&gt;ì°¸ê³ ë¡œ ì´ëŸ¬í•œ permutation matrixëŠ” ì–¸ì œë‚˜ orthogonalí•˜ë‹¤: $P^TP=PP^T=I$. (Pê°€ í–‰/ì—´ì˜ ìˆœì„œë¥¼ ì–´ë–»ê²Œ ì„ëŠ”ì§€ ìƒê°í•´ë³´ì.)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì„ì˜ì˜ $X\in \mathbb{R}^{m\times n}$ì™€ ì„ì˜ì˜ permutation matrix $P\in \mathbb{R}^{n\times n}$ì— ëŒ€í•´ì„œ, Sequence-to-sequence functionì¸ $f$ê°€ $f(XP)=f(X)P$ë¥¼ ë§Œì¡±í•˜ë©´ ì´ëŸ¬í•œ í•¨ìˆ˜ê°€ permutation equivariantí•˜ë‹¤ê³  ë§í•œë‹¤.&lt;/p&gt;

&lt;p&gt;Sequenceì˜ ìˆœì„œë¥¼ ë’¤ì„ëŠ” ì¼ì„ í•¨ìˆ˜ì— ëŒ€ì…í•˜ê¸° ì „ì— í•˜ë‚˜ í›„ì— í•˜ë‚˜ ë‹¬ë¼ì§€ì§€ ì•ŠëŠ” í•¨ìˆ˜ë¥¼ ë§í•œë‹¤ê³  ë³´ë©´ ëœë‹¤.&lt;/p&gt;

&lt;p&gt;ì°¸ê³ ë¡œ ë…¼ë¬¸ì—ì„œëŠ” ê°ê°ì˜ &lt;strong&gt;transformer (encoder) blockì´ permutation equivariantí•œ sequence-to-sequence function&lt;/strong&gt;ì„ì„ ì¦ëª…í•œë‹¤. &lt;strong&gt;(Claim 1)&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-universal-approximation&quot;&gt;3. Universal Approximation&lt;/h2&gt;

&lt;p&gt;ë”¥ëŸ¬ë‹ ì´ë¡ ì˜ ì¶œë°œì ì´ë¼ê³  í•  ë§Œí•œ ì •ë¦¬ë¡œ, Neural networkì˜ expressive powerì— ëŒ€í•´ ì•Œë ¤ì£¼ëŠ” ì •ë¦¬ì¸ â€˜&lt;strong&gt;universal approximation theorem&lt;/strong&gt;â€™ì´ ìˆë‹¤. ì´ê²ƒì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Hidden layerê°€ 1ê°œ ìˆëŠ” neural networkë§Œ ê°€ì§€ê³ ë„ ì•„ë¬´ëŸ° ì—°ì†í•¨ìˆ˜(with compact support)ë¥¼ &lt;strong&gt;ì„ì˜ì˜ (ì•„ì£¼ ì‘ì€) ì˜¤ì°¨ë¡œ ê·¼ì‚¬&lt;/strong&gt;í•  ìˆ˜ ìˆë‹¤. (ë‹¨! networkì˜ widthì—ëŠ” ì œí•œì´ ì—†ìœ¼ë©°, ì¤‘ê°„ì— ìˆëŠ” activation functionì€ ë‹¤í•­í•¨ìˆ˜ê°€ ì•„ë‹˜.)&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;ì´ì²˜ëŸ¼, Universal ApproximatorëŠ” â€˜ì„ì˜ì˜ ì •í™•ë„ë¡œ ì—„ì²­ ë§ì€ í•¨ìˆ˜ë“¤ì„ ê·¼ì‚¬í•  ìˆ˜ ìˆâ€™ëŠ” ëª¨ë¸ì„ ë‘ê³  í•˜ëŠ” ë§ì´ë‹¤.  ì´í›„ë¡œë„ universal approximationì— ëŒ€í•œ ë‹¤ë°©ë©´ì˜ ì—°êµ¬ê°€ ì´ë£¨ì–´ì¡ŒëŠ”ë°, ì´ëŠ” ì—¬ê¸°ì„œ ì†Œê°œí•˜ëŠ” ë…¼ë¬¸ì˜ section 1.2 related works &amp;amp; notationì— ì˜ ì†Œê°œë˜ì–´ ìˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;4-contextual-mapping&quot;&gt;4. Contextual Mapping&lt;/h2&gt;

&lt;p&gt;ë…¼ë¬¸ì— ë”°ë¥´ë©´, Transformerê°€ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ì´ìœ ê°€ ë³´í†µ â€˜contextual mappingâ€™ì„ ì˜ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì´ë¼ê³  í‰ê°€ëœë‹¤ê³  í•œë‹¤. ì¦‰, ê°ê°ì˜ ë¬¸ë§¥ì„ ì„œë¡œ ì˜ êµ¬ë¶„í•˜ëŠ” ëŠ¥ë ¥ì´ íƒì›”í•˜ë‹¤ê³  ë³´ëŠ” ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ë…¼ë¬¸ì—ì„œëŠ” Trasformerì˜ ì´ëŸ°ì €ëŸ° universal approximation ëŠ¥ë ¥ì„ ì¦ëª…í•˜ë ¤ í•˜ëŠ”ë°, ê·¸ ê³¼ì • ì¤‘ì— â€˜(multi-head) self-attention layersê°€ contextual mappingì„ ì˜ ê³„ì‚°í•œë‹¤â€™ëŠ” ê²ƒì„ ì¦ëª…í•˜ëŠ” ê²Œ ì •ë§ ì¤‘ìš”í•œ ì¤‘ê°„ ê³¼ì •ì´ë¼ê³  í•œë‹¤. ì´ë¥¼ ìœ„í•´ ë…¼ë¬¸ì—ì„œëŠ” contextual mappingì˜ ê°œë…ì„ ì•„ì˜ˆ ìˆ˜ì‹ì ìœ¼ë¡œ ì •ì˜í•´ë²„ë¦° ë’¤ì— ì´ë¥¼ ì¦ëª…ì— ì´ìš©í•œë‹¤. ë…¼ë¬¸ì—ì„œ ì£¼ì–´ì§„ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/papers/trasformer-formula.jpeg&quot; alt=&quot;transformer-formula.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì¦‰ contextual mappingì€ ê¸¸ì´ $n$ì¸ input sequenceë¥¼ ë°›ì•„ $n$ê°œì˜ ê°’ (í˜¹ì€ $n$ì°¨ì› ì—´ë²¡í„°)ë¥¼ ë‚´ë†“ëŠ” í•¨ìˆ˜ë¡œ ì •ì˜ëœë‹¤. ì´ë•Œ í•œ ë¬¸ì¥(sequence) ì•ˆì˜ ë‹¨ì–´ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ ì—­í• ì„ í•˜ë¯€ë¡œ ê°ê° ë‹¤ë¥¸ contextê°’(contextual mappingì˜ entry)ì´ ë§¤ê²¨ì§„ë‹¤(1ë²ˆ ì¡°ê±´). ê²Œë‹¤ê°€, ê°™ì€ ë‹¨ì–´ë¼ë„ ë‹¤ë¥¸ ë¬¸ì¥ì—ì„œëŠ” ë‹¤ë¥¸ ì˜ë¯¸ë¡œ í•´ì„ëœë‹¤ëŠ” ì˜ë¯¸ì—ì„œ, ì„œë¡œ ë‹¤ë¥¸ ë‘ input sequence(L, Lâ€™)ì— ëŒ€í•œ contextual mappingì— ìˆëŠ” ëª¨ë“  (ì´ 2nê°œì˜) entryë“¤ì€ ì „ë¶€ ë‹¤ë¥´ê²Œ ë§¤ê²¨ì§„ë‹¤(2ë²ˆ ì¡°ê±´).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ì§‘í•© $\mathbb{L}$ì´ ìœ í•œì§‘í•©ìœ¼ë¡œ ì„¤ì •ëœ ì´ìœ ëŠ” (ë‚´ ìƒê°ì—ëŠ”)&lt;/p&gt;

    &lt;p&gt;Vocabularyì˜ í¬ê¸°ë„ ìœ í•œí•˜ê³  sequence ê¸¸ì´ë„ ìœ í•œí•˜ë¯€ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” input sequenceì˜ ê°œìˆ˜ëŠ” ìœ í•œí•˜ë‹¤. Sequenceë“¤ì˜ ì§‘í•©ê³¼ ëŒ€ì‘ë˜ëŠ” ì§‘í•©ì´ $\mathbb{L}$ê³¼ ë¹„ìŠ·í•œ ê²ƒì´ë¼ë©´, $\mathbb{L}$ì„ ìœ í•œì§‘í•©ì´ë¼ê³  ë†“ì•„ë„ ê´œì°®ì„ ê²ƒì´ë‹¤. (ì´ ì¡°ê±´ì´ í•„ìˆ˜ì¸ì§€ëŠ” ì¦ëª…ì„ ë” ë“¤ì—¬ë‹¤ë´ì•¼..)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h1 id=&quot;main-text&quot;&gt;Main Text&lt;/h1&gt;

&lt;h2 id=&quot;1-universal-approximatorì„ì„-ë³´ì´ê¸°-í˜ë“ -ì´ìœ &quot;&gt;1. Universal Approximatorì„ì„ ë³´ì´ê¸° í˜ë“  ì´ìœ &lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ë„ˆë¬´ ë§ì•„ ë³´ì´ëŠ” Parameter sharing. Self-attention layerì™€ feed-forward layer ëª¨ë‘, tokenë¼ë¦¬ ê³µìœ í•˜ëŠ” parameterì˜ ìˆ˜ê°€ ë§¤ìš° ë§ë‹¤.&lt;/li&gt;
  &lt;li&gt;ë„ˆë¬´ ì ì–´ ë³´ì´ëŠ” token-wise interaction. Self-attention layerì˜ íŠ¹ì„±ìƒ pairwise dot-productë¡œë§Œ token ì‚¬ì´ì˜ interactionì„ ì¡ì•„ë‚¸ë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(ë‘˜ì§¸ ì´ìœ ëŠ” ê·¸ëŸ´ ë§Œí•˜ë‹¤ê³  ë³´ì´ëŠ”ë°, ì²«ì§¸ ì´ìœ ëŠ” ì•„ì§ ì˜ ì´í•´í•˜ì§€ ëª»í–ˆë‹¤.)&lt;/p&gt;

&lt;p&gt;ë…¼ë¬¸ì—ì„œëŠ” ìœ„ì˜ ë‘ ì´ìœ ë¡œ ì¸í•´ transformer encoder ìì²´ê°€ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” sequence-to-sequence í•¨ìˆ˜ì˜ ì¢…ë¥˜ì— ì œí•œì´ ìˆë‹¤ê³  ë³´ë©°, ì´ë¥¼ trainableí•œ positional encodingìœ¼ë¡œ í•´ê²°í•œë‹¤.&lt;/p&gt;

&lt;p&gt;â“ ì¼ë°˜ì ìœ¼ë¡œ, Parameter sharingì´ ë§ì„ìˆ˜ë¡ universal approximatorê°€ ë˜ê¸° ì–´ë ¤ìš´ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;2-ë…¼ë¬¸ì—ì„œ-ë³¸-transformer&quot;&gt;2. ë…¼ë¬¸ì—ì„œ ë³¸ Transformer&lt;/h2&gt;

&lt;p&gt;ì•„ë˜ëŠ” ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ transformer blockì— ëŒ€í•œ ì‹ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/papers/contextual-mapping.jpeg&quot; alt=&quot;contextual-mapping.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì˜ ì•Œë ¤ì ¸ ìˆë“¯, transformer encoder blockì€ multi-head self-attention layer(â€™Attnâ€™)ì™€ token-wise feed-forward layer(â€™FFâ€™)ë¼ëŠ” ë‘ (sub-)layerë¡œ ë‚˜ë‰œë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;21-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ê³µí†µì &quot;&gt;2.1. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜ ê³µí†µì &lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ìˆ˜ì‹ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ residual connectionì€ ê·¸ëŒ€ë¡œ ì‚´ë ¤ë‘ì—ˆë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;22-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ì°¨ì´ì &quot;&gt;2.2. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜ ì°¨ì´ì &lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;í•´ì„ì„ ê°„ë‹¨íˆ í•˜ê¸° ìœ„í•´ layer normalizationì€ ëºë‹¤ê³  í•œë‹¤.&lt;/li&gt;
  &lt;li&gt;Self-attention layer ì‹ì„ ë³´ë©´ ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ë³¼ ìˆ˜ ì—†ë˜ ì‹œê·¸ë§ˆ($\sum$) ê¸°í˜¸ê°€ ë³´ì¸ë‹¤. ì›ë˜ transformer ë…¼ë¬¸ì—ì„œëŠ” attention headë“¤ì„ concatenateí•˜ëŠ”ë°, ì´ëŸ¬í•œ concatenationì„ ìˆ˜ì‹ì ìœ¼ë¡œëŠ” ì €ë ‡ê²Œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì¦‰ ì˜ë¯¸ê°€ ë‹¤ë¥¸ ì‹ì´ ì•„ë‹ˆë‹¤.&lt;/li&gt;
  &lt;li&gt;Self-attention layerì˜ ì†Œë¬¸ì ì‹œê·¸ë§ˆ í•¨ìˆ˜($\sigma(\cdot)$)ëŠ” (column-wise) softmaxë¥¼ ê°€ë¦¬í‚¨ë‹¤. ê·¸ëŸ°ë° ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” scaled dot-product attentionì„ ì‚¬ìš©í•˜ëŠ” ë°˜ë©´ ì—¬ê¸°ì„œëŠ” ê·¸ëƒ¥ dot-product attentionì„ ì“°ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì¸ë‹¤. ì‚¬ì‹¤ $\boldsymbol{W}_K$ë‚˜ $\boldsymbol{W}_Q$ê°™ì€ parameterë“¤ì´ ê·¸ scaling factor($\frac{1}{\sqrt{d_k}}$)ë¥¼ í•™ìŠµí•˜ë©´ ê·¸ë§Œì´ë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;â“ Layer normalizationì„ ë¹¼ë„ ê´œì°®ì€ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?&lt;/p&gt;

&lt;h3 id=&quot;23-positional-encoding&quot;&gt;2.3. Positional encoding&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Trainableí•œ positional encodingì´ ì—†ëŠ” ìˆœìˆ˜í•œ transformer blockì€ ì˜¤ì§ â€˜permutation equivariantâ€™í•œ ì¢…ë¥˜ì˜ í•¨ìˆ˜ë§Œì„ ì˜ ê·¼ì‚¬í•  ë¿ì´ë‹¤. ê·¸ëŸ¬ë‚˜ positional encodingì„ ë„ì…í•¨ìœ¼ë¡œì¨ ì´ëŸ¬í•œ í•¨ìˆ˜ ì¢…ë¥˜ì˜ ì œí•œ ì—†ì´ ì•„ë¬´ëŸ° sequence-to-sequence í•¨ìˆ˜(with compact domain)ì„ ì˜ ê·¼ì‚¬í•  ìˆ˜ ìˆê²Œ ëœë‹¤.&lt;/li&gt;
  &lt;li&gt;Positional encoding $\boldsymbol{E}$ ì—­ì‹œ $d\times n$ í¬ê¸°ì˜ real matrixë¡œ ì •ì˜ëœë‹¤. Transformer blockì„ í•¨ìˆ˜ $g$ë¡œ ì“´ë‹¤ë©´, positional encodingì´ ë„ì…ëœ transformer blockì€ input sequence $\boldsymbol{X}$ì— ëŒ€í•´ $g(\boldsymbol{X}+\boldsymbol{E})$ë¼ê³  ì“¸ ìˆ˜ ìˆë‹¤.&lt;/li&gt;
  &lt;li&gt;ë…¼ë¬¸ì—ì„œëŠ” ì´ $\boldsymbol{E}$ê°€ trainableí•˜ë‹¤ê³  ê°€ì •í•˜ë¯€ë¡œ ì•„ë¬´ë ‡ê²Œë‚˜ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œ, í•¨ìˆ˜ë“¤ì˜ domainì´ compactí•¨ì„ ê°€ì •í•´ì„œ input sequenceê°€ $\boldsymbol{X}\in [0,1]^{d\times n}$ ê°€ ë˜ë„ë¡ í•œ ë’¤, positional encodingì„ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬ì„ ì„ì˜ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤. (Appendix C ì°¸ê³ )&lt;/li&gt;
&lt;/ul&gt;

\[\boldsymbol{E} = \begin{pmatrix} 0&amp;amp;1&amp;amp;2&amp;amp;\cdots&amp;amp;n-1\\0&amp;amp;1&amp;amp;2&amp;amp;\cdots&amp;amp;n-1\\\vdots&amp;amp;\vdots&amp;amp;\vdots&amp;amp;&amp;amp;\vdots\\0&amp;amp;1&amp;amp;2&amp;amp;\cdots&amp;amp;n-1\end{pmatrix}\]

&lt;hr /&gt;

&lt;h2 id=&quot;3-ì£¼ìš”-ê²°ê³¼-2ê°€ì§€&quot;&gt;3. ì£¼ìš” ê²°ê³¼ (2ê°€ì§€)&lt;/h2&gt;

&lt;p&gt;ë…¼ë¬¸ì´ ì£¼ì¥í•˜ëŠ” ë‘ ê°€ì§€ ì¤‘ìš”í•œ ê²°ê³¼ëŠ” Abstractì—ì„œ ì†Œê°œí•œ ì²˜ìŒ ë‘ ì¤„ê³¼ ê°™ë‹¤. ì—¬ê¸°ì„œëŠ” ë” ìì„¸í•œ ì„œìˆ ì„ ì†Œê°œí•œë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;31-theorem-2&quot;&gt;3.1. Theorem 2&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;(ì„ì˜ì˜  $\epsilon&amp;gt;0$ì™€ $1\le p &amp;lt; \infty$ì— ëŒ€í•´) í•¨ìˆ˜ $f$ê°€ ë‹¤ìŒì˜ ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤ê³  í•˜ì.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$f$ëŠ” sequence-to-sequence í•¨ìˆ˜.&lt;/li&gt;
  &lt;li&gt;$f$ì˜ supportëŠ” compact.&lt;/li&gt;
  &lt;li&gt;$f$ëŠ” ì—°ì†(w.r.t. entry-wise $\ell^p$ norm).&lt;/li&gt;
  &lt;li&gt;$f$ëŠ” &lt;strong&gt;permutation equivariant&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ê·¸ëŸ¬ë©´ ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” Transformer network $g$ê°€ ì¡´ì¬í•œë‹¤.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$g$ëŠ” $(h,m,r)=(2,1,4)$ë¥¼ ë§Œì¡±.&lt;/li&gt;
  &lt;li&gt;$d_p (f,g ) \le \epsilon$.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;ì°¸ê³ : Transformer networkë€, ê°™ì€ Transformer blockì„ ì—¬ëŸ¬ ê°œ ìŒ“ì€ ê²ƒì´ë‹¤. ë˜ ìœ„ì—ì„œ ì“°ì¸ h, m, rì€ ê°ê° ë‹¤ìŒê³¼ ê°™ì€ ê²ƒì„ ë‚˜íƒ€ë‚´ëŠ” ê¸°í˜¸ë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ë¬¸ì&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ëœ»&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$h$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;attention headì˜ ê°œìˆ˜&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$m$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;attention headì˜ í¬ê¸°&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;$r$&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;feed-forward layerì˜ hidden ì°¨ì› (=$d_{ff}$)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;32-theorem-3&quot;&gt;3.2. Theorem 3&lt;/h3&gt;

&lt;hr /&gt;
&lt;p&gt;(ì„ì˜ì˜  $\epsilon&amp;gt;0$ì™€ $1\le p &amp;lt; \infty$ì— ëŒ€í•´) í•¨ìˆ˜ $f$ê°€ ë‹¤ìŒì˜ ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤ê³  í•˜ì.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$f$ëŠ” sequence-to-sequence í•¨ìˆ˜.&lt;/li&gt;
  &lt;li&gt;$f$ì˜ domainì€ compact.&lt;/li&gt;
  &lt;li&gt;$f$ëŠ” ì—°ì†(w.r.t. entry-wise $\ell^p$ norm).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ê·¸ëŸ¬ë©´ ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” &lt;strong&gt;Transformer network $g$ with (trainable) positional encoding $\boldsymbol{E}$&lt;/strong&gt;ê°€ ì¡´ì¬í•œë‹¤.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$g$ëŠ” $(h,m,r)=(2,1,4)$ë¥¼ ë§Œì¡±.&lt;/li&gt;
  &lt;li&gt;$d_p (f,g ) \le \epsilon$.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;ê±°ì˜ ëª¨ë“  ê²ƒì´ Theorem 2ì™€ ë™ì¼í•˜ì§€ë§Œ, Transformer networkì—ëŠ” positional encodingì´ ì¶”ê°€ëê³ , ëŒ€ì‹  ê·¼ì‚¬í•˜ë ¤ëŠ” sequence-to-sequence í•¨ìˆ˜ì˜ permutation equivariant ì¡°ê±´ì´ ì‚¬ë¼ì¡Œë‹¤.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$(h,m,r)=(2,1,4)$ë¥¼ ì“°ëŠ” ì´ìœ ? (ë„ˆë¬´ ì‘ì€ block ì•„ë‹Œê°€?)&lt;/p&gt;

    &lt;p&gt;Attention headê°€ 2ê°œë°–ì— ì—†ê³ , ê·¸ í¬ê¸°ë„ ê²¨ìš° 1ì´ê³ , ì‹¬ì§€ì–´ feed-forward layerì˜ hidden ì°¨ì›ì´ 4ë°–ì— ì•ˆ ë˜ëŠ” ì‘ì€ Transformer blockì€ ì‹¤ì§ˆì ìœ¼ë¡œ ì“°ì´ì§€ ì•ŠëŠ”ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ Transformer blockì„ ì´ìš©í•œ ì´ìœ ëŠ” ë‹¨ì§€ ë‹¨ìˆœí™”ê°€ ì¦ëª…ì„ ì‰½ê²Œ í•´ì£¼ê¸° ë•Œë¬¸ë§Œì€ ì•„ë‹ˆë‹¤.&lt;/p&gt;

    &lt;p&gt;ë” í° ëª¨ë¸ì€ ìëª…í•˜ê²Œ expressive powerê°€ ë” í¬ê¸° ë•Œë¬¸ì´ë‹¤. ì‹¤ì§ˆì ìœ¼ë¡œ ì“°ì´ëŠ” transformer blockì€ í›¨ì”¬ ë” ë§ì€ parameterë¥¼ ì“¸ í…ë°, ê·¸ëŸ° modelì€ ë…¼ë¬¸ì—ì„œ ì“°ì´ëŠ” ë§¤ìš° ì‘ì€ transformer blockì— ë¹„í•˜ë©´ ë‹¹ì—°íˆ ë”ìš±ë” ë§ì€ í•¨ìˆ˜ë“¤ì„ í‘œí˜„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‹ˆ ì´ë ‡ê²Œ ì‘ì€ ìŠ¤ì¼€ì¼ë¡œ ë¬¸ì œë¥¼ ì¶•ì†Œì‹œì¼œì„œ ë¬¸ì œë¥¼ í’€ì–´ë„ ì¶©ë¶„í•˜ë‹¤.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;â“ ìœ„ì˜ ë‘ ì •ë¦¬ëŠ” universal approximationì˜ ì¸¡ë©´ì—ì„œ ë§¤ìš° ìœ ì˜ë¯¸í•œ ê²°ê³¼ë¥¼ ë‚´ê³  ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ëª¨ë‘ ì¡´ì¬ì„± ì •ë¦¬ì¸ íƒ“ì—, í›ˆë ¨ ê³¼ì •ì—ì„œ transformerê°€ â€˜ìš°ë¦¬ê°€ ì›í•˜ëŠ” í•¨ìˆ˜â€™ë¥¼ ì‹¤ì œë¡œ ì˜ ê·¼ì‚¬í•  ìˆ˜ ìˆëŠ”ì§€ëŠ” ë§í•´ì£¼ì§€ ì•ŠëŠ” ê²Œ ë¶„ëª…í•˜ë‹¤. ì´ê²ƒì´ ê°€ëŠ¥í•œì§€ëŠ” ì–´ë–»ê²Œ ì—°êµ¬í•´ì•¼ í• ê¹Œ?/ ì–´ë–»ê²Œ ì—°êµ¬ë˜ê³  ìˆì„ê¹Œ?&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;4-ì–´ë–»ê²Œ-ì¦ëª…í•˜ë‚˜&quot;&gt;4. ì–´ë–»ê²Œ ì¦ëª…í•˜ë‚˜?&lt;/h2&gt;

&lt;p&gt;Theorem 2ì™€ Theorem 3ì˜ ì¦ëª…ì€ ë§¤ìš° ìœ ì‚¬í•˜ë©°, ë³¸ë¬¸ì—ì„œëŠ” Theorem 2ì˜ ì¦ëª…ê³¼ì •ì„ ìš”ì•½í•˜ì—¬ ì„¤ëª…í•œë‹¤. ì„¸ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ì„ì˜ì˜ continuous, permutation equivariant, sequence-to-sequence function $f$ with compact supportë¥¼ ì ì ˆí•œ Transformer networkë¡œ ê·¼ì‚¬í•œë‹¤. ê·¸ ë¡œë“œë§µì€ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;1-fë¥¼-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¡œ-ê·¼ì‚¬í•˜ê¸°&quot;&gt;1) $f$ë¥¼ piece-wise ìƒìˆ˜í•¨ìˆ˜ë¡œ ê·¼ì‚¬í•˜ê¸°&lt;/h3&gt;

&lt;p&gt;ìƒìˆ˜í•¨ìˆ˜ë¼ê³  í•´ì„œ fê°€ ê°‘ìê¸° real-valuedê°€ ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤. ì—¬ê¸°ì„œì˜ ìƒìˆ˜í•¨ìˆ˜ ì—­ì‹œ í–‰ë ¬ì„ ë°›ì•„ í–‰ë ¬ì„ ë‚´ë±‰ëŠ” í•¨ìˆ˜ì¸ë°, í•¨ìˆ«ê°’ìœ¼ë¡œì„œì˜ í–‰ë ¬ì´ ê³ ì •ë˜ì–´ ìˆìœ¼ë©´ ìƒìˆ˜í•¨ìˆ˜ì¸ ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;2-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¥¼-modified-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°&quot;&gt;2) Piece-wise ìƒìˆ˜í•¨ìˆ˜ë¥¼ â€˜modifiedâ€™ Transformer networkë¡œ ê·¼ì‚¬í•˜ê¸°&lt;/h3&gt;

&lt;p&gt;â€˜Modifiedâ€™ Transformerë€, ê¸°ì¡´ì˜ Transformerì—ì„œ ì“°ì´ë˜ (column-wise) softmax í•¨ìˆ˜($\sigma$)ëŠ” column-wise hardmax($\sigma_H$)ë¡œ ëŒ€ì²´í•˜ê³ , FFì˜ activation functionìœ¼ë¡œ ì“°ì´ë˜ ReLUëŠ” ë˜ë‹¤ë¥¸ íŠ¹ì´í•œ í•¨ìˆ˜($\phi \in \Phi$, ìì„¸í•œ ì •ì˜ëŠ” ì•„ë˜ì—)ë¡œ ëŒ€ì²´í•œ ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$\Phi$ì˜ ì •ì˜&lt;/p&gt;

    &lt;p&gt;The set of all piece-wise linear functions with at most three pieces, where at least one piece is constant. (p.9)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì´ ë¶€ë¶„ì„ ì¦ëª…í•˜ê¸° ìœ„í•´, ë…¼ë¬¸ì—ì„œëŠ” modified Transformerì˜ layer ìˆœì„œë¥¼ ëœ¯ì–´ê³ ì¹˜ëŠ” ì¼ì„ í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. Residual connectionì„ ì´ìš©í•˜ë©´, self-attentionê³¼ feed-forward layerë¥¼ ë²ˆê°ˆì•„ ì ìš©í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ self-attentionë§Œ ì­‰, í˜¹ì€ feed-forward layerë§Œ ì­‰ ì´ì–´ í•©ì„±í•œ ê²ƒì„ í™œìš©í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(â€¦.) we note that even though a Transformer network stacks self-attention and feed-forward layers in an alternate manner, &lt;strong&gt;the skip connections enable these networks to employ a composition of multiple self-attention or feed-forward layers.&lt;/strong&gt; (ì¤‘ëµ) self-attention and feed-forward layers play in realizing the ability to universally approximate sequence-to-sequence functions: 1) self-attention layers compute precise contextual maps; and 2) feed-forward layers then assign the results of these contextual maps to the desired output values. (p.6)&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;â“ Modified Transformer networkì˜ layer ìˆœì„œë¥¼ ë’¤ë°”ê¾¸ì–´ ê°™ì€ ì¢…ë¥˜ì˜ layerë§Œ ì´ì–´ë¶™ì¼ ìˆ˜ ìˆëŠ” ì´ìœ ê°€ êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì¼ê¹Œ? ì—¬ê¸°ì— skip connectionì€ ì–´ë–¤ ì—­í• ì„ í• ê¹Œ?&lt;/p&gt;

&lt;h3 id=&quot;3-modified-transformer-networkë¥¼-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°&quot;&gt;3) Modified Transformer networkë¥¼ Transformer networkë¡œ ê·¼ì‚¬í•˜ê¸°&lt;/h3&gt;

&lt;p&gt;ì•ì—ì„œ ëŒ€ì²´í–ˆë˜ softmaxì™€ ReLUë¥¼ ì›ë˜ëŒ€ë¡œ ëŒë ¤ë†“ëŠ” ì‘ì—…ì´ë¼ê³  ë³´ë©´ ë  ê²ƒ ê°™ë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;5-ëª‡-ê°œì˜-blockì„-ìŒ“ì•„ì•¼-í•˜ë‚˜&quot;&gt;5. ëª‡ ê°œì˜ blockì„ ìŒ“ì•„ì•¼ í•˜ë‚˜?&lt;/h2&gt;

&lt;p&gt;Theorem 2ëŠ” ê²°ê³¼ì ìœ¼ë¡œ ëª‡ê°œì˜ Transformer blockì„ ìŒ“ì•„ì•¼ í•˜ëŠ”ì§€ ë³´ì—¬ì¤€ë‹¤. ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ”, permutation equivariant í•¨ìˆ˜ë¥¼ ì˜ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ í•„ìš”í•œ (h,m,r)=(2,1,4) Transformer blockì€ ì´ $O(n(1/\delta)^{dn}/n!)$ê°œë‹¤. ë˜í•œ, positional encodingê¹Œì§€ ë”í•´ ì¢€ ë” ê´‘ë²”ìœ„í•œ sequence-to-sequence í•¨ìˆ˜ë¥¼ ì˜ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ì„œ í•„ìš”í•œ blockì€ $O(n(1/\delta)^{dn})$ê°œë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ë•Œ $\delta$ëŠ” Theorem 2/3ì˜ ì¦ëª… 1~2ë‹¨ê³„ì—ì„œ ì“°ì¸ piecewise constant functionì˜ domainì„ êµ¬ë¶„í•˜ëŠ” gridë¥¼ ì´ë£¨ëŠ” (hyper-)cubeì˜ í•œ ë³€ì˜ ê¸¸ì´ì´ë©°, ì¶©ë¶„íˆ ì‘ìŒì„ ê°€ì •í•´ì•¼ í•œë‹¤. (ì¦ëª…ê³¼ì •ì— ë”°ë¥´ë©´, $O(\delta^{d/p} ) \le \epsilon/3)$&lt;/p&gt;

&lt;p&gt;â“ ë…¼ë¬¸ì—ì„œëŠ” ì¦ëª…ì„ ìœ„í•´ ì•„ì£¼ ì‘ì€ transformer blockì„ ì´ìš©í•˜ê³  ìˆë‹¤. ë§Œì•½ ì´ transformer blockì˜ í¬ê¸°ë¥¼ í‚¤ìš´ë‹¤ë©´ í•„ìš”í•œ blockì˜ ìˆ˜ëŠ” ì¤„ì–´ë“¤ê¹Œ? (ì•„ë§ˆ $d$ì™€ $n$ì— ë”°ë¥¸ complexityì—ëŠ” í¬ê²Œ ì°¨ì´ê°€ ìˆì§€ ì•Šì„ ê²ƒ ê°™ë‹¤. $h$, $m$, $r$ ë“±ì˜ ê°’ì€ $d$ë‚˜ $n$ì˜ ê°’ê³¼ëŠ” ê´€ë ¨ì´ ì—†ìœ¼ë¯€ë¡œ.)&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h1 id=&quot;my-comments--questions&quot;&gt;My Comments &amp;amp; Questions&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;ì„ í˜•ëŒ€ìˆ˜í•™ì„ ê½¤ë‚˜ ì“°ëŠ” ë…¼ë¬¸ì´ì§€ë§Œ ì‹¤ìƒì€ ì—„ì²­ë‚˜ê²Œ í•´ì„í•™ìŠ¤ëŸ¬ìš´ ë…¼ë¬¸ì´ì—ˆë‹¤. í•´ì„í•™1ë•Œ Weierstrass Approximation Theorem(compact domainì—ì„œ ì—°ì†í•¨ìˆ˜ë¥¼ ë‹¤í•­ì‹ìœ¼ë¡œ ì„ì˜ì˜ ì •í™•ë„ë¡œ ê·¼ì‚¬í•˜ê¸°) ë°°ì› ë˜ ê²ƒì´ ìƒˆë¡ìƒˆë¡â€¦&lt;/li&gt;
  &lt;li&gt;ìœ„ì—ì„œ ë˜ì¡Œë˜ ì§ˆë¬¸ë“¤ì€ ë‚´ê°€ ë…¼ë¬¸ì„ ì½ìœ¼ë©´ì„œë„ ëê¹Œì§€ ì´í•´í•˜ì§€ ëª»í–ˆë˜, í˜¹ì€ ìŠ¤ìŠ¤ë¡œ ë§Œì¡±ìŠ¤ëŸ½ê²Œ ëŒ€ë‹µí•˜ì§€ ëª»í–ˆë˜ ëŒ€í‘œì ì¸ ì§ˆë¬¸ë“¤ì´ë‹¤. í•œ ë²ˆ ë” ëª¨ì•„ë³´ìë©´ ì•„ë˜ì™€ ê°™ë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;â“ ì¼ë°˜ì ìœ¼ë¡œ, Parameter sharingì´ ë§ì„ìˆ˜ë¡ universal approximatorê°€ ë˜ê¸° ì–´ë ¤ìš´ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?&lt;/p&gt;

&lt;p&gt;â“ Layer normalizationì„ ë¹¼ë„ ê´œì°®ì€ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?&lt;/p&gt;

&lt;p&gt;â“ (Paraphrased:) í›ˆë ¨ ê³¼ì •ì—ì„œ transformerê°€ â€˜ìš°ë¦¬ê°€ ì›í•˜ëŠ” í•¨ìˆ˜â€™ë¥¼ ì‹¤ì œë¡œ ì˜ ê·¼ì‚¬í•  ìˆ˜ ìˆëŠ”ì§€ëŠ” ì–´ë–»ê²Œ ì•Œ ìˆ˜ ìˆì„ê¹Œ?&lt;/p&gt;

&lt;p&gt;â“ Modified Transformer networkì˜ layer ìˆœì„œë¥¼ ë’¤ë°”ê¾¸ì–´ ê°™ì€ ì¢…ë¥˜ì˜ layerë§Œ ì´ì–´ë¶™ì¼ ìˆ˜ ìˆëŠ” ì´ìœ ê°€ êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì¼ê¹Œ? ì—¬ê¸°ì— skip connectionì€ ì–´ë–¤ ì—­í• ì„ í• ê¹Œ?&lt;/p&gt;

&lt;p&gt;â“ ë…¼ë¬¸ì—ì„œëŠ” ì¦ëª…ì„ ìœ„í•´ ì•„ì£¼ ì‘ì€ transformer blockì„ ì´ìš©í•˜ê³  ìˆë‹¤. ë§Œì•½ ì´ transformer blockì˜ í¬ê¸°ë¥¼ í‚¤ìš´ë‹¤ë©´ í•„ìš”í•œ blockì˜ ìˆ˜ëŠ” ì¤„ì–´ë“¤ê¹Œ?&lt;/p&gt;
</description>
        <pubDate>Sun, 30 Jan 2022 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions/</link>
        <guid isPermaLink="true">http://localhost:4000/papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions/</guid>
        
        <category>theory</category>
        
        
        <category>papers</category>
        
      </item>
    
      <item>
        <title>ì½ê³  ì‹¶ì€ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸!</title>
        <description>&lt;h2 id=&quot;ë…¼ë¬¸-ë¦¬ìŠ¤íŠ¸-ë§í¬&quot;&gt;&lt;strong&gt;ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸: &lt;a href=&quot;https://han-5eu1.notion.site/d7c13438fa8c41218c2bfc8fd0bcebd6?v=165339f3e0e5475d81fd60e1112b4273&quot;&gt;ë§í¬&lt;/a&gt;&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;ì œ ê°œì¸ ë…¸ì…˜ í˜ì´ì§€ì— ì½ê³  ì‹¶ì€ ë…¼ë¬¸ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê¾¸ì¤€íˆ ì—…ë°ì´íŠ¸ í•´ë³´ë ¤ í•©ë‹ˆë‹¤. ë¬¼ë¡  ì½ì€ ê²ƒë³´ë‹¤ ì•ˆ ì½ì€ ë…¼ë¬¸ì´ ë” ë§ìŠµë‹ˆë‹¤ í•˜í•˜&lt;/p&gt;
</description>
        <pubDate>Sat, 29 Jan 2022 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/papers/2022/01/29/paper-list/</link>
        <guid isPermaLink="true">http://localhost:4000/papers/2022/01/29/paper-list/</guid>
        
        
        <category>papers</category>
        
      </item>
    
  </channel>
</rss>
