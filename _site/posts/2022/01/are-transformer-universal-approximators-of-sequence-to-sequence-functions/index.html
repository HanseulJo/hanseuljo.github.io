

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>[ë…¼ë¬¸ê³µë¶€] Are Transformers universal approximators of sequence-to-sequence functions? - Hanseul Cho</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Hanseul Cho">
<meta property="og:title" content="[ë…¼ë¬¸ê³µë¶€] Are Transformers universal approximators of sequence-to-sequence functions?">


  <link rel="canonical" href="http://localhost:4000/posts/2022/01/are-transformer-universal-approximators-of-sequence-to-sequence-functions/">
  <meta property="og:url" content="http://localhost:4000/posts/2022/01/are-transformer-universal-approximators-of-sequence-to-sequence-functions/">



  <meta property="og:description" content="ğŸ“Œ í•œ ì¤„ ìš”ì•½: Transformerì˜ expressive powerë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë³´ì¸ ì²« ë…¼ë¬¸">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2022-01-30T00:00:00-08:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Hanseul Cho",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Hanseul Cho Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->
<link rel="apple-touch-icon" sizes="180x180" href="/assets/icon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/icon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/icon/favicon-16x16.png">
<link rel="shortcut icon" href="/assets/icon/favicon.ico">
<link rel="manifest" href="/assets/icon/site.webmanifest">

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->
  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="http://localhost:4000/">Hanseul Cho</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/about/">About</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/posts/">Posts</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="http://localhost:4000/images/me.jpeg" class="author__avatar" alt="Hanseul Cho">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Hanseul Cho</h3>
    <p class="author__bio">MSc Student at OptiML Lab in KAIST AI.</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Seoul, South Korea</li>
      
      
      
      
      
       
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/HanseulJo"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="[ë…¼ë¬¸ê³µë¶€] Are Transformers universal approximators of sequence-to-sequence functions?">
    <meta itemprop="description" content="ğŸ“Œ í•œ ì¤„ ìš”ì•½: Transformerì˜ expressive powerë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë³´ì¸ ì²« ë…¼ë¬¸">
    <meta itemprop="datePublished" content="January 30, 2022">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">[ë…¼ë¬¸ê³µë¶€] Are Transformers universal approximators of sequence-to-sequence functions?
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  11 minute read
	
</p>
          
        

        <!--Authors-->
        
        
        <!---Venue, Date-->
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2022-01-30T00:00:00-08:00">January 30, 2022</time></p>
        
        
        <!--URL-->
        

        
    
        </header>
      

      <section class="page__content" itemprop="text">
        <p>ğŸ“Œ í•œ ì¤„ ìš”ì•½: <strong>Transformerì˜ expressive powerë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë³´ì¸ ì²« ë…¼ë¬¸</strong></p>

<p>ğŸ¤” ì°¸ê³ : ë…¸ì…˜ìœ¼ë¡œ ì‘ì„±í•œ <a href="https://han-5eu1.notion.site/Are-Transformers-universal-approximators-of-sequence-to-sequence-functions-158eac79332a4d81b1b7cccff9b1b0ce">ì›ë¬¸</a>ì„ ì˜®ê²¨ì˜¨ ê²ƒì…ë‹ˆë‹¤.</p>

<ul id="markdown-toc">
  <li><a href="#abstract" id="markdown-toc-abstract">Abstract</a></li>
  <li><a href="#keywords--definitions" id="markdown-toc-keywords--definitions">Keywords &amp; Definitions</a>    <ul>
      <li><a href="#1-sequence-to-sequence-function" id="markdown-toc-1-sequence-to-sequence-function">1. Sequence-to-sequence Function</a></li>
      <li><a href="#2-permutation-equivariant" id="markdown-toc-2-permutation-equivariant">2. Permutation Equivariant</a></li>
      <li><a href="#3-universal-approximation" id="markdown-toc-3-universal-approximation">3. Universal Approximation</a></li>
      <li><a href="#4-contextual-mapping" id="markdown-toc-4-contextual-mapping">4. Contextual Mapping</a></li>
    </ul>
  </li>
  <li><a href="#main-text" id="markdown-toc-main-text">Main Text</a>    <ul>
      <li><a href="#1-universal-approximatorì„ì„-ë³´ì´ê¸°-í˜ë“ -ì´ìœ " id="markdown-toc-1-universal-approximatorì„ì„-ë³´ì´ê¸°-í˜ë“ -ì´ìœ ">1. Universal Approximatorì„ì„ ë³´ì´ê¸° í˜ë“  ì´ìœ </a></li>
      <li><a href="#2-ë…¼ë¬¸ì—ì„œ-ë³¸-transformer" id="markdown-toc-2-ë…¼ë¬¸ì—ì„œ-ë³¸-transformer">2. ë…¼ë¬¸ì—ì„œ ë³¸ Transformer</a>        <ul>
          <li><a href="#21-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ê³µí†µì " id="markdown-toc-21-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ê³µí†µì ">2.1. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜ ê³µí†µì </a></li>
          <li><a href="#22-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ì°¨ì´ì " id="markdown-toc-22-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ì°¨ì´ì ">2.2. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜ ì°¨ì´ì </a></li>
          <li><a href="#23-positional-encoding" id="markdown-toc-23-positional-encoding">2.3. Positional encoding</a></li>
        </ul>
      </li>
      <li><a href="#3-ì£¼ìš”-ê²°ê³¼-2ê°€ì§€" id="markdown-toc-3-ì£¼ìš”-ê²°ê³¼-2ê°€ì§€">3. ì£¼ìš” ê²°ê³¼ (2ê°€ì§€)</a>        <ul>
          <li><a href="#31-theorem-2" id="markdown-toc-31-theorem-2">3.1. Theorem 2</a></li>
          <li><a href="#32-theorem-3" id="markdown-toc-32-theorem-3">3.2. Theorem 3</a></li>
        </ul>
      </li>
      <li><a href="#4-ì–´ë–»ê²Œ-ì¦ëª…í•˜ë‚˜" id="markdown-toc-4-ì–´ë–»ê²Œ-ì¦ëª…í•˜ë‚˜">4. ì–´ë–»ê²Œ ì¦ëª…í•˜ë‚˜?</a>        <ul>
          <li><a href="#1-fë¥¼-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¡œ-ê·¼ì‚¬í•˜ê¸°" id="markdown-toc-1-fë¥¼-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¡œ-ê·¼ì‚¬í•˜ê¸°">1) $f$ë¥¼ piece-wise ìƒìˆ˜í•¨ìˆ˜ë¡œ ê·¼ì‚¬í•˜ê¸°</a></li>
          <li><a href="#2-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¥¼-modified-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°" id="markdown-toc-2-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¥¼-modified-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°">2) Piece-wise ìƒìˆ˜í•¨ìˆ˜ë¥¼ â€˜modifiedâ€™ Transformer networkë¡œ ê·¼ì‚¬í•˜ê¸°</a></li>
          <li><a href="#3-modified-transformer-networkë¥¼-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°" id="markdown-toc-3-modified-transformer-networkë¥¼-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°">3) Modified Transformer networkë¥¼ Transformer networkë¡œ ê·¼ì‚¬í•˜ê¸°</a></li>
        </ul>
      </li>
      <li><a href="#5-ëª‡-ê°œì˜-blockì„-ìŒ“ì•„ì•¼-í•˜ë‚˜" id="markdown-toc-5-ëª‡-ê°œì˜-blockì„-ìŒ“ì•„ì•¼-í•˜ë‚˜">5. ëª‡ ê°œì˜ blockì„ ìŒ“ì•„ì•¼ í•˜ë‚˜?</a></li>
    </ul>
  </li>
  <li><a href="#my-comments--questions" id="markdown-toc-my-comments--questions">My Comments &amp; Questions</a></li>
</ul>

<h2 id="abstract">Abstract</h2>

<ul>
  <li><strong>Transformer encoder</strong>ëŠ” <strong>â€˜permutation equivariant</strong>â€™í•œ ì„±ì§ˆì„ ê°–ëŠ” <strong>ì—°ì†</strong>ì¸ â€˜<strong>sequence-to-sequence</strong>â€™ í•¨ìˆ˜(with compact support)ì— ëŒ€í•œ universal approximatorì„ì„ ë³´ì¸ë‹¤.</li>
  <li>Transformer encoderì—ë‹¤ <strong>learnableí•œ positional encodingsë¥¼ ê°™ì´ ì“°ë©´</strong> <strong>ì„ì˜ì˜</strong>(permutation equivariantí•˜ì§€ ì•Šì•„ë„) <strong>ì—°ì†</strong>ì¸ â€˜<strong>sequence-to-sequence</strong>â€™ í•¨ìˆ˜(with compact domain)ë¥¼ universally approximateí•¨ì„ ë³´ì¸ë‹¤.</li>
  <li>Contextual mappingì´ë¼ëŠ” ê²ƒì„ ìˆ˜ì‹ì ìœ¼ë¡œ ì •ì˜í–ˆìœ¼ë©°, Transformer Encoderì˜ <strong>multi-head</strong> <strong>self-attention layerë“¤ì´ ì…ë ¥ sequenceì— ëŒ€í•œ contextual mappingì„ ì˜ ê³„ì‚°í•¨</strong>ì„ ë³´ì¸ë‹¤.</li>
  <li>(ì‹¤í—˜ë„ ì§„í–‰í•˜ì˜€ìœ¼ë‚˜ ì—¬ê¸°ì„œëŠ” ìƒëµ)</li>
</ul>

<hr />
<hr />

<h2 id="keywords--definitions">Keywords &amp; Definitions</h2>

<h3 id="1-sequence-to-sequence-function">1. Sequence-to-sequence Function</h3>

<p>$\mathbb{R}^{d\times n}$ì—ì„œ $\mathbb{R}^{d\times n}$ë¡œ ê°€ëŠ” í•¨ìˆ˜ë¥¼ <strong>sequence-to-sequence</strong> functionì´ë¼ê³  ë§í•œë‹¤. ì •í™•íˆëŠ” ì •ì˜ì—­ë„ ì¹˜ì—­ë„ ëª¨ë‘ subset of $\mathbb{R}^{d\times n}$ì¸ í•¨ìˆ˜ë¥¼ ë§í•œë‹¤. ($\mathbb{R}^{d\times n}$: the set of all $d\times n$  real matrices)</p>

<p>ì´ë•Œ $d$ì™€ $n$ì€ ê°ê°, <a href="https://arxiv.org/abs/1706.03762">Transformer ë…¼ë¬¸</a>ì—ì„œ ì–¸ê¸‰í•˜ëŠ” embedding ì°¨ì›ê³¼ ì…ë ¥ sequence ê¸¸ì´ë¡œ ë¹„ìœ ëœë‹¤. ê¸°ì¡´ Transformer ë…¼ë¬¸ì—ì„œë„ ê±°ì˜ ê°™ì€ í‘œê¸°ë¥¼ ì‚¬ìš©í–ˆë‹¤($d_{\text{model}} = d$). í•œ ê°€ì§€ ì°¨ì´ê°€ ìˆë‹¤ë©´, Transformer ë…¼ë¬¸ì—ì„œëŠ” $n\times d$ í–‰ë ¬ì„ ì“°ëŠ” ë°˜ë©´, ì´ ë…¼ë¬¸ì—ì„œëŠ” ê·¸ ë°˜ëŒ€($d\times n$ í–‰ë ¬)ë¥¼ ì´ìš©í•˜ê¸° ë•Œë¬¸ì—, í–‰ë ¬ì˜ ê° ì—´(column)ì´ í•œ input word embedding(í˜¹ì€ token)ìœ¼ë¡œ ë¹„ìœ ëœë‹¤. ì•ˆê·¸ë˜ë„ ì´ ë…¼ë¬¸ì—ì„œ ê³„ì†í•´ì„œ $d\times n$ í–‰ë ¬ $X$ë¥¼ input sequenceë¼ê³  ì¹­í•œë‹¤.</p>

<ul>
  <li>
    <p>Sequence-to-sequence í•¨ìˆ˜ì˜ ì—°ì†ì„± ì •ì˜</p>

    <p>Sequence-to-sequence functionì´ í–‰ë ¬ì„ ë°›ì•„ í–‰ë ¬ì„ ë‚´ë±‰ëŠ” í•¨ìˆ˜ì´ë‹¤ ë³´ë‹ˆ ì—°ì†ì„±ë„ ì˜ ì •ì˜ë˜ì–´ì•¼ í•œë‹¤. ë…¼ë¬¸ì—ì„œëŠ” $\mathbb{R}^{d\times n}$ì— entry-wise $\ell^p$ norm($|\cdot|_p$)ê³¼ ê·¸ì— ëŒ€í•œ <a href="https://mathworld.wolfram.com/NormTopology.html">norm topology</a>ë¥¼ ì£¼ê³  ê·¸ ìœ„ì—ì„œ ì—°ì†ì„±ì„ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ì´ë•Œ $p$ì˜ ê°’ì€ $1\le p&lt;\infty$.</p>
  </li>
  <li>
    <p>í•¨ìˆ˜ ê°„ì˜ ê±°ë¦¬(function metric)</p>

    <p>í•¨ìˆ˜ë¼ë¦¬ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ ì§€ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ function ì‚¬ì´ì˜ distanceë¥¼ ì •ì˜í•œë‹¤. ì¦‰ sequence-to-sequence function spaceì˜ metric $d_p$ì„ ì“°ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

    <p><img src="/assets/img/papers/metric-formula.png" alt="metric-formula" /></p>

    <p>(Usualí•œ $\ell^p$  function normì„ ì´ìš©í•´ì„œ, ë…¼ë¬¸ì— ìˆëŠ” í‘œê¸°ì™€ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ ì ì–´ë³´ì•˜ë‹¤.)</p>

    <ul>
      <li>
        <p>Note:</p>

        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ë…¼ë¬¸ì—ì„œëŠ” ì–¸ì œë‚˜ compact domain, compact supportë¥¼ ê°€ì •í•˜ê¸° ë•Œë¬¸ì—, $N_p(f)$ê°€ ë¬´í•œëŒ€ë¡œ ë°œì‚°í•  ê±±ì •ì€ í•˜ì§€ ì•Šì•„ë„ ë  ê²ƒ ê°™ë‹¤.
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h3 id="2-permutation-equivariant">2. Permutation Equivariant</h3>

<ul>
  <li>
    <p>Permutation matrixë€</p>

    <p>Permutation matrixëŠ” ê° í–‰ê³¼ ê° ì—´ë§ˆë‹¤ 1ì´ ë”± í•˜ë‚˜ì”© ìˆëŠ” ì •ì‚¬ê°í–‰ë ¬ì´ë‹¤. ì–´ë–¤ í–‰ë ¬ $A\in \mathbb{R}^{m\times n}$ì— Permutation matrix $P$ë¥¼ ê³±í•˜ë©´ $A$ì˜ í–‰ ë˜ëŠ” ì—´ì˜ ìˆœì„œë¥¼ ë’¤ì£½ë°•ì£½ ì„ì–´ ë†“ì€ ê²ƒê³¼ ê°™ë‹¤. ì¢€ ë” ì •í™•íˆëŠ”, (1) $P\in \mathbb{R}^{n\times n}$ì´ë¼ë©´ $AP$ëŠ” $A$ì˜  ì—´ë“¤ì˜ ìˆœì„œë¥¼ ì„ì–´ë†“ì€ í–‰ë ¬ì´ ë˜ê³ , (2) $P\in \mathbb{R}^{m\times m}$ì´ë¼ë©´ $PA$ëŠ” $A$ì˜  í–‰ë“¤ì˜ ìˆœì„œë¥¼ ì„ì–´ë†“ì€ í–‰ë ¬ì´ ëœë‹¤. ì˜ˆë¥¼ ë“¤ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[\begin{pmatrix} 1&amp;2&amp;3 \\ 4&amp;5&amp;6 \\ 7&amp;8&amp;9\end{pmatrix}\begin{pmatrix} 0&amp;1&amp;0 \\ 0&amp;0&amp;1 \\ 1&amp;0&amp;0\end{pmatrix} = \begin{pmatrix} 3&amp;1&amp;2 \\ 6&amp;4&amp;5 \\ 9&amp;7&amp;8\end{pmatrix}\]

    <p>ì°¸ê³ ë¡œ ì´ëŸ¬í•œ permutation matrixëŠ” ì–¸ì œë‚˜ orthogonalí•˜ë‹¤: $P^TP=PP^T=I$. (Pê°€ í–‰/ì—´ì˜ ìˆœì„œë¥¼ ì–´ë–»ê²Œ ì„ëŠ”ì§€ ìƒê°í•´ë³´ì.)</p>
  </li>
</ul>

<p>ì„ì˜ì˜ $X\in \mathbb{R}^{m\times n}$ì™€ ì„ì˜ì˜ permutation matrix $P\in \mathbb{R}^{n\times n}$ì— ëŒ€í•´ì„œ, Sequence-to-sequence functionì¸ $f$ê°€ $f(XP)=f(X)P$ë¥¼ ë§Œì¡±í•˜ë©´ ì´ëŸ¬í•œ í•¨ìˆ˜ê°€ permutation equivariantí•˜ë‹¤ê³  ë§í•œë‹¤.</p>

<p>Sequenceì˜ ìˆœì„œë¥¼ ë’¤ì„ëŠ” ì¼ì„ í•¨ìˆ˜ì— ëŒ€ì…í•˜ê¸° ì „ì— í•˜ë‚˜ í›„ì— í•˜ë‚˜ ë‹¬ë¼ì§€ì§€ ì•ŠëŠ” í•¨ìˆ˜ë¥¼ ë§í•œë‹¤ê³  ë³´ë©´ ëœë‹¤.</p>

<p>ì°¸ê³ ë¡œ ë…¼ë¬¸ì—ì„œëŠ” ê°ê°ì˜ <strong>transformer (encoder) blockì´ permutation equivariantí•œ sequence-to-sequence function</strong>ì„ì„ ì¦ëª…í•œë‹¤. <strong>(Claim 1)</strong></p>

<h3 id="3-universal-approximation">3. Universal Approximation</h3>

<p>ë”¥ëŸ¬ë‹ ì´ë¡ ì˜ ì¶œë°œì ì´ë¼ê³  í•  ë§Œí•œ ì •ë¦¬ë¡œ, Neural networkì˜ expressive powerì— ëŒ€í•´ ì•Œë ¤ì£¼ëŠ” ì •ë¦¬ì¸ â€˜<strong>universal approximation theorem</strong>â€™ì´ ìˆë‹¤. ì´ê²ƒì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<blockquote>
  <p>Hidden layerê°€ 1ê°œ ìˆëŠ” neural networkë§Œ ê°€ì§€ê³ ë„ ì•„ë¬´ëŸ° ì—°ì†í•¨ìˆ˜(with compact support)ë¥¼ <strong>ì„ì˜ì˜ (ì•„ì£¼ ì‘ì€) ì˜¤ì°¨ë¡œ ê·¼ì‚¬</strong>í•  ìˆ˜ ìˆë‹¤. (ë‹¨! networkì˜ widthì—ëŠ” ì œí•œì´ ì—†ìœ¼ë©°, ì¤‘ê°„ì— ìˆëŠ” activation functionì€ ë‹¤í•­í•¨ìˆ˜ê°€ ì•„ë‹˜.)</p>
</blockquote>

<p>ì´ì²˜ëŸ¼, Universal ApproximatorëŠ” â€˜ì„ì˜ì˜ ì •í™•ë„ë¡œ ì—„ì²­ ë§ì€ í•¨ìˆ˜ë“¤ì„ ê·¼ì‚¬í•  ìˆ˜ ìˆâ€™ëŠ” ëª¨ë¸ì„ ë‘ê³  í•˜ëŠ” ë§ì´ë‹¤.  ì´í›„ë¡œë„ universal approximationì— ëŒ€í•œ ë‹¤ë°©ë©´ì˜ ì—°êµ¬ê°€ ì´ë£¨ì–´ì¡ŒëŠ”ë°, ì´ëŠ” ì—¬ê¸°ì„œ ì†Œê°œí•˜ëŠ” ë…¼ë¬¸ì˜ section 1.2 related works &amp; notationì— ì˜ ì†Œê°œë˜ì–´ ìˆë‹¤.</p>

<h3 id="4-contextual-mapping">4. Contextual Mapping</h3>

<p>ë…¼ë¬¸ì— ë”°ë¥´ë©´, Transformerê°€ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ì´ìœ ê°€ ë³´í†µ â€˜contextual mappingâ€™ì„ ì˜ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì´ë¼ê³  í‰ê°€ëœë‹¤ê³  í•œë‹¤. ì¦‰, ê°ê°ì˜ ë¬¸ë§¥ì„ ì„œë¡œ ì˜ êµ¬ë¶„í•˜ëŠ” ëŠ¥ë ¥ì´ íƒì›”í•˜ë‹¤ê³  ë³´ëŠ” ê²ƒì´ë‹¤.</p>

<p>ë…¼ë¬¸ì—ì„œëŠ” Trasformerì˜ ì´ëŸ°ì €ëŸ° universal approximation ëŠ¥ë ¥ì„ ì¦ëª…í•˜ë ¤ í•˜ëŠ”ë°, ê·¸ ê³¼ì • ì¤‘ì— â€˜(multi-head) self-attention layersê°€ contextual mappingì„ ì˜ ê³„ì‚°í•œë‹¤â€™ëŠ” ê²ƒì„ ì¦ëª…í•˜ëŠ” ê²Œ ì •ë§ ì¤‘ìš”í•œ ì¤‘ê°„ ê³¼ì •ì´ë¼ê³  í•œë‹¤. ì´ë¥¼ ìœ„í•´ ë…¼ë¬¸ì—ì„œëŠ” contextual mappingì˜ ê°œë…ì„ ì•„ì˜ˆ ìˆ˜ì‹ì ìœ¼ë¡œ ì •ì˜í•´ë²„ë¦° ë’¤ì— ì´ë¥¼ ì¦ëª…ì— ì´ìš©í•œë‹¤. ë…¼ë¬¸ì—ì„œ ì£¼ì–´ì§„ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<p><img src="/assets/img/papers/trasformer-formula.jpeg" alt="transformer-formula.jpeg" /></p>

<p>ì¦‰ contextual mappingì€ ê¸¸ì´ $n$ì¸ input sequenceë¥¼ ë°›ì•„ $n$ê°œì˜ ê°’ (í˜¹ì€ $n$ì°¨ì› ì—´ë²¡í„°)ë¥¼ ë‚´ë†“ëŠ” í•¨ìˆ˜ë¡œ ì •ì˜ëœë‹¤. ì´ë•Œ í•œ ë¬¸ì¥(sequence) ì•ˆì˜ ë‹¨ì–´ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ ì—­í• ì„ í•˜ë¯€ë¡œ ê°ê° ë‹¤ë¥¸ contextê°’(contextual mappingì˜ entry)ì´ ë§¤ê²¨ì§„ë‹¤(1ë²ˆ ì¡°ê±´). ê²Œë‹¤ê°€, ê°™ì€ ë‹¨ì–´ë¼ë„ ë‹¤ë¥¸ ë¬¸ì¥ì—ì„œëŠ” ë‹¤ë¥¸ ì˜ë¯¸ë¡œ í•´ì„ëœë‹¤ëŠ” ì˜ë¯¸ì—ì„œ, ì„œë¡œ ë‹¤ë¥¸ ë‘ input sequence(L, Lâ€™)ì— ëŒ€í•œ contextual mappingì— ìˆëŠ” ëª¨ë“  (ì´ 2nê°œì˜) entryë“¤ì€ ì „ë¶€ ë‹¤ë¥´ê²Œ ë§¤ê²¨ì§„ë‹¤(2ë²ˆ ì¡°ê±´).</p>

<ul>
  <li>
    <p>ì§‘í•© $\mathbb{L}$ì´ ìœ í•œì§‘í•©ìœ¼ë¡œ ì„¤ì •ëœ ì´ìœ ëŠ” (ë‚´ ìƒê°ì—ëŠ”)</p>

    <p>Vocabularyì˜ í¬ê¸°ë„ ìœ í•œí•˜ê³  sequence ê¸¸ì´ë„ ìœ í•œí•˜ë¯€ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” input sequenceì˜ ê°œìˆ˜ëŠ” ìœ í•œí•˜ë‹¤. Sequenceë“¤ì˜ ì§‘í•©ê³¼ ëŒ€ì‘ë˜ëŠ” ì§‘í•©ì´ $\mathbb{L}$ê³¼ ë¹„ìŠ·í•œ ê²ƒì´ë¼ë©´, $\mathbb{L}$ì„ ìœ í•œì§‘í•©ì´ë¼ê³  ë†“ì•„ë„ ê´œì°®ì„ ê²ƒì´ë‹¤. (ì´ ì¡°ê±´ì´ í•„ìˆ˜ì¸ì§€ëŠ” ì¦ëª…ì„ ë” ë“¤ì—¬ë‹¤ë´ì•¼..)</p>
  </li>
</ul>

<hr />
<hr />

<h2 id="main-text">Main Text</h2>

<h3 id="1-universal-approximatorì„ì„-ë³´ì´ê¸°-í˜ë“ -ì´ìœ ">1. Universal Approximatorì„ì„ ë³´ì´ê¸° í˜ë“  ì´ìœ </h3>

<ul>
  <li>ë„ˆë¬´ ë§ì•„ ë³´ì´ëŠ” Parameter sharing. Self-attention layerì™€ feed-forward layer ëª¨ë‘, tokenë¼ë¦¬ ê³µìœ í•˜ëŠ” parameterì˜ ìˆ˜ê°€ ë§¤ìš° ë§ë‹¤.</li>
  <li>ë„ˆë¬´ ì ì–´ ë³´ì´ëŠ” token-wise interaction. Self-attention layerì˜ íŠ¹ì„±ìƒ pairwise dot-productë¡œë§Œ token ì‚¬ì´ì˜ interactionì„ ì¡ì•„ë‚¸ë‹¤.</li>
</ul>

<p>(ë‘˜ì§¸ ì´ìœ ëŠ” ê·¸ëŸ´ ë§Œí•˜ë‹¤ê³  ë³´ì´ëŠ”ë°, ì²«ì§¸ ì´ìœ ëŠ” ì•„ì§ ì˜ ì´í•´í•˜ì§€ ëª»í–ˆë‹¤.)</p>

<p>ë…¼ë¬¸ì—ì„œëŠ” ìœ„ì˜ ë‘ ì´ìœ ë¡œ ì¸í•´ transformer encoder ìì²´ê°€ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” sequence-to-sequence í•¨ìˆ˜ì˜ ì¢…ë¥˜ì— ì œí•œì´ ìˆë‹¤ê³  ë³´ë©°, ì´ë¥¼ trainableí•œ positional encodingìœ¼ë¡œ í•´ê²°í•œë‹¤.</p>

<p>â“ ì¼ë°˜ì ìœ¼ë¡œ, Parameter sharingì´ ë§ì„ìˆ˜ë¡ universal approximatorê°€ ë˜ê¸° ì–´ë ¤ìš´ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?</p>

<hr />

<h3 id="2-ë…¼ë¬¸ì—ì„œ-ë³¸-transformer">2. ë…¼ë¬¸ì—ì„œ ë³¸ Transformer</h3>

<p>ì•„ë˜ëŠ” ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ transformer blockì— ëŒ€í•œ ì‹ì´ë‹¤.</p>

<p><img src="/assets/img/papers/contextual-mapping.jpeg" alt="contextual-mapping.jpeg" /></p>

<p>ì˜ ì•Œë ¤ì ¸ ìˆë“¯, transformer encoder blockì€ multi-head self-attention layer(â€™Attnâ€™)ì™€ token-wise feed-forward layer(â€™FFâ€™)ë¼ëŠ” ë‘ (sub-)layerë¡œ ë‚˜ë‰œë‹¤.</p>

<h4 id="21-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ê³µí†µì ">2.1. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜ ê³µí†µì </h4>

<ul>
  <li>ìˆ˜ì‹ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ residual connectionì€ ê·¸ëŒ€ë¡œ ì‚´ë ¤ë‘ì—ˆë‹¤.</li>
</ul>

<h4 id="22-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ì°¨ì´ì ">2.2. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜ ì°¨ì´ì </h4>

<ul>
  <li>í•´ì„ì„ ê°„ë‹¨íˆ í•˜ê¸° ìœ„í•´ layer normalizationì€ ëºë‹¤ê³  í•œë‹¤.</li>
  <li>Self-attention layer ì‹ì„ ë³´ë©´ ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ë³¼ ìˆ˜ ì—†ë˜ ì‹œê·¸ë§ˆ($\sum$) ê¸°í˜¸ê°€ ë³´ì¸ë‹¤. ì›ë˜ transformer ë…¼ë¬¸ì—ì„œëŠ” attention headë“¤ì„ concatenateí•˜ëŠ”ë°, ì´ëŸ¬í•œ concatenationì„ ìˆ˜ì‹ì ìœ¼ë¡œëŠ” ì €ë ‡ê²Œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì¦‰ ì˜ë¯¸ê°€ ë‹¤ë¥¸ ì‹ì´ ì•„ë‹ˆë‹¤.</li>
  <li>Self-attention layerì˜ ì†Œë¬¸ì ì‹œê·¸ë§ˆ í•¨ìˆ˜($\sigma(\cdot)$)ëŠ” (column-wise) softmaxë¥¼ ê°€ë¦¬í‚¨ë‹¤. ê·¸ëŸ°ë° ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” scaled dot-product attentionì„ ì‚¬ìš©í•˜ëŠ” ë°˜ë©´ ì—¬ê¸°ì„œëŠ” ê·¸ëƒ¥ dot-product attentionì„ ì“°ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì¸ë‹¤. ì‚¬ì‹¤ $\boldsymbol{W}_K$ë‚˜ $\boldsymbol{W}_Q$ê°™ì€ parameterë“¤ì´ ê·¸ scaling factor($\frac{1}{\sqrt{d_k}}$)ë¥¼ í•™ìŠµí•˜ë©´ ê·¸ë§Œì´ë‹¤.</li>
</ul>

<p>â“ Layer normalizationì„ ë¹¼ë„ ê´œì°®ì€ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?</p>

<h4 id="23-positional-encoding">2.3. Positional encoding</h4>

<ul>
  <li>Trainableí•œ positional encodingì´ ì—†ëŠ” ìˆœìˆ˜í•œ transformer blockì€ ì˜¤ì§ â€˜permutation equivariantâ€™í•œ ì¢…ë¥˜ì˜ í•¨ìˆ˜ë§Œì„ ì˜ ê·¼ì‚¬í•  ë¿ì´ë‹¤. ê·¸ëŸ¬ë‚˜ positional encodingì„ ë„ì…í•¨ìœ¼ë¡œì¨ ì´ëŸ¬í•œ í•¨ìˆ˜ ì¢…ë¥˜ì˜ ì œí•œ ì—†ì´ ì•„ë¬´ëŸ° sequence-to-sequence í•¨ìˆ˜(with compact domain)ì„ ì˜ ê·¼ì‚¬í•  ìˆ˜ ìˆê²Œ ëœë‹¤.</li>
  <li>Positional encoding $\boldsymbol{E}$ ì—­ì‹œ $d\times n$ í¬ê¸°ì˜ real matrixë¡œ ì •ì˜ëœë‹¤. Transformer blockì„ í•¨ìˆ˜ $g$ë¡œ ì“´ë‹¤ë©´, positional encodingì´ ë„ì…ëœ transformer blockì€ input sequence $\boldsymbol{X}$ì— ëŒ€í•´ $g(\boldsymbol{X}+\boldsymbol{E})$ë¼ê³  ì“¸ ìˆ˜ ìˆë‹¤.</li>
  <li>ë…¼ë¬¸ì—ì„œëŠ” ì´ $\boldsymbol{E}$ê°€ trainableí•˜ë‹¤ê³  ê°€ì •í•˜ë¯€ë¡œ ì•„ë¬´ë ‡ê²Œë‚˜ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œ, í•¨ìˆ˜ë“¤ì˜ domainì´ compactí•¨ì„ ê°€ì •í•´ì„œ input sequenceê°€ $\boldsymbol{X}\in [0,1]^{d\times n}$ ê°€ ë˜ë„ë¡ í•œ ë’¤, positional encodingì„ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬ì„ ì„ì˜ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤. (Appendix C ì°¸ê³ )</li>
</ul>

\[\boldsymbol{E} = \begin{pmatrix} 0&amp;1&amp;2&amp;\cdots&amp;n-1\\0&amp;1&amp;2&amp;\cdots&amp;n-1\\\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots\\0&amp;1&amp;2&amp;\cdots&amp;n-1\end{pmatrix}\]

<hr />

<h3 id="3-ì£¼ìš”-ê²°ê³¼-2ê°€ì§€">3. ì£¼ìš” ê²°ê³¼ (2ê°€ì§€)</h3>

<p>ë…¼ë¬¸ì´ ì£¼ì¥í•˜ëŠ” ë‘ ê°€ì§€ ì¤‘ìš”í•œ ê²°ê³¼ëŠ” Abstractì—ì„œ ì†Œê°œí•œ ì²˜ìŒ ë‘ ì¤„ê³¼ ê°™ë‹¤. ì—¬ê¸°ì„œëŠ” ë” ìì„¸í•œ ì„œìˆ ì„ ì†Œê°œí•œë‹¤.</p>

<h4 id="31-theorem-2">3.1. Theorem 2</h4>

<hr />

<p>(ì„ì˜ì˜  $\epsilon&gt;0$ì™€ $1\le p &lt; \infty$ì— ëŒ€í•´) í•¨ìˆ˜ $f$ê°€ ë‹¤ìŒì˜ ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤ê³  í•˜ì.</p>

<ol>
  <li>$f$ëŠ” sequence-to-sequence í•¨ìˆ˜.</li>
  <li>$f$ì˜ supportëŠ” compact.</li>
  <li>$f$ëŠ” ì—°ì†(w.r.t. entry-wise $\ell^p$ norm).</li>
  <li>$f$ëŠ” <strong>permutation equivariant</strong>.</li>
</ol>

<p>ê·¸ëŸ¬ë©´ ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” Transformer network $g$ê°€ ì¡´ì¬í•œë‹¤.</p>

<ol>
  <li>$g$ëŠ” $(h,m,r)=(2,1,4)$ë¥¼ ë§Œì¡±.</li>
  <li>$d_p (f,g ) \le \epsilon$.</li>
</ol>

<hr />

<ul>
  <li>ì°¸ê³ : Transformer networkë€, ê°™ì€ Transformer blockì„ ì—¬ëŸ¬ ê°œ ìŒ“ì€ ê²ƒì´ë‹¤. ë˜ ìœ„ì—ì„œ ì“°ì¸ h, m, rì€ ê°ê° ë‹¤ìŒê³¼ ê°™ì€ ê²ƒì„ ë‚˜íƒ€ë‚´ëŠ” ê¸°í˜¸ë‹¤.</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">ë¬¸ì</th>
      <th style="text-align: center">ëœ»</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$h$</td>
      <td style="text-align: center">attention headì˜ ê°œìˆ˜</td>
    </tr>
    <tr>
      <td style="text-align: center">$m$</td>
      <td style="text-align: center">attention headì˜ í¬ê¸°</td>
    </tr>
    <tr>
      <td style="text-align: center">$r$</td>
      <td style="text-align: center">feed-forward layerì˜ hidden ì°¨ì› (=$d_{ff}$)</td>
    </tr>
  </tbody>
</table>

<h4 id="32-theorem-3">3.2. Theorem 3</h4>

<hr />
<p>(ì„ì˜ì˜  $\epsilon&gt;0$ì™€ $1\le p &lt; \infty$ì— ëŒ€í•´) í•¨ìˆ˜ $f$ê°€ ë‹¤ìŒì˜ ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤ê³  í•˜ì.</p>

<ol>
  <li>$f$ëŠ” sequence-to-sequence í•¨ìˆ˜.</li>
  <li>$f$ì˜ domainì€ compact.</li>
  <li>$f$ëŠ” ì—°ì†(w.r.t. entry-wise $\ell^p$ norm).</li>
</ol>

<p>ê·¸ëŸ¬ë©´ ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” <strong>Transformer network $g$ with (trainable) positional encoding $\boldsymbol{E}$</strong>ê°€ ì¡´ì¬í•œë‹¤.</p>

<ol>
  <li>$g$ëŠ” $(h,m,r)=(2,1,4)$ë¥¼ ë§Œì¡±.</li>
  <li>$d_p (f,g ) \le \epsilon$.</li>
</ol>

<hr />

<p>ê±°ì˜ ëª¨ë“  ê²ƒì´ Theorem 2ì™€ ë™ì¼í•˜ì§€ë§Œ, Transformer networkì—ëŠ” positional encodingì´ ì¶”ê°€ëê³ , ëŒ€ì‹  ê·¼ì‚¬í•˜ë ¤ëŠ” sequence-to-sequence í•¨ìˆ˜ì˜ permutation equivariant ì¡°ê±´ì´ ì‚¬ë¼ì¡Œë‹¤.</p>

<ul>
  <li>
    <p>$(h,m,r)=(2,1,4)$ë¥¼ ì“°ëŠ” ì´ìœ ? (ë„ˆë¬´ ì‘ì€ block ì•„ë‹Œê°€?)</p>

    <p>Attention headê°€ 2ê°œë°–ì— ì—†ê³ , ê·¸ í¬ê¸°ë„ ê²¨ìš° 1ì´ê³ , ì‹¬ì§€ì–´ feed-forward layerì˜ hidden ì°¨ì›ì´ 4ë°–ì— ì•ˆ ë˜ëŠ” ì‘ì€ Transformer blockì€ ì‹¤ì§ˆì ìœ¼ë¡œ ì“°ì´ì§€ ì•ŠëŠ”ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ Transformer blockì„ ì´ìš©í•œ ì´ìœ ëŠ” ë‹¨ì§€ ë‹¨ìˆœí™”ê°€ ì¦ëª…ì„ ì‰½ê²Œ í•´ì£¼ê¸° ë•Œë¬¸ë§Œì€ ì•„ë‹ˆë‹¤.</p>

    <p>ë” í° ëª¨ë¸ì€ ìëª…í•˜ê²Œ expressive powerê°€ ë” í¬ê¸° ë•Œë¬¸ì´ë‹¤. ì‹¤ì§ˆì ìœ¼ë¡œ ì“°ì´ëŠ” transformer blockì€ í›¨ì”¬ ë” ë§ì€ parameterë¥¼ ì“¸ í…ë°, ê·¸ëŸ° modelì€ ë…¼ë¬¸ì—ì„œ ì“°ì´ëŠ” ë§¤ìš° ì‘ì€ transformer blockì— ë¹„í•˜ë©´ ë‹¹ì—°íˆ ë”ìš±ë” ë§ì€ í•¨ìˆ˜ë“¤ì„ í‘œí˜„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‹ˆ ì´ë ‡ê²Œ ì‘ì€ ìŠ¤ì¼€ì¼ë¡œ ë¬¸ì œë¥¼ ì¶•ì†Œì‹œì¼œì„œ ë¬¸ì œë¥¼ í’€ì–´ë„ ì¶©ë¶„í•˜ë‹¤.</p>
  </li>
</ul>

<p>â“ ìœ„ì˜ ë‘ ì •ë¦¬ëŠ” universal approximationì˜ ì¸¡ë©´ì—ì„œ ë§¤ìš° ìœ ì˜ë¯¸í•œ ê²°ê³¼ë¥¼ ë‚´ê³  ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ëª¨ë‘ ì¡´ì¬ì„± ì •ë¦¬ì¸ íƒ“ì—, í›ˆë ¨ ê³¼ì •ì—ì„œ transformerê°€ â€˜ìš°ë¦¬ê°€ ì›í•˜ëŠ” í•¨ìˆ˜â€™ë¥¼ ì‹¤ì œë¡œ ì˜ ê·¼ì‚¬í•  ìˆ˜ ìˆëŠ”ì§€ëŠ” ë§í•´ì£¼ì§€ ì•ŠëŠ” ê²Œ ë¶„ëª…í•˜ë‹¤. ì´ê²ƒì´ ê°€ëŠ¥í•œì§€ëŠ” ì–´ë–»ê²Œ ì—°êµ¬í•´ì•¼ í• ê¹Œ?/ ì–´ë–»ê²Œ ì—°êµ¬ë˜ê³  ìˆì„ê¹Œ?</p>

<hr />

<h3 id="4-ì–´ë–»ê²Œ-ì¦ëª…í•˜ë‚˜">4. ì–´ë–»ê²Œ ì¦ëª…í•˜ë‚˜?</h3>

<p>Theorem 2ì™€ Theorem 3ì˜ ì¦ëª…ì€ ë§¤ìš° ìœ ì‚¬í•˜ë©°, ë³¸ë¬¸ì—ì„œëŠ” Theorem 2ì˜ ì¦ëª…ê³¼ì •ì„ ìš”ì•½í•˜ì—¬ ì„¤ëª…í•œë‹¤. ì„¸ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ì„ì˜ì˜ continuous, permutation equivariant, sequence-to-sequence function $f$ with compact supportë¥¼ ì ì ˆí•œ Transformer networkë¡œ ê·¼ì‚¬í•œë‹¤. ê·¸ ë¡œë“œë§µì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<h4 id="1-fë¥¼-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¡œ-ê·¼ì‚¬í•˜ê¸°">1) $f$ë¥¼ piece-wise ìƒìˆ˜í•¨ìˆ˜ë¡œ ê·¼ì‚¬í•˜ê¸°</h4>

<p>ìƒìˆ˜í•¨ìˆ˜ë¼ê³  í•´ì„œ fê°€ ê°‘ìê¸° real-valuedê°€ ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤. ì—¬ê¸°ì„œì˜ ìƒìˆ˜í•¨ìˆ˜ ì—­ì‹œ í–‰ë ¬ì„ ë°›ì•„ í–‰ë ¬ì„ ë‚´ë±‰ëŠ” í•¨ìˆ˜ì¸ë°, í•¨ìˆ«ê°’ìœ¼ë¡œì„œì˜ í–‰ë ¬ì´ ê³ ì •ë˜ì–´ ìˆìœ¼ë©´ ìƒìˆ˜í•¨ìˆ˜ì¸ ê²ƒì´ë‹¤.</p>

<h4 id="2-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¥¼-modified-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°">2) Piece-wise ìƒìˆ˜í•¨ìˆ˜ë¥¼ â€˜modifiedâ€™ Transformer networkë¡œ ê·¼ì‚¬í•˜ê¸°</h4>

<p>â€˜Modifiedâ€™ Transformerë€, ê¸°ì¡´ì˜ Transformerì—ì„œ ì“°ì´ë˜ (column-wise) softmax í•¨ìˆ˜($\sigma$)ëŠ” column-wise hardmax($\sigma_H$)ë¡œ ëŒ€ì²´í•˜ê³ , FFì˜ activation functionìœ¼ë¡œ ì“°ì´ë˜ ReLUëŠ” ë˜ë‹¤ë¥¸ íŠ¹ì´í•œ í•¨ìˆ˜($\phi \in \Phi$, ìì„¸í•œ ì •ì˜ëŠ” ì•„ë˜ì—)ë¡œ ëŒ€ì²´í•œ ê²ƒì´ë‹¤.</p>

<ul>
  <li>
    <p>$\Phi$ì˜ ì •ì˜</p>

    <p>The set of all piece-wise linear functions with at most three pieces, where at least one piece is constant. (p.9)</p>
  </li>
</ul>

<p>ì´ ë¶€ë¶„ì„ ì¦ëª…í•˜ê¸° ìœ„í•´, ë…¼ë¬¸ì—ì„œëŠ” modified Transformerì˜ layer ìˆœì„œë¥¼ ëœ¯ì–´ê³ ì¹˜ëŠ” ì¼ì„ í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. Residual connectionì„ ì´ìš©í•˜ë©´, self-attentionê³¼ feed-forward layerë¥¼ ë²ˆê°ˆì•„ ì ìš©í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ self-attentionë§Œ ì­‰, í˜¹ì€ feed-forward layerë§Œ ì­‰ ì´ì–´ í•©ì„±í•œ ê²ƒì„ í™œìš©í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.</p>

<blockquote>
  <p>(â€¦.) we note that even though a Transformer network stacks self-attention and feed-forward layers in an alternate manner, <strong>the skip connections enable these networks to employ a composition of multiple self-attention or feed-forward layers.</strong> (ì¤‘ëµ) self-attention and feed-forward layers play in realizing the ability to universally approximate sequence-to-sequence functions: 1) self-attention layers compute precise contextual maps; and 2) feed-forward layers then assign the results of these contextual maps to the desired output values. (p.6)</p>
</blockquote>

<p>â“ Modified Transformer networkì˜ layer ìˆœì„œë¥¼ ë’¤ë°”ê¾¸ì–´ ê°™ì€ ì¢…ë¥˜ì˜ layerë§Œ ì´ì–´ë¶™ì¼ ìˆ˜ ìˆëŠ” ì´ìœ ê°€ êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì¼ê¹Œ? ì—¬ê¸°ì— skip connectionì€ ì–´ë–¤ ì—­í• ì„ í• ê¹Œ?</p>

<h4 id="3-modified-transformer-networkë¥¼-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°">3) Modified Transformer networkë¥¼ Transformer networkë¡œ ê·¼ì‚¬í•˜ê¸°</h4>

<p>ì•ì—ì„œ ëŒ€ì²´í–ˆë˜ softmaxì™€ ReLUë¥¼ ì›ë˜ëŒ€ë¡œ ëŒë ¤ë†“ëŠ” ì‘ì—…ì´ë¼ê³  ë³´ë©´ ë  ê²ƒ ê°™ë‹¤.</p>

<hr />

<h3 id="5-ëª‡-ê°œì˜-blockì„-ìŒ“ì•„ì•¼-í•˜ë‚˜">5. ëª‡ ê°œì˜ blockì„ ìŒ“ì•„ì•¼ í•˜ë‚˜?</h3>

<p>Theorem 2ëŠ” ê²°ê³¼ì ìœ¼ë¡œ ëª‡ê°œì˜ Transformer blockì„ ìŒ“ì•„ì•¼ í•˜ëŠ”ì§€ ë³´ì—¬ì¤€ë‹¤. ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ”, permutation equivariant í•¨ìˆ˜ë¥¼ ì˜ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ í•„ìš”í•œ (h,m,r)=(2,1,4) Transformer blockì€ ì´ $O(n(1/\delta)^{dn}/n!)$ê°œë‹¤. ë˜í•œ, positional encodingê¹Œì§€ ë”í•´ ì¢€ ë” ê´‘ë²”ìœ„í•œ sequence-to-sequence í•¨ìˆ˜ë¥¼ ì˜ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ì„œ í•„ìš”í•œ blockì€ $O(n(1/\delta)^{dn})$ê°œë‹¤.</p>

<p>ì´ë•Œ $\delta$ëŠ” Theorem 2/3ì˜ ì¦ëª… 1~2ë‹¨ê³„ì—ì„œ ì“°ì¸ piecewise constant functionì˜ domainì„ êµ¬ë¶„í•˜ëŠ” gridë¥¼ ì´ë£¨ëŠ” (hyper-)cubeì˜ í•œ ë³€ì˜ ê¸¸ì´ì´ë©°, ì¶©ë¶„íˆ ì‘ìŒì„ ê°€ì •í•´ì•¼ í•œë‹¤. (ì¦ëª…ê³¼ì •ì— ë”°ë¥´ë©´, $O(\delta^{d/p} ) \le \epsilon/3)$</p>

<p>â“ ë…¼ë¬¸ì—ì„œëŠ” ì¦ëª…ì„ ìœ„í•´ ì•„ì£¼ ì‘ì€ transformer blockì„ ì´ìš©í•˜ê³  ìˆë‹¤. ë§Œì•½ ì´ transformer blockì˜ í¬ê¸°ë¥¼ í‚¤ìš´ë‹¤ë©´ í•„ìš”í•œ blockì˜ ìˆ˜ëŠ” ì¤„ì–´ë“¤ê¹Œ? (ì•„ë§ˆ $d$ì™€ $n$ì— ë”°ë¥¸ complexityì—ëŠ” í¬ê²Œ ì°¨ì´ê°€ ìˆì§€ ì•Šì„ ê²ƒ ê°™ë‹¤. $h$, $m$, $r$ ë“±ì˜ ê°’ì€ $d$ë‚˜ $n$ì˜ ê°’ê³¼ëŠ” ê´€ë ¨ì´ ì—†ìœ¼ë¯€ë¡œ.)</p>

<hr />
<hr />

<h2 id="my-comments--questions">My Comments &amp; Questions</h2>

<ul>
  <li>ì„ í˜•ëŒ€ìˆ˜í•™ì„ ê½¤ë‚˜ ì“°ëŠ” ë…¼ë¬¸ì´ì§€ë§Œ ì‹¤ìƒì€ ì—„ì²­ë‚˜ê²Œ í•´ì„í•™ìŠ¤ëŸ¬ìš´ ë…¼ë¬¸ì´ì—ˆë‹¤. í•´ì„í•™1ë•Œ Weierstrass Approximation Theorem(compact domainì—ì„œ ì—°ì†í•¨ìˆ˜ë¥¼ ë‹¤í•­ì‹ìœ¼ë¡œ ì„ì˜ì˜ ì •í™•ë„ë¡œ ê·¼ì‚¬í•˜ê¸°) ë°°ì› ë˜ ê²ƒì´ ìƒˆë¡ìƒˆë¡â€¦</li>
  <li>ìœ„ì—ì„œ ë˜ì¡Œë˜ ì§ˆë¬¸ë“¤ì€ ë‚´ê°€ ë…¼ë¬¸ì„ ì½ìœ¼ë©´ì„œë„ ëê¹Œì§€ ì´í•´í•˜ì§€ ëª»í–ˆë˜, í˜¹ì€ ìŠ¤ìŠ¤ë¡œ ë§Œì¡±ìŠ¤ëŸ½ê²Œ ëŒ€ë‹µí•˜ì§€ ëª»í–ˆë˜ ëŒ€í‘œì ì¸ ì§ˆë¬¸ë“¤ì´ë‹¤. í•œ ë²ˆ ë” ëª¨ì•„ë³´ìë©´ ì•„ë˜ì™€ ê°™ë‹¤.</li>
</ul>

<p>â“ ì¼ë°˜ì ìœ¼ë¡œ, Parameter sharingì´ ë§ì„ìˆ˜ë¡ universal approximatorê°€ ë˜ê¸° ì–´ë ¤ìš´ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?</p>

<p>â“ Layer normalizationì„ ë¹¼ë„ ê´œì°®ì€ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?</p>

<p>â“ (Paraphrased:) í›ˆë ¨ ê³¼ì •ì—ì„œ transformerê°€ â€˜ìš°ë¦¬ê°€ ì›í•˜ëŠ” í•¨ìˆ˜â€™ë¥¼ ì‹¤ì œë¡œ ì˜ ê·¼ì‚¬í•  ìˆ˜ ìˆëŠ”ì§€ëŠ” ì–´ë–»ê²Œ ì•Œ ìˆ˜ ìˆì„ê¹Œ?</p>

<p>â“ Modified Transformer networkì˜ layer ìˆœì„œë¥¼ ë’¤ë°”ê¾¸ì–´ ê°™ì€ ì¢…ë¥˜ì˜ layerë§Œ ì´ì–´ë¶™ì¼ ìˆ˜ ìˆëŠ” ì´ìœ ê°€ êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì¼ê¹Œ? ì—¬ê¸°ì— skip connectionì€ ì–´ë–¤ ì—­í• ì„ í• ê¹Œ?</p>

<p>â“ ë…¼ë¬¸ì—ì„œëŠ” ì¦ëª…ì„ ìœ„í•´ ì•„ì£¼ ì‘ì€ transformer blockì„ ì´ìš©í•˜ê³  ìˆë‹¤. ë§Œì•½ ì´ transformer blockì˜ í¬ê¸°ë¥¼ í‚¤ìš´ë‹¤ë©´ í•„ìš”í•œ blockì˜ ìˆ˜ëŠ” ì¤„ì–´ë“¤ê¹Œ?</p>

        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="http://localhost:4000/tags/#theory" class="page__taxonomy-item" rel="tag">Theory      </a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#transformer" class="page__taxonomy-item" rel="tag">Transformer</a><span class="sep">, </span>
    
      
      
      <a href="http://localhost:4000/tags/#universal-approximation" class="page__taxonomy-item" rel="tag">Universal Approximation</a>
    
    </span>
  </p>




      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/posts/2022/01/are-transformer-universal-approximators-of-sequence-to-sequence-functions/" class="btn btn--twitter" title="Share on Twitter"><i class="fab fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/posts/2022/01/are-transformer-universal-approximators-of-sequence-to-sequence-functions/" class="btn btn--facebook" title="Share on Facebook"><i class="fab fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/posts/2022/01/are-transformer-universal-approximators-of-sequence-to-sequence-functions/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fab fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>

      


    </div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/HanseulJo"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Hanseul Cho. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

