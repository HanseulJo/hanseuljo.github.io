var store = [{
        "title": "[ë…¼ë¬¸ê³µë¶€] Are Transformers universal approximators of sequence-to-sequence functions?",
        "excerpt":"ğŸ“Œ í•œ ì¤„ ìš”ì•½: Transformerì˜ expressive powerë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë³´ì¸ ì²« ë…¼ë¬¸ ğŸ¤” ì°¸ê³ : ë…¸ì…˜ìœ¼ë¡œ ì‘ì„±í•œ ì›ë¬¸ì„ ì˜®ê²¨ì˜¨ ê²ƒì…ë‹ˆë‹¤. Abstract Keywords &amp; Definitions 1. Sequence-to-sequence Function 2. Permutation Equivariant 3. Universal Approximation 4. Contextual Mapping Main Text 1. Universal Approximatorì„ì„ ë³´ì´ê¸° í˜ë“  ì´ìœ  2. ë…¼ë¬¸ì—ì„œ ë³¸ Transformer 2.1. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜...","categories": [],
        "tags": ["transformer","universal approximation","theory"],
        "url": "/posts/2022/01/are-transformer-universal-approximators-of-sequence-to-sequence-functions/",
        "teaser": null
      },{
        "title": "SGDA with shuffling: faster convergence for nonconvex-PÅ minimax optimization",
        "excerpt":"Abstract Stochastic gradient descent-ascent (SGDA) is one of the main workhorses for solving finite-sum minimax optimization problems. Most practical implementations of SGDA randomly reshuffle components and sequentially use them (i.e., without-replacement sampling); however, there are few theoretical results on this approach for minimax algorithms, especially outside the easier-to-analyze (strongly-)monotone setups....","categories": ["minimax optimization","SGDA","without-replacement sampling","shuffling-based"],
        "tags": [],
        "url": "/publication/2022-10-12-sgda-with-shuffling",
        "teaser": null
      },]
