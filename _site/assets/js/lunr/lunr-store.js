var store = [{
        "title": "[논문공부] Are Transformers universal approximators of sequence-to-sequence functions?",
        "excerpt":"📌 한 줄 요약: Transformer의 expressive power를 이론적으로 보인 첫 논문 🤔 참고: 노션으로 작성한 원문을 옮겨온 것입니다. Abstract Keywords &amp; Definitions 1. Sequence-to-sequence Function 2. Permutation Equivariant 3. Universal Approximation 4. Contextual Mapping Main Text 1. Universal Approximator임을 보이기 힘든 이유 2. 논문에서 본 Transformer 2.1. 기존 Transformer 논문과의...","categories": [],
        "tags": ["transformer","universal approximation","theory"],
        "url": "/posts/2022/01/are-transformer-universal-approximators-of-sequence-to-sequence-functions/",
        "teaser": null
      },{
        "title": "SGDA with shuffling: faster convergence for nonconvex-PŁ minimax optimization",
        "excerpt":"Abstract Stochastic gradient descent-ascent (SGDA) is one of the main workhorses for solving finite-sum minimax optimization problems. Most practical implementations of SGDA randomly reshuffle components and sequentially use them (i.e., without-replacement sampling); however, there are few theoretical results on this approach for minimax algorithms, especially outside the easier-to-analyze (strongly-)monotone setups....","categories": ["minimax optimization","SGDA","without-replacement sampling","shuffling-based"],
        "tags": [],
        "url": "/publication/2022-10-12-sgda-with-shuffling",
        "teaser": null
      },]
