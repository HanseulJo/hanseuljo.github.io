<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-01-22T22:44:15+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Hanseul Cho</title><subtitle>PhD Student at OptiML Lab in KAIST AI. Working on ML/DL/Opt+Theory and LMs.</subtitle><author><name>Hanseul Cho</name></author><entry><title type="html">[Paper Reading] Viewing Log-Depth Transformers via the Lens of Distributed Computing</title><link href="http://localhost:4000/posts/log-depth-transformer/" rel="alternate" type="text/html" title="[Paper Reading] Viewing Log-Depth Transformers via the Lens of Distributed Computing" /><published>2024-10-10T00:00:00+09:00</published><updated>2024-10-10T00:00:00+09:00</updated><id>http://localhost:4000/posts/log-depth-transformer</id><content type="html" xml:base="http://localhost:4000/posts/log-depth-transformer/">&lt;p&gt;Please visit the link to open the slide:  &lt;a href=&quot;/files/group_meeting/GroupMeeting241010_HanseulCho_LogDepthTransformer.pdf&quot;&gt;&lt;strong&gt;View PDF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-references&quot;&gt;Main References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Clayton Sanford, Danial Hsu, and Matus Telgarsky. &lt;a href=&quot;https://openreview.net/forum?id=QCZabhKQhB&quot;&gt;Transformers, Parallel Computation, and Logarithmic Depth.&lt;/a&gt; ICML 2024.&lt;/li&gt;
  &lt;li&gt;Clayton Sanford, Bahare Fatemi, Ethan Hall, Anton Tsitsulin, Mehran Kazemi, Jonathan Halcrow, Bryan Perozzi, and Vahab Mirrokni. &lt;a href=&quot;https://arxiv.org/abs/2405.18512&quot;&gt;Understanding Transformer Reasoning Capabilities via Graph Algorithms.&lt;/a&gt; NeurIPS 2024.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Hanseul Cho</name></author><category term="paper review" /><category term="group meeting" /><category term="transformers" /><category term="parallel computation" /><category term="graph algorithms" /><category term="distributed computing" /><summary type="html">Presented in OptiML Group Meeting (Fall 2024)</summary></entry><entry><title type="html">[Paper Reading] StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization</title><link href="http://localhost:4000/posts/stable-ssm/" rel="alternate" type="text/html" title="[Paper Reading] StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization" /><published>2024-07-16T00:00:00+09:00</published><updated>2024-07-16T00:00:00+09:00</updated><id>http://localhost:4000/posts/stable-ssm</id><content type="html" xml:base="http://localhost:4000/posts/stable-ssm/">&lt;p&gt;Please visit the link to open the slide:  &lt;a href=&quot;/files/group_meeting/GroupMeeting240716_HanseulCho_StableSSM.pdf&quot;&gt;&lt;strong&gt;View PDF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-references&quot;&gt;Main References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Shida Wang and Qianxiao Li. &lt;a href=&quot;https://arxiv.org/abs/2311.14495&quot;&gt;StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization.&lt;/a&gt; ICML 2024.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Hanseul Cho</name></author><category term="paper review" /><category term="group meeting" /><category term="state space model" /><category term="curse of memory" /><category term="stable reparameterization" /><summary type="html">Presented in OptiML Group Meeting (Summer 2024)</summary></entry><entry><title type="html">[Paper Reading] Convex and Non-convex Optimization under Generalized Smoothness</title><link href="http://localhost:4000/posts/generalized-smoothness/" rel="alternate" type="text/html" title="[Paper Reading] Convex and Non-convex Optimization under Generalized Smoothness" /><published>2024-06-03T00:00:00+09:00</published><updated>2024-06-03T00:00:00+09:00</updated><id>http://localhost:4000/posts/generalized-smoothness</id><content type="html" xml:base="http://localhost:4000/posts/generalized-smoothness/">&lt;p&gt;Please visit the link to open the slide:  &lt;a href=&quot;/files/courseworks/AI709presentation-HanseulCho-GeneralizedSmoothness.pdf&quot;&gt;&lt;strong&gt;View PDF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-references&quot;&gt;Main References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Haochuan Li, Jian Qian, Yi Tian, Alexander Rakhlin, and Ali Jadbabaie. &lt;a href=&quot;https://openreview.net/forum?id=8aunGrXdkl&quot;&gt;Convex and Non-convex Optimization under Generalized Smoothness.&lt;/a&gt; NeurIPS 2023.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Hanseul Cho</name></author><category term="paper review" /><category term="group meeting" /><category term="generalized smoothness" /><category term="optimization" /><summary type="html">Presented in AI709 Advanced Deep Learning Theory (Spring 2024)</summary></entry><entry><title type="html">[Paper Reading] Understanding Gradient Descent on Edge of Stability in Deep Learning</title><link href="http://localhost:4000/posts/edge-of-stability-gd/" rel="alternate" type="text/html" title="[Paper Reading] Understanding Gradient Descent on Edge of Stability in Deep Learning" /><published>2024-03-20T00:00:00+09:00</published><updated>2024-03-20T00:00:00+09:00</updated><id>http://localhost:4000/posts/edge-of-stability-gd</id><content type="html" xml:base="http://localhost:4000/posts/edge-of-stability-gd/">&lt;p&gt;Please visit the link to open the slide:  &lt;a href=&quot;/files/courseworks/AI709presentation-HanseulCho-GDonEoS.pdf&quot;&gt;&lt;strong&gt;View PDF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-references&quot;&gt;Main References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. &lt;a href=&quot;https://arxiv.org/abs/2205.09745&quot;&gt;Understanding Gradient Descent on Edge of Stability in Deep Learning.&lt;/a&gt; ICML 2022.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Hanseul Cho</name></author><category term="paper review" /><category term="group meeting" /><category term="edge of stability" /><category term="normalized gradient descent" /><summary type="html">Presented in AI709 Advanced Deep Learning Theory (Spring 2024)</summary></entry><entry><title type="html">[Paper Reading] A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning</title><link href="http://localhost:4000/posts/u-turn-double-descent/" rel="alternate" type="text/html" title="[Paper Reading] A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning" /><published>2024-03-07T00:00:00+09:00</published><updated>2024-03-07T00:00:00+09:00</updated><id>http://localhost:4000/posts/u-turn-double-descent</id><content type="html" xml:base="http://localhost:4000/posts/u-turn-double-descent/">&lt;p&gt;Please visit the link to open the slide:  &lt;a href=&quot;/files/group_meeting/GroupMeeting240306_HansuelCho_Uturn.pdf&quot;&gt;&lt;strong&gt;View PDF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-references&quot;&gt;Main References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Alicia Curth, Alan Jeffares, and Mihaela van der Schaar. &lt;a href=&quot;https://openreview.net/forum?id=O0Lz8XZT2b&quot;&gt;A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning.&lt;/a&gt; NeurIPS 2023.&lt;/li&gt;
  &lt;li&gt;Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. &lt;a href=&quot;https://www.pnas.org/doi/10.1073/pnas.1903070116&quot;&gt;Reconciling modern machine-learning practice and the classical bias–variance trade-off.&lt;/a&gt; PNAS 2019.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Hanseul Cho</name></author><category term="paper review" /><category term="group meeting" /><category term="double descent" /><category term="machine learning" /><summary type="html">Presented in OptiML Group Meeting (Spring 2024)</summary></entry><entry><title type="html">[Coursework Report] An Overview on Optimal Transport and its Application to Model Fusion</title><link href="http://localhost:4000/posts/optimal-transport-model-fusion/" rel="alternate" type="text/html" title="[Coursework Report] An Overview on Optimal Transport and its Application to Model Fusion" /><published>2023-12-16T00:00:00+09:00</published><updated>2023-12-16T00:00:00+09:00</updated><id>http://localhost:4000/posts/optimal-transport-final</id><content type="html" xml:base="http://localhost:4000/posts/optimal-transport-model-fusion/">&lt;p&gt;Please visit the link to open the report:  &lt;a href=&quot;/files/courseworks/Optimal_Transport_Final_Essay.pdf&quot;&gt;&lt;strong&gt;View PDF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;In this report, we briefly overview the theory of optimal transport (OT),entropically regularized OT and Sinkhorn algorithm, and their application to deep learning model fusion technique. For the theory of OT, we delve into the derivation of dual OT problem and the proof of no duality gap result (i.e., strong duality). Through the lens of duality, we can analyze the entropic OT and its dual problem, and derive Sinkhorn algorithm. We then provide a short survey of convergence results of Sinkhorn algorithm and its variants. Lastly, we turn our attention to model fusion, which combine the power of several differently trained deep learning models (i.e., neural networks) into a single powerful model. We illustrate OTfusion, a method of aggregating several neural networks via optimal transport, and we offer a short discussion of further application of it.&lt;/p&gt;

&lt;h2 id=&quot;main-references&quot;&gt;Main References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Sidak Pal Singh and Martin Jaggi. &lt;a href=&quot;https://arxiv.org/abs/1910.05653&quot;&gt;Model Fusion via Optimal Transport.&lt;/a&gt; NeurIPS 2020.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Hanseul Cho</name></author><category term="coursework" /><category term="report" /><category term="optimal transport" /><category term="duality" /><category term="entropic regularization" /><category term="model fusion" /><summary type="html">Final report for the coursework &quot;Optimal Transport + Gradient Flow&quot;</summary></entry><entry><title type="html">[Paper Reading] Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions</title><link href="http://localhost:4000/posts/implicit-bias-large-depth/" rel="alternate" type="text/html" title="[Paper Reading] Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions" /><published>2023-09-08T00:00:00+09:00</published><updated>2023-09-08T00:00:00+09:00</updated><id>http://localhost:4000/posts/implicit-bias-large-depth</id><content type="html" xml:base="http://localhost:4000/posts/implicit-bias-large-depth/">&lt;p&gt;Please visit the link to open the slide:  &lt;a href=&quot;/files/group_meeting/GroupMeeting230908_HSCho_ImplicitBiasLargeDepth.pdf&quot;&gt;&lt;strong&gt;View PDF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-references&quot;&gt;Main References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Arthur Jacot. &lt;a href=&quot;https://openreview.net/forum?id=6iDHce-0B-a&quot;&gt;Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions.&lt;/a&gt; ICLR 2023.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Hanseul Cho</name></author><category term="paper review" /><category term="group meeting" /><category term="implicit bias" /><category term="low rank" /><summary type="html">Presented in OptiML Group Meeting (Fall 2023)</summary></entry><entry><title type="html">[Paper Reading] Adversarial training descends without descent: Finding actual descent directions based on Danskin’s Theorem</title><link href="http://localhost:4000/posts/adversarial-training-danskin/" rel="alternate" type="text/html" title="[Paper Reading] Adversarial training descends without descent: Finding actual descent directions based on Danskin’s Theorem" /><published>2023-03-17T00:00:00+09:00</published><updated>2023-03-17T00:00:00+09:00</updated><id>http://localhost:4000/posts/adversarial-training-danskin</id><content type="html" xml:base="http://localhost:4000/posts/adversarial-training-danskin/">&lt;p&gt;Please visit the link to open the slide:  &lt;a href=&quot;/files/group_meeting/GroupMeeting230201_HSCho_AdversarialTraining.pdf&quot;&gt;&lt;strong&gt;View PDF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-references&quot;&gt;Main References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Fabian Latorre, Igor Krawczuk, Leello Tadesse Dadi, Thomas Pethick, and Volkan Cevher. &lt;a href=&quot;https://openreview.net/forum?id=I3HCE7Ro78H&quot;&gt;Finding Actual Descent Directions for Adversarial Training.&lt;/a&gt; ICLR 2023.&lt;/li&gt;
  &lt;li&gt;Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu &lt;a href=&quot;https://openreview.net/forum?id=rJzIBfZAb&quot;&gt;Towards Deep Learning Models Resistant to Adversarial Attacks.&lt;/a&gt; ICLR 2018.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Hanseul Cho</name></author><category term="paper review" /><category term="group meeting" /><category term="adversarial training" /><category term="danskin&apos;s lemma" /><summary type="html">Presented in OptiML Group Meeting (Spring 2023)</summary></entry><entry><title type="html">[Paper Reading] On Learning Fairness and Accuracy on Multiple Subgroups</title><link href="http://localhost:4000/posts/multi-group-fairness/" rel="alternate" type="text/html" title="[Paper Reading] On Learning Fairness and Accuracy on Multiple Subgroups" /><published>2023-01-04T00:00:00+09:00</published><updated>2023-01-04T00:00:00+09:00</updated><id>http://localhost:4000/posts/multi-group-fairness</id><content type="html" xml:base="http://localhost:4000/posts/multi-group-fairness/">&lt;p&gt;Please visit the link to open the slide:  &lt;a href=&quot;/files/group_meeting/GroupMeeting230104_HSCho_MultiGroupFairness.pdf&quot;&gt;&lt;strong&gt;View PDF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-references&quot;&gt;Main References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Changjian Shui, Gezheng Xu, Qi CHEN, Jiaqi Li, Charles Ling, Tal Arbel, Boyu Wang, and Christian Gagné. &lt;a href=&quot;https://openreview.net/forum?id=YsRH6uVcx2l&quot;&gt;On Learning Fairness and Accuracy on Multiple Subgroups.&lt;/a&gt; NeurIPS 2022.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Hanseul Cho</name></author><category term="paper review" /><category term="group meeting" /><category term="fairness" /><category term="multiple subgroup fairness" /><summary type="html">Presented in OptiML Group Meeting (Spring 2023)</summary></entry><entry><title type="html">[Paper Reading] Offline Reinforcement Learning with Implicit Q Learning</title><link href="http://localhost:4000/posts/implicit-q-learning/" rel="alternate" type="text/html" title="[Paper Reading] Offline Reinforcement Learning with Implicit Q Learning" /><published>2022-11-28T00:00:00+09:00</published><updated>2022-11-28T00:00:00+09:00</updated><id>http://localhost:4000/posts/implicit-q-learning</id><content type="html" xml:base="http://localhost:4000/posts/implicit-q-learning/">&lt;p&gt;Please visit the link to open the slide:  &lt;a href=&quot;/files/courseworks/AI611presentation_HCho_IQL.pdf&quot;&gt;&lt;strong&gt;View PDF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-references&quot;&gt;Main References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Ilya Kostrikov, Ashvin Nair, and Sergey Levine. &lt;a href=&quot;https://openreview.net/forum?id=68n2s9ZJWF8&quot;&gt;Offline Reinforcement Learning with Implicit Q Learning.&lt;/a&gt; ICLR 2022.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Hanseul Cho</name></author><category term="paper review" /><category term="coursework" /><category term="offline RL" /><category term="implicit q learning" /><summary type="html">Presented in AI611 Deep Reinforcement Learning (Fall 2022)</summary></entry></feed>