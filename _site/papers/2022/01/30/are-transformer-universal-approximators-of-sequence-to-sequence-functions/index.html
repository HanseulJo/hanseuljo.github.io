<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v6.4.0 (https://qwtel.com/hydejack/)
-->









<head>
  <!-- =============== -->
<!-- META            -->
<!-- =============== -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="format-detection" content="telephone=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="naver-site-verification" content="22c5b4ef3bfda7fc6100671413989219de7a4ac8"/>
<meta property="og:title" content="[ë…¼ë¬¸ë¦¬ë·°] Are Transformers universal approximators of sequence-to-sequence functions?">
<meta property="og:type" content="article">





  <meta property="og:image" content="http://localhost:4000/assets/img/logo.png">


<meta property="og:image:width" content="640" />
<meta property="og:image:height" content="360" />



  <title>[ë…¼ë¬¸ë¦¬ë·°] Are Transformers universal approximators of sequence-to-sequence functions? &middot; Hanseul's Blog</title>



<meta name="description" content="ğŸ“Œ í•œ ì¤„ ìš”ì•½: Transformerì˜ expressive powerë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë³´ì¸ ì²«ë²ˆì§¸ ë…¼ë¬¸

">
<meta property="og:description" content="ğŸ“Œ í•œ ì¤„ ìš”ì•½: Transformerì˜ expressive powerë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë³´ì¸ ì²«ë²ˆì§¸ ë…¼ë¬¸

">



<!-- =============== -->
<!-- LINKS           -->
<!-- =============== -->
<link rel="canonical" href="http://localhost:4000/papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions/">
<meta property="og:url" content="http://localhost:4000/papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions/">

<link rel="alternate" type="application/atom+xml" title="Hanseul's Blog Feed" href="http://localhost:4000/feed.xml">


  <link rel="prev" href="http://localhost:4000/papers/2022/01/29/paper-list/">




<link rel="apple-touch-icon" href="http://localhost:4000/apple-touch-icon.png">
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
<!-- Place favicon.ico in the root directory -->

<!-- =============== -->
<!-- SCRIPTS         -->
<!-- =============== -->
<script>
  !function(n,e){function t(n,e){n.onload=function(){this.onerror=this.onload=null,e(null,n)},n.onerror=function(){this.onerror=this.onload=null,e(new Error("Failed to load "+this.src),n)}}function o(n,e){n.onreadystatechange=function(){"complete"!=this.readyState&&"loaded"!=this.readyState||(this.onreadystatechange=null,e(null,n))}}n._loaded=!1,n.loadJSDeferred=function(a,d){function r(){n._loaded=!0;var r=e.createElement("script");r.src=a,d&&(("onload"in r?t:o)(r,d),r.onload||t(r,d));var l=e.getElementsByTagName("script")[0];l.parentNode.insertBefore(r,l)}n._loaded?r():n.addEventListener?n.addEventListener("load",r,!1):n.attachEvent?n.attachEvent("onload",r):n.onload=r}}(window,document);

  !function(e){"use strict";var n=function(n,t,o){function i(e){if(a.body)return e();setTimeout(function(){i(e)})}function r(){l.addEventListener&&l.removeEventListener("load",r),l.media=o||"all"}var d,a=e.document,l=a.createElement("link");if(t)d=t;else{var f=(a.body||a.getElementsByTagName("head")[0]).childNodes;d=f[f.length-1]}var s=a.styleSheets;l.rel="stylesheet",l.href=n,l.media="only x",i(function(){d.parentNode.insertBefore(l,t?d:d.nextSibling)});var u=function(e){for(var n=l.href,t=s.length;t--;)if(s[t].href===n)return e();setTimeout(function(){u(e)})};return l.addEventListener&&l.addEventListener("load",r),l.onloadcssdefined=u,u(r),l};"undefined"!=typeof exports?exports.loadCSS=n:e.loadCSS=n}("undefined"!=typeof global?global:this);

  !function(t){if(t.loadCSS){var e=loadCSS.relpreload={};if(e.support=function(){try{return t.document.createElement("link").relList.supports("preload")}catch(t){return!1}},e.poly=function(){for(var e=t.document.getElementsByTagName("link"),r=0;r<e.length;r++){var n=e[r];"preload"===n.rel&&"style"===n.getAttribute("as")&&(t.loadCSS(n.href,n,n.getAttribute("media")),n.rel=null)}},!e.support()){e.poly();var r=t.setInterval(e.poly,300);t.addEventListener&&t.addEventListener("load",function(){e.poly(),t.clearInterval(r)}),t.attachEvent&&t.attachEvent("onload",function(){t.clearInterval(r)})}}}(this);

  window.disablePushState = false;
  window.disableDrawer = false;
</script>

<!--[if lt IE 9]>
<script src="https://unpkg.com/html5shiv/dist/html5shiv.min.js"></script>
<![endif]-->

<!-- =============== -->
<!-- STYLES          -->
<!-- =============== -->
<!--[if gt IE 8]><!---->
<style>
  
  article,aside,dialog,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}mark{background:#FF0;color:#000}*{box-sizing:border-box}html,body{margin:0;padding:0}html{font-size:16px;line-height:1.75}body{color:#333;background-color:#fff;overflow-y:scroll}a{text-decoration:none}.lead{margin-left:-1rem;margin-right:-1rem}img,.img{display:block;max-width:100%;margin-bottom:1rem;border:none}img.lead,.img.lead{max-width:calc(100% + 2rem);width:calc(100% + 2rem)}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6,.heading{font-weight:bold;text-rendering:optimizeLegibility}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6{margin:1.6rem 0 1rem;line-height:1.6}h1,.h1{font-size:2rem;line-height:1.25}h2,.h2{font-size:1.5rem}h3,.h3{font-size:1.17em}p{margin-top:0;margin-bottom:1rem}p.lead{font-size:1.25rem;font-weight:300;padding:0 1rem}ul,ol,dl{margin-top:0;margin-bottom:1rem}ul,ol{padding-left:1.25rem}hr{position:relative;margin:1.5rem 0;border:0;border-top:1px solid #eee}.hr{padding-bottom:.5rem;border-bottom:1px solid #eee;margin-bottom:1.5rem}h4,h5,h6,.h4,.h5,.h6{margin-bottom:0.5rem;font-size:1rem}table{margin-bottom:1rem;width:100%;width:calc(100% + 2rem);margin-left:-1rem;border:1px solid #e5e5e5;border-collapse:collapse;border-spacing:0}td,th{padding:.25rem .5rem;border:1px solid #e5e5e5}td:first-child,th:first-child{padding-left:1rem}td:last-child,th:last-child{padding-right:1rem}thead+tbody,tbody+tbody,tfoot{border-top:3px double #e5e5e5}tbody tr:nth-child(odd) td,tbody tr:nth-child(odd) th{background-color:#f9f9f9}footer{margin-bottom:2rem}.page,.post{margin-bottom:3em}.page li+li,.post li+li{margin-top:.25rem}.page>header,.post>header{margin-bottom:2rem}.page-title,.post-title{margin-top:0}.post-date{display:block;margin-top:-0.5rem;margin-bottom:1rem;color:#9a9a9a}.related-posts{padding-left:0;list-style:none}.related-posts>li,.related-posts>li+li{margin-top:1rem}.related-posts>li>small,.related-posts>li+li>small{font-size:75%;color:#9a9a9a}.message{margin-bottom:1rem;padding:1rem;color:#787878;background-color:#f9f9f9;margin-left:-1rem;margin-right:-1rem}body,main{position:relative;overflow-x:hidden}@media screen{body::before{content:'';background:#e5e5e5;position:absolute;left:0;top:0;bottom:0}}@media screen and (min-width: 40em){html{font-size:17px}}@media screen and (min-width: 54em){html{font-size:16px}}@media screen and (min-width: 88em){html{font-size:17px}}@media screen and (min-width: 125em){html{font-size:18px}}.sr-only{display:none}.clearfix,.sidebar-social::after,.clearafter::after{content:"";display:table;clear:both}a,.a{position:relative;padding-bottom:.15rem;border-style:hidden}.img{overflow:hidden;background-color:#f9f9f9}.img>img{margin:0;width:100%;height:100%}.sixteen-nine{position:relative}.sixteen-nine::before{display:block;content:"";width:100%;padding-top:56.25%}.sixteen-nine>*{position:absolute;top:0;left:0;right:0;bottom:0}h1+hr,h2+hr,h3+hr,h4+hr,h5+hr,h6+hr{margin-top:0}.fade-in{animation-duration:500ms;animation-name:fade-in;animation-fill-mode:forwards}@keyframes fade-in{from{transform:translateY(-2rem);opacity:0}50%{transform:translateY(-2rem);opacity:0}to{transform:translateY(0);opacity:1}}.mb6{margin-bottom:10rem}.sidebar{color:rgba(255,255,255,0.75);text-align:left}.sidebar::before{content:"";position:absolute;z-index:2;top:0;left:0;bottom:0;right:0;background:linear-gradient(to bottom, rgba(32,32,32,0) 0%, rgba(32,32,32,0.5) 100%)}.sidebar a{color:#fff;border-bottom-color:rgba(255,255,255,0.2)}.right-side{width:100%;margin-left:auto;margin-right:auto}.right-side .ad-first{text-align:center}@media screen{.right-side{max-width:38rem;min-height:100vh}.right-side .ad-second{display:none}}@media screen and (min-width: 54em){.right-side{margin-left:20rem;margin-right:1rem;padding:4rem 1rem 12rem}.right-side .ad-second{text-align:center;display:block}}@media screen and (min-width: 72em){.right-side{margin-left:22rem;max-width:42rem}}@media screen and (min-width: 88em){.right-side{width:162px;margin-left:0rem;margin-right:0rem;padding:0rem;margin-top:10rem;display:block;float:left}}@media screen and (min-width: 96em){.right-side{width:300px;margin-right:0rem}}#_yDrawer{position:relative}@media screen{#_yDrawer{min-height:640px;min-height:100vh}}@media screen and (min-width: 54em){#_yDrawer{width:18rem;margin-left:0}}.sidebar-bg{position:absolute;height:100%;overflow:hidden;top:0;right:0;bottom:0;left:0;background:#202020 center / cover}.sidebar-box{display:flex;justify-content:center}.sidebar-sticky{position:relative;z-index:3}@media screen{.sidebar-sticky{-ms-overflow-style:none;overflow:-moz-scrollbars-none;height:100%;overflow:auto;position:absolute;padding:3rem 0rem;right:2.5rem;left:2.5rem}}.sidebar-sticky::-webkit-scrollbar{display:none}.sidebar-about>h1{color:#fff;font-size:2rem}.sidebar-nav>ul{list-style:none;padding-left:0;margin-bottom:.5rem}a.sidebar-nav-item{width:100%;font-weight:normal;display:block;line-height:1.75;padding:.25rem 0;border-bottom:1px solid rgba(255,255,255,0.2)}a.sidebar-nav-subitem{font-weight:normal;display:block;line-height:1.75;padding:.25rem 0;border-bottom:1px solid rgba(255,255,255,0.2)}@media screen{.y-drawer-scrim{z-index:2}.y-drawer-content{width:18rem;left:-18rem;z-index:3}}.sidebar-social{margin-bottom:.5rem}.sidebar-social>ul{list-style:none;padding-left:0;margin:0 -.25rem}.sidebar-social>ul>li{float:left}.sidebar-social>ul>li>a{display:inline-block;text-align:center;font-size:1.6rem;line-height:3rem;width:3.1249rem;height:4rem;padding:.5rem 0}.sidebar-social>ul li+li{margin-top:0}.fixed-top{position:fixed;top:0;left:0;width:100%;z-index:1}.navbar>.content{padding-top:0;padding-bottom:0;min-height:0;height:0}.menu{display:inline-block;padding:1.75rem 1.5rem;border-bottom:none;margin-left:-1.5rem;color:#9a9a9a !important}.menu::after{content:"\2630"}@media screen and (min-width: 54em){.menu{padding:1.25rem 1.5rem;position:absolute;left:-9999px}.menu:focus{position:static}}.animation-main{pointer-events:none}.loading{display:none}@media print{.menu{display:none}}.animation-main{opacity:0;will-change:opacity}.loading{position:absolute;top:0;right:0;padding:5.25rem 4.5rem;transform-origin:top right;transform:scale(0.33)}.content{position:relative;margin-left:auto;margin-right:auto;padding:5rem 1rem 12rem}@media screen{.content{min-height:100vh}}@media screen and (min-width: 54em){.content{padding:4rem 1rem 12rem;margin-left:19rem;margin-right:3rem}}@media screen and (min-width: 72em){.content{max-width:42rem;margin-left:21rem}}@media screen and (min-width: 88em){.content{float:left;width:100%;margin-left:22rem;margin-right:5rem}}@media screen and (min-width: 96em){.content{max-width:44rem}}@media screen and (min-width: 102em){.content{margin-left:25rem;margin-right:8rem}}.me{width:6.5rem;height:6.5rem;align-self:center;margin-right:20px;border-radius:100%;position:relative}@media screen and (min-width: 40em){.me{width:7rem;height:7rem}}@media screen and (min-width: 54em){.me{width:6.5rem;height:6.5rem}}@media screen and (min-width: 72em){.me{width:7rem;height:7rem}}main>footer{width:100%;position:absolute;bottom:0;left:0;right:0;padding:0 1rem;color:#9a9a9a;font-size:smaller;text-align:center}main>footer>p{margin-bottom:0}html{font-family:'Sans-serif'}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6,.heading{font-family:'Sans-serif'}

</style>


<link rel="preload" href="http://localhost:4000/assets/css/hydejack.css?v=6.4.0" as="style" onload="this.rel='stylesheet'">

<style id="_pageStyle">

.content a{color:#148bba;border-color:rgba(20,139,186,0.2)}.content a:hover{border-color:#148bba}:focus{outline-color:#148bba}::selection{color:#fff;background:#148bba}::-moz-selection{color:#fff;background:#148bba}

</style>


<noscript>
  <link rel="stylesheet" href="http://localhost:4000/assets/css/hydejack.css?v=6.4.0">
  
  
  

  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato">
    <style>
      html { font-family: 'Lato', 'Sans-serif' }
      h1, h2, h3, h4, h5, h6, .h1, .h2, .h3, .h4, .h5, .h6, .heading { font-family: 'Lato', 'Sans-serif' }
    </style>
  

  
  <link rel="stylesheet" href="http://localhost:4000/assets/icomoon/style.css">
</noscript>
<!--<![endif]-->

</head>

<body>
  <!-- =============== -->
<!-- MENU            -->
<!-- =============== -->
<div class="navbar fixed-top">
  <div class="content">
    <span class="sr-only">Jump to:</span>
    <a id="_menu" class="menu no-hover" href="#_title">
      <span class="sr-only">Menu</span>
    </a>
  </div>
</div>

<!-- =============== -->
<!-- CONTENT         -->
<!-- =============== -->
<div id="_yPushState">
  <div class="fade-in">
    <main id="_main" class="content" role="main" data-color="#148bba" data-image="/assets/img/side.jpeg">
      

<article id="post-papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions" class="post" role="article">
  <header>
    <h1 class="post-title">
      
        [ë…¼ë¬¸ë¦¬ë·°] Are Transformers universal approximators of sequence-to-sequence functions?
        
    </h1>

    <p class="post-date heading">
      <time datetime="2022-01-30T00:00:00+09:00">30 Jan 2022</time>
      









in <a href="/category/papers/" data-flip="title">Papers</a>

      









on <a href="/tag/papers-theory/" data-flip="title">Theory</a>

    </p>

    
  <div class="hr" style="padding-bottom:0"></div>


  </header>
  

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  
  <div class="markdown-body">

    <!--  -->
    <style>
      .myAd1190 {
        width: 48%;
        height: 280px;
      }

      .myAd1290 {
        width: 48%;
        height: 280px;
      }

      @media(max-width: 800px) {
        .myAd1190 {
          display: none;
        }

        .myAd1290 {
          width: 98%;
        }
      }

      .row {
        display: flex;
        justify-content: space-between;
        align-items: center;
        height: 300px;
      }

      .row-center {
        display: flex;
        justify-content: center;
        height: 258px;
      }
    </style>
    <div class='row'>
      <ins class="adsbygoogle myAd1190" data-ad-client="ca-pub-9134477021095729" data-ad-slot="6559875097"></ins>
      <ins class="adsbygoogle myAd1290" data-ad-client="ca-pub-9134477021095729" data-ad-slot="6559875097"></ins>
    </div>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
    </script>

    <br />
    <p>ğŸ“Œ í•œ ì¤„ ìš”ì•½: <strong>Transformerì˜ expressive powerë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë³´ì¸ ì²«ë²ˆì§¸ ë…¼ë¬¸</strong></p>

<p>ğŸ¤” ì°¸ê³ : ë…¸ì…˜ìœ¼ë¡œ ì‘ì„±í•œ <a href="https://han-5eu1.notion.site/Are-Transformers-universal-approximators-of-sequence-to-sequence-functions-158eac79332a4d81b1b7cccff9b1b0ce">ì›ë¬¸</a>ì„ ì˜®ê²¨ì˜¨ ê²ƒì…ë‹ˆë‹¤.</p>

<ul id="markdown-toc">
  <li><a href="#abstract" id="markdown-toc-abstract">Abstract</a></li>
  <li><a href="#keywords--definitions" id="markdown-toc-keywords--definitions">Keywords &amp; Definitions</a>    <ul>
      <li><a href="#1-sequence-to-sequence-function" id="markdown-toc-1-sequence-to-sequence-function">1. Sequence-to-sequence Function</a></li>
      <li><a href="#2-permutation-equivariant" id="markdown-toc-2-permutation-equivariant">2. Permutation Equivariant</a></li>
      <li><a href="#3-universal-approximation" id="markdown-toc-3-universal-approximation">3. Universal Approximation</a></li>
      <li><a href="#4-contextual-mapping" id="markdown-toc-4-contextual-mapping">4. Contextual Mapping</a></li>
    </ul>
  </li>
  <li><a href="#main-text" id="markdown-toc-main-text">Main Text</a>    <ul>
      <li><a href="#1-universal-approximatorì„ì„-ë³´ì´ê¸°-í˜ë“ -ì´ìœ " id="markdown-toc-1-universal-approximatorì„ì„-ë³´ì´ê¸°-í˜ë“ -ì´ìœ ">1. Universal Approximatorì„ì„ ë³´ì´ê¸° í˜ë“  ì´ìœ </a></li>
      <li><a href="#2-ë…¼ë¬¸ì—ì„œ-ë³¸-transformer" id="markdown-toc-2-ë…¼ë¬¸ì—ì„œ-ë³¸-transformer">2. ë…¼ë¬¸ì—ì„œ ë³¸ Transformer</a>        <ul>
          <li><a href="#21-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ê³µí†µì " id="markdown-toc-21-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ê³µí†µì ">2.1. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜ ê³µí†µì </a></li>
          <li><a href="#22-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ì°¨ì´ì " id="markdown-toc-22-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ì°¨ì´ì ">2.2. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜ ì°¨ì´ì </a></li>
          <li><a href="#23-positional-encoding" id="markdown-toc-23-positional-encoding">2.3. Positional encoding</a></li>
        </ul>
      </li>
      <li><a href="#3-ì£¼ìš”-ê²°ê³¼-2ê°€ì§€" id="markdown-toc-3-ì£¼ìš”-ê²°ê³¼-2ê°€ì§€">3. ì£¼ìš” ê²°ê³¼ (2ê°€ì§€)</a>        <ul>
          <li><a href="#31-theorem-2" id="markdown-toc-31-theorem-2">3.1. Theorem 2</a></li>
          <li><a href="#32-theorem-3" id="markdown-toc-32-theorem-3">3.2. Theorem 3</a></li>
        </ul>
      </li>
      <li><a href="#4-ì–´ë–»ê²Œ-ì¦ëª…í•˜ë‚˜" id="markdown-toc-4-ì–´ë–»ê²Œ-ì¦ëª…í•˜ë‚˜">4. ì–´ë–»ê²Œ ì¦ëª…í•˜ë‚˜?</a>        <ul>
          <li><a href="#1-fë¥¼-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¡œ-ê·¼ì‚¬í•˜ê¸°" id="markdown-toc-1-fë¥¼-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¡œ-ê·¼ì‚¬í•˜ê¸°">1) $f$ë¥¼ piece-wise ìƒìˆ˜í•¨ìˆ˜ë¡œ ê·¼ì‚¬í•˜ê¸°</a></li>
          <li><a href="#2-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¥¼-modified-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°" id="markdown-toc-2-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¥¼-modified-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°">2) Piece-wise ìƒìˆ˜í•¨ìˆ˜ë¥¼ â€˜modifiedâ€™ Transformer networkë¡œ ê·¼ì‚¬í•˜ê¸°</a></li>
          <li><a href="#3-modified-transformer-networkë¥¼-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°" id="markdown-toc-3-modified-transformer-networkë¥¼-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°">3) Modified Transformer networkë¥¼ Transformer networkë¡œ ê·¼ì‚¬í•˜ê¸°</a></li>
        </ul>
      </li>
      <li><a href="#5-ëª‡-ê°œì˜-blockì„-ìŒ“ì•„ì•¼-í•˜ë‚˜" id="markdown-toc-5-ëª‡-ê°œì˜-blockì„-ìŒ“ì•„ì•¼-í•˜ë‚˜">5. ëª‡ ê°œì˜ blockì„ ìŒ“ì•„ì•¼ í•˜ë‚˜?</a></li>
    </ul>
  </li>
  <li><a href="#my-comments--questions" id="markdown-toc-my-comments--questions">My Comments &amp; Questions</a></li>
</ul>

<h1 id="abstract">Abstract</h1>

<ul>
  <li><strong>Transformer encoder</strong>ëŠ” <strong>â€˜permutation equivariant</strong>â€™í•œ ì„±ì§ˆì„ ê°–ëŠ” <strong>ì—°ì†</strong>ì¸ â€˜<strong>sequence-to-sequence</strong>â€™ í•¨ìˆ˜(with compact support)ì— ëŒ€í•œ universal approximatorì„ì„ ë³´ì¸ë‹¤.</li>
  <li>Transformer encoderì—ë‹¤ <strong>learnableí•œ positional encodingsë¥¼ ê°™ì´ ì“°ë©´</strong> <strong>ì„ì˜ì˜</strong>(permutation equivariantí•˜ì§€ ì•Šì•„ë„) <strong>ì—°ì†</strong>ì¸ â€˜<strong>sequence-to-sequence</strong>â€™ í•¨ìˆ˜(with compact domain)ë¥¼ universally approximateí•¨ì„ ë³´ì¸ë‹¤.</li>
  <li>Contextual mappingì´ë¼ëŠ” ê²ƒì„ ìˆ˜ì‹ì ìœ¼ë¡œ ì •ì˜í–ˆìœ¼ë©°, Transformer Encoderì˜ <strong>multi-head</strong> <strong>self-attention layerë“¤ì´ ì…ë ¥ sequenceì— ëŒ€í•œ contextual mappingì„ ì˜ ê³„ì‚°í•¨</strong>ì„ ë³´ì¸ë‹¤.</li>
  <li>(ì‹¤í—˜ë„ ì§„í–‰í•˜ì˜€ìœ¼ë‚˜ ì—¬ê¸°ì„œëŠ” ìƒëµ)</li>
</ul>

<hr />
<hr />

<h1 id="keywords--definitions">Keywords &amp; Definitions</h1>

<h2 id="1-sequence-to-sequence-function">1. Sequence-to-sequence Function</h2>

<p>$\mathbb{R}^{d\times n}$ì—ì„œ $\mathbb{R}^{d\times n}$ë¡œ ê°€ëŠ” í•¨ìˆ˜ë¥¼ <strong>sequence-to-sequence</strong> functionì´ë¼ê³  ë§í•œë‹¤. ì •í™•íˆëŠ” ì •ì˜ì—­ë„ ì¹˜ì—­ë„ ëª¨ë‘ subset of $\mathbb{R}^{d\times n}$ì¸ í•¨ìˆ˜ë¥¼ ë§í•œë‹¤. ($\mathbb{R}^{d\times n}$: the set of all $d\times n$  real matrices)</p>

<p>ì´ë•Œ $d$ì™€ $n$ì€ ê°ê°, <a href="https://arxiv.org/abs/1706.03762">Transformer ë…¼ë¬¸</a>ì—ì„œ ì–¸ê¸‰í•˜ëŠ” embedding ì°¨ì›ê³¼ ì…ë ¥ sequence ê¸¸ì´ë¡œ ë¹„ìœ ëœë‹¤. ê¸°ì¡´ Transformer ë…¼ë¬¸ì—ì„œë„ ê±°ì˜ ê°™ì€ í‘œê¸°ë¥¼ ì‚¬ìš©í–ˆë‹¤($d_{\text{model}} = d$). í•œ ê°€ì§€ ì°¨ì´ê°€ ìˆë‹¤ë©´, Transformer ë…¼ë¬¸ì—ì„œëŠ” $n\times d$ í–‰ë ¬ì„ ì“°ëŠ” ë°˜ë©´, ì´ ë…¼ë¬¸ì—ì„œëŠ” ê·¸ ë°˜ëŒ€($d\times n$ í–‰ë ¬)ë¥¼ ì´ìš©í•˜ê¸° ë•Œë¬¸ì—, í–‰ë ¬ì˜ ê° ì—´(column)ì´ í•œ input word embedding(í˜¹ì€ token)ìœ¼ë¡œ ë¹„ìœ ëœë‹¤. ì•ˆê·¸ë˜ë„ ì´ ë…¼ë¬¸ì—ì„œ ê³„ì†í•´ì„œ $d\times n$ í–‰ë ¬ $X$ë¥¼ input sequenceë¼ê³  ì¹­í•œë‹¤.</p>

<ul>
  <li>
    <p>Sequence-to-sequence í•¨ìˆ˜ì˜ ì—°ì†ì„± ì •ì˜</p>

    <p>Sequence-to-sequence functionì´ í–‰ë ¬ì„ ë°›ì•„ í–‰ë ¬ì„ ë‚´ë±‰ëŠ” í•¨ìˆ˜ì´ë‹¤ ë³´ë‹ˆ ì—°ì†ì„±ë„ ì˜ ì •ì˜ë˜ì–´ì•¼ í•œë‹¤. ë…¼ë¬¸ì—ì„œëŠ” $\mathbb{R}^{d\times n}$ì— entry-wise $\ell^p$ norm($|\cdot|_p$)ê³¼ ê·¸ì— ëŒ€í•œ <a href="https://mathworld.wolfram.com/NormTopology.html">norm topology</a>ë¥¼ ì£¼ê³  ê·¸ ìœ„ì—ì„œ ì—°ì†ì„±ì„ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ì´ë•Œ $p$ì˜ ê°’ì€ $1\le p&lt;\infty$.</p>
  </li>
  <li>
    <p>í•¨ìˆ˜ ê°„ì˜ ê±°ë¦¬(function metric)</p>

    <p>í•¨ìˆ˜ë¼ë¦¬ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ ì§€ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ function ì‚¬ì´ì˜ distanceë¥¼ ì •ì˜í•œë‹¤. ì¦‰ sequence-to-sequence function spaceì˜ metric $d_p$ì„ ì“°ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

    <p><img src="/assets/img/papers/metric-formula.png" alt="metric-formula" /></p>

    <p>(Usualí•œ $\ell^p$  function normì„ ì´ìš©í•´ì„œ, ë…¼ë¬¸ì— ìˆëŠ” í‘œê¸°ì™€ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ ì ì–´ë³´ì•˜ë‹¤.)</p>

    <ul>
      <li>
        <p>Note:</p>

        <p>ë…¼ë¬¸ì—ì„œëŠ” ì–¸ì œë‚˜ compact domain, compact supportë¥¼ ê°€ì •í•˜ê¸° ë•Œë¬¸ì—, $N_p(f)$ê°€ ë¬´í•œëŒ€ë¡œ ë°œì‚°í•  ê±±ì •ì€ í•˜ì§€ ì•Šì•„ë„ ë  ê²ƒ ê°™ë‹¤.</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-permutation-equivariant">2. Permutation Equivariant</h2>

<ul>
  <li>
    <p>Permutation matrixë€</p>

    <p>Permutation matrixëŠ” ê° í–‰ê³¼ ê° ì—´ë§ˆë‹¤ 1ì´ ë”± í•˜ë‚˜ì”© ìˆëŠ” ì •ì‚¬ê°í–‰ë ¬ì´ë‹¤. ì–´ë–¤ í–‰ë ¬ $A\in \mathbb{R}^{m\times n}$ì— Permutation matrix $P$ë¥¼ ê³±í•˜ë©´ $A$ì˜ í–‰ ë˜ëŠ” ì—´ì˜ ìˆœì„œë¥¼ ë’¤ì£½ë°•ì£½ ì„ì–´ ë†“ì€ ê²ƒê³¼ ê°™ë‹¤. ì¢€ ë” ì •í™•íˆëŠ”, (1) $P\in \mathbb{R}^{n\times n}$ì´ë¼ë©´ $AP$ëŠ” $A$ì˜  ì—´ë“¤ì˜ ìˆœì„œë¥¼ ì„ì–´ë†“ì€ í–‰ë ¬ì´ ë˜ê³ , (2) $P\in \mathbb{R}^{m\times m}$ì´ë¼ë©´ $PA$ëŠ” $A$ì˜  í–‰ë“¤ì˜ ìˆœì„œë¥¼ ì„ì–´ë†“ì€ í–‰ë ¬ì´ ëœë‹¤. ì˜ˆë¥¼ ë“¤ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[\begin{pmatrix} 1&amp;2&amp;3 \\ 4&amp;5&amp;6 \\ 7&amp;8&amp;9\end{pmatrix}\begin{pmatrix} 0&amp;1&amp;0 \\ 0&amp;0&amp;1 \\ 1&amp;0&amp;0\end{pmatrix} = \begin{pmatrix} 3&amp;1&amp;2 \\ 6&amp;4&amp;5 \\ 9&amp;7&amp;8\end{pmatrix}\]

    <p>ì°¸ê³ ë¡œ ì´ëŸ¬í•œ permutation matrixëŠ” ì–¸ì œë‚˜ orthogonalí•˜ë‹¤: $P^TP=PP^T=I$. (Pê°€ í–‰/ì—´ì˜ ìˆœì„œë¥¼ ì–´ë–»ê²Œ ì„ëŠ”ì§€ ìƒê°í•´ë³´ì.)</p>
  </li>
</ul>

<p>ì„ì˜ì˜ $X\in \mathbb{R}^{m\times n}$ì™€ ì„ì˜ì˜ permutation matrix $P\in \mathbb{R}^{n\times n}$ì— ëŒ€í•´ì„œ, Sequence-to-sequence functionì¸ $f$ê°€ $f(XP)=f(X)P$ë¥¼ ë§Œì¡±í•˜ë©´ ì´ëŸ¬í•œ í•¨ìˆ˜ê°€ permutation equivariantí•˜ë‹¤ê³  ë§í•œë‹¤.</p>

<p>Sequenceì˜ ìˆœì„œë¥¼ ë’¤ì„ëŠ” ì¼ì„ í•¨ìˆ˜ì— ëŒ€ì…í•˜ê¸° ì „ì— í•˜ë‚˜ í›„ì— í•˜ë‚˜ ë‹¬ë¼ì§€ì§€ ì•ŠëŠ” í•¨ìˆ˜ë¥¼ ë§í•œë‹¤ê³  ë³´ë©´ ëœë‹¤.</p>

<p>ì°¸ê³ ë¡œ ë…¼ë¬¸ì—ì„œëŠ” ê°ê°ì˜ <strong>transformer (encoder) blockì´ permutation equivariantí•œ sequence-to-sequence function</strong>ì„ì„ ì¦ëª…í•œë‹¤. <strong>(Claim 1)</strong></p>

<h2 id="3-universal-approximation">3. Universal Approximation</h2>

<p>ë”¥ëŸ¬ë‹ ì´ë¡ ì˜ ì¶œë°œì ì´ë¼ê³  í•  ë§Œí•œ ì •ë¦¬ë¡œ, Neural networkì˜ expressive powerì— ëŒ€í•´ ì•Œë ¤ì£¼ëŠ” ì •ë¦¬ì¸ â€˜<strong>universal approximation theorem</strong>â€™ì´ ìˆë‹¤. ì´ê²ƒì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<blockquote>
  <p>Hidden layerê°€ 1ê°œ ìˆëŠ” neural networkë§Œ ê°€ì§€ê³ ë„ ì•„ë¬´ëŸ° ì—°ì†í•¨ìˆ˜(with compact support)ë¥¼ <strong>ì„ì˜ì˜ (ì•„ì£¼ ì‘ì€) ì˜¤ì°¨ë¡œ ê·¼ì‚¬</strong>í•  ìˆ˜ ìˆë‹¤. (ë‹¨! networkì˜ widthì—ëŠ” ì œí•œì´ ì—†ìœ¼ë©°, ì¤‘ê°„ì— ìˆëŠ” activation functionì€ ë‹¤í•­í•¨ìˆ˜ê°€ ì•„ë‹˜.)</p>

</blockquote>

<p>ì´ì²˜ëŸ¼, Universal ApproximatorëŠ” â€˜ì„ì˜ì˜ ì •í™•ë„ë¡œ ì—„ì²­ ë§ì€ í•¨ìˆ˜ë“¤ì„ ê·¼ì‚¬í•  ìˆ˜ ìˆâ€™ëŠ” ëª¨ë¸ì„ ë‘ê³  í•˜ëŠ” ë§ì´ë‹¤.  ì´í›„ë¡œë„ universal approximationì— ëŒ€í•œ ë‹¤ë°©ë©´ì˜ ì—°êµ¬ê°€ ì´ë£¨ì–´ì¡ŒëŠ”ë°, ì´ëŠ” ì—¬ê¸°ì„œ ì†Œê°œí•˜ëŠ” ë…¼ë¬¸ì˜ section 1.2 related works &amp; notationì— ì˜ ì†Œê°œë˜ì–´ ìˆë‹¤.</p>

<h2 id="4-contextual-mapping">4. Contextual Mapping</h2>

<p>ë…¼ë¬¸ì— ë”°ë¥´ë©´, Transformerê°€ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ì´ìœ ê°€ ë³´í†µ â€˜contextual mappingâ€™ì„ ì˜ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì´ë¼ê³  í‰ê°€ëœë‹¤ê³  í•œë‹¤. ì¦‰, ê°ê°ì˜ ë¬¸ë§¥ì„ ì„œë¡œ ì˜ êµ¬ë¶„í•˜ëŠ” ëŠ¥ë ¥ì´ íƒì›”í•˜ë‹¤ê³  ë³´ëŠ” ê²ƒì´ë‹¤.</p>

<p>ë…¼ë¬¸ì—ì„œëŠ” Trasformerì˜ ì´ëŸ°ì €ëŸ° universal approximation ëŠ¥ë ¥ì„ ì¦ëª…í•˜ë ¤ í•˜ëŠ”ë°, ê·¸ ê³¼ì • ì¤‘ì— â€˜(multi-head) self-attention layersê°€ contextual mappingì„ ì˜ ê³„ì‚°í•œë‹¤â€™ëŠ” ê²ƒì„ ì¦ëª…í•˜ëŠ” ê²Œ ì •ë§ ì¤‘ìš”í•œ ì¤‘ê°„ ê³¼ì •ì´ë¼ê³  í•œë‹¤. ì´ë¥¼ ìœ„í•´ ë…¼ë¬¸ì—ì„œëŠ” contextual mappingì˜ ê°œë…ì„ ì•„ì˜ˆ ìˆ˜ì‹ì ìœ¼ë¡œ ì •ì˜í•´ë²„ë¦° ë’¤ì— ì´ë¥¼ ì¦ëª…ì— ì´ìš©í•œë‹¤. ë…¼ë¬¸ì—ì„œ ì£¼ì–´ì§„ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<p><img src="/assets/img/papers/trasformer-formula.jpeg" alt="transformer-formula.jpeg" /></p>

<p>ì¦‰ contextual mappingì€ ê¸¸ì´ $n$ì¸ input sequenceë¥¼ ë°›ì•„ $n$ê°œì˜ ê°’ (í˜¹ì€ $n$ì°¨ì› ì—´ë²¡í„°)ë¥¼ ë‚´ë†“ëŠ” í•¨ìˆ˜ë¡œ ì •ì˜ëœë‹¤. ì´ë•Œ í•œ ë¬¸ì¥(sequence) ì•ˆì˜ ë‹¨ì–´ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ ì—­í• ì„ í•˜ë¯€ë¡œ ê°ê° ë‹¤ë¥¸ contextê°’(contextual mappingì˜ entry)ì´ ë§¤ê²¨ì§„ë‹¤(1ë²ˆ ì¡°ê±´). ê²Œë‹¤ê°€, ê°™ì€ ë‹¨ì–´ë¼ë„ ë‹¤ë¥¸ ë¬¸ì¥ì—ì„œëŠ” ë‹¤ë¥¸ ì˜ë¯¸ë¡œ í•´ì„ëœë‹¤ëŠ” ì˜ë¯¸ì—ì„œ, ì„œë¡œ ë‹¤ë¥¸ ë‘ input sequence(L, Lâ€™)ì— ëŒ€í•œ contextual mappingì— ìˆëŠ” ëª¨ë“  (ì´ 2nê°œì˜) entryë“¤ì€ ì „ë¶€ ë‹¤ë¥´ê²Œ ë§¤ê²¨ì§„ë‹¤(2ë²ˆ ì¡°ê±´).</p>

<ul>
  <li>
    <p>ì§‘í•© $\mathbb{L}$ì´ ìœ í•œì§‘í•©ìœ¼ë¡œ ì„¤ì •ëœ ì´ìœ ëŠ” (ë‚´ ìƒê°ì—ëŠ”)</p>

    <p>Vocabularyì˜ í¬ê¸°ë„ ìœ í•œí•˜ê³  sequence ê¸¸ì´ë„ ìœ í•œí•˜ë¯€ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” input sequenceì˜ ê°œìˆ˜ëŠ” ìœ í•œí•˜ë‹¤. Sequenceë“¤ì˜ ì§‘í•©ê³¼ ëŒ€ì‘ë˜ëŠ” ì§‘í•©ì´ $\mathbb{L}$ê³¼ ë¹„ìŠ·í•œ ê²ƒì´ë¼ë©´, $\mathbb{L}$ì„ ìœ í•œì§‘í•©ì´ë¼ê³  ë†“ì•„ë„ ê´œì°®ì„ ê²ƒì´ë‹¤. (ì´ ì¡°ê±´ì´ í•„ìˆ˜ì¸ì§€ëŠ” ì¦ëª…ì„ ë” ë“¤ì—¬ë‹¤ë´ì•¼..)</p>
  </li>
</ul>

<hr />
<hr />

<h1 id="main-text">Main Text</h1>

<h2 id="1-universal-approximatorì„ì„-ë³´ì´ê¸°-í˜ë“ -ì´ìœ ">1. Universal Approximatorì„ì„ ë³´ì´ê¸° í˜ë“  ì´ìœ </h2>

<ul>
  <li>ë„ˆë¬´ ë§ì•„ ë³´ì´ëŠ” Parameter sharing. Self-attention layerì™€ feed-forward layer ëª¨ë‘, tokenë¼ë¦¬ ê³µìœ í•˜ëŠ” parameterì˜ ìˆ˜ê°€ ë§¤ìš° ë§ë‹¤.</li>
  <li>ë„ˆë¬´ ì ì–´ ë³´ì´ëŠ” token-wise interaction. Self-attention layerì˜ íŠ¹ì„±ìƒ pairwise dot-productë¡œë§Œ token ì‚¬ì´ì˜ interactionì„ ì¡ì•„ë‚¸ë‹¤.</li>
</ul>

<p>(ë‘˜ì§¸ ì´ìœ ëŠ” ê·¸ëŸ´ ë§Œí•˜ë‹¤ê³  ë³´ì´ëŠ”ë°, ì²«ì§¸ ì´ìœ ëŠ” ì•„ì§ ì˜ ì´í•´í•˜ì§€ ëª»í–ˆë‹¤.)</p>

<p>ë…¼ë¬¸ì—ì„œëŠ” ìœ„ì˜ ë‘ ì´ìœ ë¡œ ì¸í•´ transformer encoder ìì²´ê°€ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” sequence-to-sequence í•¨ìˆ˜ì˜ ì¢…ë¥˜ì— ì œí•œì´ ìˆë‹¤ê³  ë³´ë©°, ì´ë¥¼ trainableí•œ positional encodingìœ¼ë¡œ í•´ê²°í•œë‹¤.</p>

<p>â“ ì¼ë°˜ì ìœ¼ë¡œ, Parameter sharingì´ ë§ì„ìˆ˜ë¡ universal approximatorê°€ ë˜ê¸° ì–´ë ¤ìš´ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?</p>

<hr />

<h2 id="2-ë…¼ë¬¸ì—ì„œ-ë³¸-transformer">2. ë…¼ë¬¸ì—ì„œ ë³¸ Transformer</h2>

<p>ì•„ë˜ëŠ” ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ transformer blockì— ëŒ€í•œ ì‹ì´ë‹¤.</p>

<p><img src="/assets/img/papers/contextual-mapping.jpeg" alt="contextual-mapping.jpeg" /></p>

<p>ì˜ ì•Œë ¤ì ¸ ìˆë“¯, transformer encoder blockì€ multi-head self-attention layer(â€™Attnâ€™)ì™€ token-wise feed-forward layer(â€™FFâ€™)ë¼ëŠ” ë‘ (sub-)layerë¡œ ë‚˜ë‰œë‹¤.</p>

<h3 id="21-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ê³µí†µì ">2.1. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜ ê³µí†µì </h3>

<ul>
  <li>ìˆ˜ì‹ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ residual connectionì€ ê·¸ëŒ€ë¡œ ì‚´ë ¤ë‘ì—ˆë‹¤.</li>
</ul>

<h3 id="22-ê¸°ì¡´-transformer-ë…¼ë¬¸ê³¼ì˜-ì°¨ì´ì ">2.2. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜ ì°¨ì´ì </h3>

<ul>
  <li>í•´ì„ì„ ê°„ë‹¨íˆ í•˜ê¸° ìœ„í•´ layer normalizationì€ ëºë‹¤ê³  í•œë‹¤.</li>
  <li>Self-attention layer ì‹ì„ ë³´ë©´ ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ë³¼ ìˆ˜ ì—†ë˜ ì‹œê·¸ë§ˆ($\sum$) ê¸°í˜¸ê°€ ë³´ì¸ë‹¤. ì›ë˜ transformer ë…¼ë¬¸ì—ì„œëŠ” attention headë“¤ì„ concatenateí•˜ëŠ”ë°, ì´ëŸ¬í•œ concatenationì„ ìˆ˜ì‹ì ìœ¼ë¡œëŠ” ì €ë ‡ê²Œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì¦‰ ì˜ë¯¸ê°€ ë‹¤ë¥¸ ì‹ì´ ì•„ë‹ˆë‹¤.</li>
  <li>Self-attention layerì˜ ì†Œë¬¸ì ì‹œê·¸ë§ˆ í•¨ìˆ˜($\sigma(\cdot)$)ëŠ” (column-wise) softmaxë¥¼ ê°€ë¦¬í‚¨ë‹¤. ê·¸ëŸ°ë° ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” scaled dot-product attentionì„ ì‚¬ìš©í•˜ëŠ” ë°˜ë©´ ì—¬ê¸°ì„œëŠ” ê·¸ëƒ¥ dot-product attentionì„ ì“°ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì¸ë‹¤. ì‚¬ì‹¤ $\boldsymbol{W}_K$ë‚˜ $\boldsymbol{W}_Q$ê°™ì€ parameterë“¤ì´ ê·¸ scaling factor($\frac{1}{\sqrt{d_k}}$)ë¥¼ í•™ìŠµí•˜ë©´ ê·¸ë§Œì´ë‹¤.</li>
</ul>

<p>â“ Layer normalizationì„ ë¹¼ë„ ê´œì°®ì€ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?</p>

<h3 id="23-positional-encoding">2.3. Positional encoding</h3>

<ul>
  <li>Trainableí•œ positional encodingì´ ì—†ëŠ” ìˆœìˆ˜í•œ transformer blockì€ ì˜¤ì§ â€˜permutation equivariantâ€™í•œ ì¢…ë¥˜ì˜ í•¨ìˆ˜ë§Œì„ ì˜ ê·¼ì‚¬í•  ë¿ì´ë‹¤. ê·¸ëŸ¬ë‚˜ positional encodingì„ ë„ì…í•¨ìœ¼ë¡œì¨ ì´ëŸ¬í•œ í•¨ìˆ˜ ì¢…ë¥˜ì˜ ì œí•œ ì—†ì´ ì•„ë¬´ëŸ° sequence-to-sequence í•¨ìˆ˜(with compact domain)ì„ ì˜ ê·¼ì‚¬í•  ìˆ˜ ìˆê²Œ ëœë‹¤.</li>
  <li>Positional encoding $\boldsymbol{E}$ ì—­ì‹œ $d\times n$ í¬ê¸°ì˜ real matrixë¡œ ì •ì˜ëœë‹¤. Transformer blockì„ í•¨ìˆ˜ $g$ë¡œ ì“´ë‹¤ë©´, positional encodingì´ ë„ì…ëœ transformer blockì€ input sequence $\boldsymbol{X}$ì— ëŒ€í•´ $g(\boldsymbol{X}+\boldsymbol{E})$ë¼ê³  ì“¸ ìˆ˜ ìˆë‹¤.</li>
  <li>ë…¼ë¬¸ì—ì„œëŠ” ì´ $\boldsymbol{E}$ê°€ trainableí•˜ë‹¤ê³  ê°€ì •í•˜ë¯€ë¡œ ì•„ë¬´ë ‡ê²Œë‚˜ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œ, í•¨ìˆ˜ë“¤ì˜ domainì´ compactí•¨ì„ ê°€ì •í•´ì„œ input sequenceê°€ $\boldsymbol{X}\in [0,1]^{d\times n}$ ê°€ ë˜ë„ë¡ í•œ ë’¤, positional encodingì„ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬ì„ ì„ì˜ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤. (Appendix C ì°¸ê³ )</li>
</ul>

\[\boldsymbol{E} = \begin{pmatrix} 0&amp;1&amp;2&amp;\cdots&amp;n-1\\0&amp;1&amp;2&amp;\cdots&amp;n-1\\\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots\\0&amp;1&amp;2&amp;\cdots&amp;n-1\end{pmatrix}\]

<hr />

<h2 id="3-ì£¼ìš”-ê²°ê³¼-2ê°€ì§€">3. ì£¼ìš” ê²°ê³¼ (2ê°€ì§€)</h2>

<p>ë…¼ë¬¸ì´ ì£¼ì¥í•˜ëŠ” ë‘ ê°€ì§€ ì¤‘ìš”í•œ ê²°ê³¼ëŠ” Abstractì—ì„œ ì†Œê°œí•œ ì²˜ìŒ ë‘ ì¤„ê³¼ ê°™ë‹¤. ì—¬ê¸°ì„œëŠ” ë” ìì„¸í•œ ì„œìˆ ì„ ì†Œê°œí•œë‹¤.</p>

<h3 id="31-theorem-2">3.1. Theorem 2</h3>

<hr />

<p>(ì„ì˜ì˜  $\epsilon&gt;0$ì™€ $1\le p &lt; \infty$ì— ëŒ€í•´) í•¨ìˆ˜ $f$ê°€ ë‹¤ìŒì˜ ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤ê³  í•˜ì.</p>
<ol>
  <li>$f$ëŠ” sequence-to-sequence í•¨ìˆ˜.</li>
  <li>$f$ì˜ supportëŠ” compact.</li>
  <li>$f$ëŠ” ì—°ì†(w.r.t. entry-wise $\ell^p$ norm).</li>
  <li>$f$ëŠ” <strong>permutation equivariant</strong>.</li>
</ol>

<p>ê·¸ëŸ¬ë©´ ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” Transformer network $g$ê°€ ì¡´ì¬í•œë‹¤.</p>
<ol>
  <li>$g$ëŠ” $(h,m,r)=(2,1,4)$ë¥¼ ë§Œì¡±.</li>
  <li>$d_p (f,g ) \le \epsilon$.</li>
</ol>

<hr />

<ul>
  <li>ì°¸ê³ : Transformer networkë€, ê°™ì€ Transformer blockì„ ì—¬ëŸ¬ ê°œ ìŒ“ì€ ê²ƒì´ë‹¤. ë˜ ìœ„ì—ì„œ ì“°ì¸ h, m, rì€ ê°ê° ë‹¤ìŒê³¼ ê°™ì€ ê²ƒì„ ë‚˜íƒ€ë‚´ëŠ” ê¸°í˜¸ë‹¤.</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">ë¬¸ì</th>
      <th style="text-align: center">ëœ»</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">$h$</td>
      <td style="text-align: center">attention headì˜ ê°œìˆ˜</td>
    </tr>
    <tr>
      <td style="text-align: center">$m$</td>
      <td style="text-align: center">attention headì˜ í¬ê¸°</td>
    </tr>
    <tr>
      <td style="text-align: center">$r$</td>
      <td style="text-align: center">feed-forward layerì˜ hidden ì°¨ì› (=$d_{ff}$)</td>
    </tr>
  </tbody>
</table>

<h3 id="32-theorem-3">3.2. Theorem 3</h3>

<hr />
<p>(ì„ì˜ì˜  $\epsilon&gt;0$ì™€ $1\le p &lt; \infty$ì— ëŒ€í•´) í•¨ìˆ˜ $f$ê°€ ë‹¤ìŒì˜ ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤ê³  í•˜ì.</p>
<ol>
  <li>$f$ëŠ” sequence-to-sequence í•¨ìˆ˜.</li>
  <li>$f$ì˜ domainì€ compact.</li>
  <li>$f$ëŠ” ì—°ì†(w.r.t. entry-wise $\ell^p$ norm).</li>
</ol>

<p>ê·¸ëŸ¬ë©´ ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” <strong>Transformer network $g$ with (trainable) positional encoding $\boldsymbol{E}$</strong>ê°€ ì¡´ì¬í•œë‹¤.</p>
<ol>
  <li>$g$ëŠ” $(h,m,r)=(2,1,4)$ë¥¼ ë§Œì¡±.</li>
  <li>$d_p (f,g ) \le \epsilon$.</li>
</ol>

<hr />

<p>ê±°ì˜ ëª¨ë“  ê²ƒì´ Theorem 2ì™€ ë™ì¼í•˜ì§€ë§Œ, Transformer networkì—ëŠ” positional encodingì´ ì¶”ê°€ëê³ , ëŒ€ì‹  ê·¼ì‚¬í•˜ë ¤ëŠ” sequence-to-sequence í•¨ìˆ˜ì˜ permutation equivariant ì¡°ê±´ì´ ì‚¬ë¼ì¡Œë‹¤.</p>

<ul>
  <li>
    <p>$(h,m,r)=(2,1,4)$ë¥¼ ì“°ëŠ” ì´ìœ ? (ë„ˆë¬´ ì‘ì€ block ì•„ë‹Œê°€?)</p>

    <p>Attention headê°€ 2ê°œë°–ì— ì—†ê³ , ê·¸ í¬ê¸°ë„ ê²¨ìš° 1ì´ê³ , ì‹¬ì§€ì–´ feed-forward layerì˜ hidden ì°¨ì›ì´ 4ë°–ì— ì•ˆ ë˜ëŠ” ì‘ì€ Transformer blockì€ ì‹¤ì§ˆì ìœ¼ë¡œ ì“°ì´ì§€ ì•ŠëŠ”ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ Transformer blockì„ ì´ìš©í•œ ì´ìœ ëŠ” ë‹¨ì§€ ë‹¨ìˆœí™”ê°€ ì¦ëª…ì„ ì‰½ê²Œ í•´ì£¼ê¸° ë•Œë¬¸ë§Œì€ ì•„ë‹ˆë‹¤.</p>

    <p>ë” í° ëª¨ë¸ì€ ìëª…í•˜ê²Œ expressive powerê°€ ë” í¬ê¸° ë•Œë¬¸ì´ë‹¤. ì‹¤ì§ˆì ìœ¼ë¡œ ì“°ì´ëŠ” transformer blockì€ í›¨ì”¬ ë” ë§ì€ parameterë¥¼ ì“¸ í…ë°, ê·¸ëŸ° modelì€ ë…¼ë¬¸ì—ì„œ ì“°ì´ëŠ” ë§¤ìš° ì‘ì€ transformer blockì— ë¹„í•˜ë©´ ë‹¹ì—°íˆ ë”ìš±ë” ë§ì€ í•¨ìˆ˜ë“¤ì„ í‘œí˜„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‹ˆ ì´ë ‡ê²Œ ì‘ì€ ìŠ¤ì¼€ì¼ë¡œ ë¬¸ì œë¥¼ ì¶•ì†Œì‹œì¼œì„œ ë¬¸ì œë¥¼ í’€ì–´ë„ ì¶©ë¶„í•˜ë‹¤.</p>
  </li>
</ul>

<p>â“ ìœ„ì˜ ë‘ ì •ë¦¬ëŠ” universal approximationì˜ ì¸¡ë©´ì—ì„œ ë§¤ìš° ìœ ì˜ë¯¸í•œ ê²°ê³¼ë¥¼ ë‚´ê³  ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ëª¨ë‘ ì¡´ì¬ì„± ì •ë¦¬ì¸ íƒ“ì—, í›ˆë ¨ ê³¼ì •ì—ì„œ transformerê°€ â€˜ìš°ë¦¬ê°€ ì›í•˜ëŠ” í•¨ìˆ˜â€™ë¥¼ ì‹¤ì œë¡œ ì˜ ê·¼ì‚¬í•  ìˆ˜ ìˆëŠ”ì§€ëŠ” ë§í•´ì£¼ì§€ ì•ŠëŠ” ê²Œ ë¶„ëª…í•˜ë‹¤. ì´ê²ƒì´ ê°€ëŠ¥í•œì§€ëŠ” ì–´ë–»ê²Œ ì—°êµ¬í•´ì•¼ í• ê¹Œ?/ ì–´ë–»ê²Œ ì—°êµ¬ë˜ê³  ìˆì„ê¹Œ?</p>

<hr />

<h2 id="4-ì–´ë–»ê²Œ-ì¦ëª…í•˜ë‚˜">4. ì–´ë–»ê²Œ ì¦ëª…í•˜ë‚˜?</h2>

<p>Theorem 2ì™€ Theorem 3ì˜ ì¦ëª…ì€ ë§¤ìš° ìœ ì‚¬í•˜ë©°, ë³¸ë¬¸ì—ì„œëŠ” Theorem 2ì˜ ì¦ëª…ê³¼ì •ì„ ìš”ì•½í•˜ì—¬ ì„¤ëª…í•œë‹¤. ì„¸ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ì„ì˜ì˜ continuous, permutation equivariant, sequence-to-sequence function $f$ with compact supportë¥¼ ì ì ˆí•œ Transformer networkë¡œ ê·¼ì‚¬í•œë‹¤. ê·¸ ë¡œë“œë§µì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<h3 id="1-fë¥¼-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¡œ-ê·¼ì‚¬í•˜ê¸°">1) $f$ë¥¼ piece-wise ìƒìˆ˜í•¨ìˆ˜ë¡œ ê·¼ì‚¬í•˜ê¸°</h3>

<p>ìƒìˆ˜í•¨ìˆ˜ë¼ê³  í•´ì„œ fê°€ ê°‘ìê¸° real-valuedê°€ ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤. ì—¬ê¸°ì„œì˜ ìƒìˆ˜í•¨ìˆ˜ ì—­ì‹œ í–‰ë ¬ì„ ë°›ì•„ í–‰ë ¬ì„ ë‚´ë±‰ëŠ” í•¨ìˆ˜ì¸ë°, í•¨ìˆ«ê°’ìœ¼ë¡œì„œì˜ í–‰ë ¬ì´ ê³ ì •ë˜ì–´ ìˆìœ¼ë©´ ìƒìˆ˜í•¨ìˆ˜ì¸ ê²ƒì´ë‹¤.</p>

<h3 id="2-piece-wise-ìƒìˆ˜í•¨ìˆ˜ë¥¼-modified-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°">2) Piece-wise ìƒìˆ˜í•¨ìˆ˜ë¥¼ â€˜modifiedâ€™ Transformer networkë¡œ ê·¼ì‚¬í•˜ê¸°</h3>

<p>â€˜Modifiedâ€™ Transformerë€, ê¸°ì¡´ì˜ Transformerì—ì„œ ì“°ì´ë˜ (column-wise) softmax í•¨ìˆ˜($\sigma$)ëŠ” column-wise hardmax($\sigma_H$)ë¡œ ëŒ€ì²´í•˜ê³ , FFì˜ activation functionìœ¼ë¡œ ì“°ì´ë˜ ReLUëŠ” ë˜ë‹¤ë¥¸ íŠ¹ì´í•œ í•¨ìˆ˜($\phi \in \Phi$, ìì„¸í•œ ì •ì˜ëŠ” ì•„ë˜ì—)ë¡œ ëŒ€ì²´í•œ ê²ƒì´ë‹¤.</p>

<ul>
  <li>
    <p>$\Phi$ì˜ ì •ì˜</p>

    <p>The set of all piece-wise linear functions with at most three pieces, where at least one piece is constant. (p.9)</p>
  </li>
</ul>

<p>ì´ ë¶€ë¶„ì„ ì¦ëª…í•˜ê¸° ìœ„í•´, ë…¼ë¬¸ì—ì„œëŠ” modified Transformerì˜ layer ìˆœì„œë¥¼ ëœ¯ì–´ê³ ì¹˜ëŠ” ì¼ì„ í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. Residual connectionì„ ì´ìš©í•˜ë©´, self-attentionê³¼ feed-forward layerë¥¼ ë²ˆê°ˆì•„ ì ìš©í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ self-attentionë§Œ ì­‰, í˜¹ì€ feed-forward layerë§Œ ì­‰ ì´ì–´ í•©ì„±í•œ ê²ƒì„ í™œìš©í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.</p>

<blockquote>
  <p>(â€¦.) we note that even though a Transformer network stacks self-attention and feed-forward layers in an alternate manner, <strong>the skip connections enable these networks to employ a composition of multiple self-attention or feed-forward layers.</strong> (ì¤‘ëµ) self-attention and feed-forward layers play in realizing the ability to universally approximate sequence-to-sequence functions: 1) self-attention layers compute precise contextual maps; and 2) feed-forward layers then assign the results of these contextual maps to the desired output values. (p.6)</p>

</blockquote>

<p>â“ Modified Transformer networkì˜ layer ìˆœì„œë¥¼ ë’¤ë°”ê¾¸ì–´ ê°™ì€ ì¢…ë¥˜ì˜ layerë§Œ ì´ì–´ë¶™ì¼ ìˆ˜ ìˆëŠ” ì´ìœ ê°€ êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì¼ê¹Œ? ì—¬ê¸°ì— skip connectionì€ ì–´ë–¤ ì—­í• ì„ í• ê¹Œ?</p>

<h3 id="3-modified-transformer-networkë¥¼-transformer-networkë¡œ-ê·¼ì‚¬í•˜ê¸°">3) Modified Transformer networkë¥¼ Transformer networkë¡œ ê·¼ì‚¬í•˜ê¸°</h3>

<p>ì•ì—ì„œ ëŒ€ì²´í–ˆë˜ softmaxì™€ ReLUë¥¼ ì›ë˜ëŒ€ë¡œ ëŒë ¤ë†“ëŠ” ì‘ì—…ì´ë¼ê³  ë³´ë©´ ë  ê²ƒ ê°™ë‹¤.</p>

<hr />

<h2 id="5-ëª‡-ê°œì˜-blockì„-ìŒ“ì•„ì•¼-í•˜ë‚˜">5. ëª‡ ê°œì˜ blockì„ ìŒ“ì•„ì•¼ í•˜ë‚˜?</h2>

<p>Theorem 2ëŠ” ê²°ê³¼ì ìœ¼ë¡œ ëª‡ê°œì˜ Transformer blockì„ ìŒ“ì•„ì•¼ í•˜ëŠ”ì§€ ë³´ì—¬ì¤€ë‹¤. ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ”, permutation equivariant í•¨ìˆ˜ë¥¼ ì˜ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ í•„ìš”í•œ (h,m,r)=(2,1,4) Transformer blockì€ ì´ $O(n(1/\delta)^{dn}/n!)$ê°œë‹¤. ë˜í•œ, positional encodingê¹Œì§€ ë”í•´ ì¢€ ë” ê´‘ë²”ìœ„í•œ sequence-to-sequence í•¨ìˆ˜ë¥¼ ì˜ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ì„œ í•„ìš”í•œ blockì€ $O(n(1/\delta)^{dn})$ê°œë‹¤.</p>

<p>ì´ë•Œ $\delta$ëŠ” Theorem 2/3ì˜ ì¦ëª… 1~2ë‹¨ê³„ì—ì„œ ì“°ì¸ piecewise constant functionì˜ domainì„ êµ¬ë¶„í•˜ëŠ” gridë¥¼ ì´ë£¨ëŠ” (hyper-)cubeì˜ í•œ ë³€ì˜ ê¸¸ì´ì´ë©°, ì¶©ë¶„íˆ ì‘ìŒì„ ê°€ì •í•´ì•¼ í•œë‹¤. (ì¦ëª…ê³¼ì •ì— ë”°ë¥´ë©´, $O(\delta^{d/p} ) \le \epsilon/3)$</p>

<p>â“ ë…¼ë¬¸ì—ì„œëŠ” ì¦ëª…ì„ ìœ„í•´ ì•„ì£¼ ì‘ì€ transformer blockì„ ì´ìš©í•˜ê³  ìˆë‹¤. ë§Œì•½ ì´ transformer blockì˜ í¬ê¸°ë¥¼ í‚¤ìš´ë‹¤ë©´ í•„ìš”í•œ blockì˜ ìˆ˜ëŠ” ì¤„ì–´ë“¤ê¹Œ? (ì•„ë§ˆ $d$ì™€ $n$ì— ë”°ë¥¸ complexityì—ëŠ” í¬ê²Œ ì°¨ì´ê°€ ìˆì§€ ì•Šì„ ê²ƒ ê°™ë‹¤. $h$, $m$, $r$ ë“±ì˜ ê°’ì€ $d$ë‚˜ $n$ì˜ ê°’ê³¼ëŠ” ê´€ë ¨ì´ ì—†ìœ¼ë¯€ë¡œ.)</p>

<hr />
<hr />

<h1 id="my-comments--questions">My Comments &amp; Questions</h1>

<ul>
  <li>ì„ í˜•ëŒ€ìˆ˜í•™ì„ ê½¤ë‚˜ ì“°ëŠ” ë…¼ë¬¸ì´ì§€ë§Œ ì‹¤ìƒì€ ì—„ì²­ë‚˜ê²Œ í•´ì„í•™ìŠ¤ëŸ¬ìš´ ë…¼ë¬¸ì´ì—ˆë‹¤. í•´ì„í•™1ë•Œ Weierstrass Approximation Theorem(compact domainì—ì„œ ì—°ì†í•¨ìˆ˜ë¥¼ ë‹¤í•­ì‹ìœ¼ë¡œ ì„ì˜ì˜ ì •í™•ë„ë¡œ ê·¼ì‚¬í•˜ê¸°) ë°°ì› ë˜ ê²ƒì´ ìƒˆë¡ìƒˆë¡â€¦</li>
  <li>ìœ„ì—ì„œ ë˜ì¡Œë˜ ì§ˆë¬¸ë“¤ì€ ë‚´ê°€ ë…¼ë¬¸ì„ ì½ìœ¼ë©´ì„œë„ ëê¹Œì§€ ì´í•´í•˜ì§€ ëª»í–ˆë˜, í˜¹ì€ ìŠ¤ìŠ¤ë¡œ ë§Œì¡±ìŠ¤ëŸ½ê²Œ ëŒ€ë‹µí•˜ì§€ ëª»í–ˆë˜ ëŒ€í‘œì ì¸ ì§ˆë¬¸ë“¤ì´ë‹¤. í•œ ë²ˆ ë” ëª¨ì•„ë³´ìë©´ ì•„ë˜ì™€ ê°™ë‹¤.</li>
</ul>

<p>â“ ì¼ë°˜ì ìœ¼ë¡œ, Parameter sharingì´ ë§ì„ìˆ˜ë¡ universal approximatorê°€ ë˜ê¸° ì–´ë ¤ìš´ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?</p>

<p>â“ Layer normalizationì„ ë¹¼ë„ ê´œì°®ì€ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?</p>

<p>â“ (Paraphrased:) í›ˆë ¨ ê³¼ì •ì—ì„œ transformerê°€ â€˜ìš°ë¦¬ê°€ ì›í•˜ëŠ” í•¨ìˆ˜â€™ë¥¼ ì‹¤ì œë¡œ ì˜ ê·¼ì‚¬í•  ìˆ˜ ìˆëŠ”ì§€ëŠ” ì–´ë–»ê²Œ ì•Œ ìˆ˜ ìˆì„ê¹Œ?</p>

<p>â“ Modified Transformer networkì˜ layer ìˆœì„œë¥¼ ë’¤ë°”ê¾¸ì–´ ê°™ì€ ì¢…ë¥˜ì˜ layerë§Œ ì´ì–´ë¶™ì¼ ìˆ˜ ìˆëŠ” ì´ìœ ê°€ êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì¼ê¹Œ? ì—¬ê¸°ì— skip connectionì€ ì–´ë–¤ ì—­í• ì„ í• ê¹Œ?</p>

<p>â“ ë…¼ë¬¸ì—ì„œëŠ” ì¦ëª…ì„ ìœ„í•´ ì•„ì£¼ ì‘ì€ transformer blockì„ ì´ìš©í•˜ê³  ìˆë‹¤. ë§Œì•½ ì´ transformer blockì˜ í¬ê¸°ë¥¼ í‚¤ìš´ë‹¤ë©´ í•„ìš”í•œ blockì˜ ìˆ˜ëŠ” ì¤„ì–´ë“¤ê¹Œ?</p>

    <br />
    <br />
  </div>
  <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
  

</article>

<hr class="dingbat" />

<div class="share">
  <h2>Share this post</h2>
  <div class="share-body">
    <a href="http://twitter.com/share?text=[ë…¼ë¬¸ë¦¬ë·°] Are Transformers universal approximators of sequence-to-sequence functions?&amp;url=http://localhost:4000/papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions/"
      onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
      <span class="icon-twitter">
      </span>
    </a>
    <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions/"
      onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
      <span class="icon-facebook">
      </span>
    </a>
  </div>
</div>
<br />





  <aside class="author" role="complementary">
    <div class="author">
  <h2 class="page-title hr">
    About
  </h2>
<div class="author-body">
  
    
  

  

  <img
    src="/assets/img/me.jpeg"
    class="me"
    alt="Hanseul cho"
    
    
  />


  
  <div class="author-body-description">
    <p>I am an M.Sc. student in OptiML Lab @ <a href="http://gsai.kaist.ac.kr"><strong>KAIST AI</strong></a>, advised by Prof. <a href="https://chulheeyun.github.io"><strong>Chulhee Yun</strong></a>.
Recently, I focus on deep learning theory.</p>

  </div>
</div>
</div>

  </aside>





<aside class="related" role="complementary">
  <h2 class="hr">Related Posts</h2>

  <ul class="related-posts">
    
      
      
      
        
          
          
        
        
          


<li class="h4">
  <a href="/papers/2022/01/29/paper-list/" data-flip="title">
    <span>ì½ê³  ì‹¶ì€ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸!</span>
  </a>
  <small><time datetime="2022-01-29T00:00:00+09:00">
    29 Jan 2022
  </time></small>
</li>

        
      
    
  </ul>
</aside>



      
        <aside class="comments" role="complementary">
  <h2>Comments</h2>
  <hr/>

  <div id="disqus_thread"></div>

  <script>
    !function(s,i){function e(e){var t=s.pageYOffset||i.body.scrollTop;s.DISQUS&&!s._disqusThis&&!s._disqusFirst&&t+s.innerHeight>=s._disqusThreadOffsetTop&&(s._disqusThis=!0,s.DISQUS.reset({reload:!0,config:d}))}var d=function(){this.page.title="[ë…¼ë¬¸ë¦¬ë·°] Are Transformers universal approximators of sequence-to-sequence functions?",this.page.identifier="/papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions",this.page.url="http://localhost:4000/papers/2022/01/30/are-transformer-universal-approximators-of-sequence-to-sequence-functions/"};s._disqusFirst=void 0===s._disqusFirst||s._disqusFirst,s._disqusLoading=void 0!==s._disqusLoading&&s._disqusLoading,s._disqusThis=!1,s._disqusThreadOffsetTop=i.getElementById("disqus_thread").offsetTop,s._disqusLoading?s._disqusFirst=!1:(s._disqusLoading=!0,loadJSDeferred("//hanseul.disqus.com/embed.js"),s.addEventListener?s.addEventListener("scroll",e,{passive:!0}):s.attachEvent?s.attachEvent("onscroll",e):s.onscroll=e)}(window,document);

  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</aside>

      

      <footer>
  <hr/>
  
    <p>Â© 2022. by hanseul</p>

  
  <p>
    <code>Powered by <a href="https://qwtel.com/hydejack/">Hydejack v6.4.0</a></code>
  </p>
</footer>

    </main>
    <div class="right-side">
  <div class="ad-first">
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <!-- ë¸”ë¡œê·¸-ìƒë‹¨-ëª¨ë°”ì¼ -->
    <ins class="adsbygoogle"
         style="display:inline-block;width:100%;"
         data-ad-client="ca-pub-9134477021095729"
         data-ad-slot="6559875097"
         data-ad-format="auto"></ins>
    <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
  </div>
<br/>
<br/>
  <div class="ad-second">
    <!-- ë¸”ë¡œê·¸-ìŠ¤ì¹´ì´ìŠ¤í¬ë˜í¼ -->
    <ins class="adsbygoogle"
         style="display:inline-block;max-width:320px;width:100%;height:600px"
         data-ad-client="ca-pub-9134477021095729"
         data-ad-slot="6826803092"></ins>
    <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
  </div>
</div>

  </div>
  <div id="_yDrawer">
  <div id="_sidebar" class="sidebar">
    <div class="sidebar-bg" style="background-color:#148bba;background-image:url(/assets/img/side.jpeg)"></div>
    <header class="sidebar-sticky" role="banner">
      <br/>
      <div class="sidebar-about">
        <h1><a id="_title" href="/">Hanseul's Blog</a></h1>
        <p>ML/AI</p>

      </div>

      <br/>
      <br/>
      <nav class="sidebar-nav heading" role="navigation">
        <span class="sr-only">Navigation:</span>
<ul>
  

  

  
  
  
  
  
    <li>
      <input type="checkbox" id="list-item-1"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/category/papers/">Papers</a>
       <label class="folder" for="list-item-1">â–¾</label>
    </div>
     <ul class="list-body">
       
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/papers-theory/">Theory</a>
             </li>
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/papers-experiment/">Experiment</a>
             </li>
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/papers-observation/">Observation</a>
             </li>
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/papers-openproblem/">OpenProblem</a>
             </li>
           
         
     </ul>
    </li>

  
  
    <li>
      <input type="checkbox" id="list-item-2"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/category/courseworks/">CourseWorks</a>
       <label class="folder" for="list-item-2">â–¾</label>
    </div>
     <ul class="list-body">
       
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/courseworks-ai502/">ì‹¬ì¸µí•™ìŠµ(AI502)</a>
             </li>
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/courseworks-ai506/">ë°ì´í„°ë§ˆì´ë‹ë°ê²€ìƒ‰(AI506)</a>
             </li>
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/courseworks-ai616/">ì‹¬ì¸µí•™ìŠµì´ë¡ (AI616)</a>
             </li>
           
         
           
         
           
         
     </ul>
    </li>

  
  
    <li>
      <input type="checkbox" id="list-item-3"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/about/">About: Hanseul Cho</a>
       
    </div>
     <ul class="list-body">
       
           
         
           
         
           
         
           
         
           
         
           
         
           
         
     </ul>
    </li>

  
</ul>

      </nav>
    <br/>
    <br/>
      <div class="sidebar-box">
        
          
  

  

  <img
    src="/assets/img/me.jpeg"
    class="me"
    alt="Hanseul cho"
    
    
  />


        
      </div>
      <p>M.Sc. Student in OptiML Lab @ KAIST AI.</p>

      
      
        <div class="sidebar-social">
          <span class="sr-only">Social:</span>
<ul>
  
    









<li>
  <a href="https://github.com/HanseulJo">
    <span class="icon-github" title="GitHub"></span>
    <span class="sr-only">GitHub</span>
  </a>
</li>

  
    









<li>
  <a href="mailto:jhs4015@kaist.ac.kr">
    <span class="icon-mail" title="Email"></span>
    <span class="sr-only">Email</span>
  </a>
</li>

  
</ul>

        </div>
      
    </header>
  </div>
</div>

</div>

<!-- =============== -->
<!-- SCRIPTS         -->
<!-- =============== -->

<script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', '{"tracking_id"=>"G-3K595H6QML"}', 'auto');
  ga('send', 'pageview');
  loadJSDeferred('https://www.google-analytics.com/analytics.js');
</script>





<!--[if gt IE 8]><!---->
<script src="//ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"></script>
<script>
  WebFont.load({
    
    google: {
      families: 'Lato'.split('|')
    },
    

    custom: {
      families: ['icomoon'],
      urls: ['/assets/icomoon/style.css']
    }
  });
</script>
<!--<![endif]-->


  <!--[if gt IE 9]><!---->
  
  <script>loadJSDeferred('/assets/js/hydejack.js?v=6.4.0');</script>

  
  <!--<![endif]-->



<script type="text/javascript" async
	src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	extensions: ["tex2jax.js"],
	jax: ["input/TeX", "output/HTML-CSS"],
	tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		processEscapes: true
	},
	"HTML-CSS": { availableFonts: ["TeX"] }
});
</script>

</body>

</html>
