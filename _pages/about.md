---
permalink: /
title: "Welcome!"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<style>
gray { color: gray; font-size: 75%;}
.nobull {
  margin:0px; padding:0px;
  list-style: none;
  padding-left: 2rem;
  text-indent: -1.6rem;
}
.nobull2 {
  line-height:1em;
  padding-left: 1rem;
  text-indent: 0rem;
}
</style>

My name is Hanseul Cho(조한슬). I am a Ph.D. student in the [Optimization & Machine Learning (OptiML) Laboratory](https://chulheeyun.github.io), advised by [Prof. Chulhee Yun](https://chulheeyun.github.io) at [Kim Jaechul Graduate School of AI](https://gsai.kaist.ac.kr) in [Korea Advanced Institute of Science and Technology](https://www.kaist.ac.kr/en/) (KAIST AI).

I am interested in a broad range of fields in optimization, machine learning (ML), and deep learning (DL), especially focusing on both **mathematical/theoretical analysis** and **empirical improvements (usually based on theoretical understanding)**. Recently, I have been into **understanding and mitigating the fundamental limitations of modern language models** (e.g., length generalization and compositional generalization problems). Also, I am always interested in **hierarchical/multi-level optimization** (e.g., minimax optimization), **optimization with constraints** (e.g., fairness in ML), **optimization under circumstance shifts** (e.g., reinforcement learning and continual learning).

News
---

<ul class="nobull">
  <li>📰 [Sep. '24] Two papers got accepted to <b>NeurIPS 2024</b>! 🎉 <a href="https://arxiv.org/abs/2405.20671">One</a> is about length generalization of arithmetic Transfomers, and another is about mitigating loss of plasticity in incremental neural net training. See you in Vancouver🇨🇦!</li>
  <li>📰 [Jun. '24] An early version of our <a href="https://arxiv.org/abs/2405.20671">paper</a> on length generalization of Transformers got accepted to the ICML 2024 Workshop on <a href="https://longcontextfm.github.io/">Long-Context Foundation Models</a>!</li>
  <li>📰 [May. '24] A <a href="https://openreview.net/forum?id=s6ZAT8MLKU">paper</a> got accepted to <b>ICML 2024</b> as a <b>spotlight paper</b> (top 3.5% among all submissions)! 🎉 We show global convergence of Alt-GDA (which is <i>strictly</i> faster than Sim-GDA) and propose an enhanced algorithm called Alex-GDA for minimax optimization. See you in Vienna🇦🇹!</li>
  <li>📰 [Sep. '23] Two papers are accepted to <b>NeurIPS 2023</b>! 🎉 One is about <a href="https://arxiv.org/abs/2310.18593">Fair Streaming PCA</a> and another is about <a href="https://arxiv.org/abs/2306.10711">enhancing plasticity in RL</a>.</li>
  <li>📰 [Jan. '23] Our <a href="https://openreview.net/forum?id=6xXtM8bFFJ">paper</a> about shuffling-based stochastic gradient descent-ascent got accepted to <b>ICLR 2023</b>! 🎉</li>
  <li>📰 [Nov. '22] Our <a href="https://arxiv.org/abs/2210.05995">paper</a> about shuffling-based stochastic gradient descent-ascent is accepted to 2022 <a href="http://aiassociation.kr">Korea AI Association</a> + <a href="https://www.navercorp.com/en">NAVER</a> Autumnal Joint Conference (JKAIA 2022) and selected as the <b>NAVER Outstanding Theory Paper</b>! </li>
  <li>📰 [Oct. '22] I am happy to announce that our very first <a href="https://arxiv.org/abs/2210.05995">preprint</a> is now on arXiv!  It is about convergence analysis of shuffling-based stochastic gradient descent-ascent. </li>
  <li>📰 [Feb. '22] Now I am part of <a href="https://chulheeyun.github.io">OptiML Lab</a> of KAIST AI. </li>
</ul>

Education
---

<ul class="nobull">
  <li>🏫 Ph.D. in Artificial Intelligence <gray>KAIST, Sept. 2023 - Current</gray></li>
  <li>🏫 M.Sc. in Artificial Intelligence <gray>KAIST, Mar. 2022 - Aug. 2023</gray></li>
  <li>🏫 B.Sc. in Mathematical Sciences <gray>KAIST, Mar. 2017 - Feb. 2022</gray></li>
    <ul class="nobull2" style="color:gray">
    <li>Minor in Computing Sciences / Summa Cum Laude</li>
    </ul>
</ul>

Contact & Info
---

📋 **Curriculum Vitae**: [Here](../files/Curriculum_Vitae__Hanseul_Cho.pdf)  
📧 Email address: jhs4015 at kaist dot ac dot kr  