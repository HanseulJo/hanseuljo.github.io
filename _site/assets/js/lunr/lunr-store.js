var store = [{
        "title": "SGDA with shuffling: faster convergence for nonconvex-P≈Å minimax optimization",
        "excerpt":"Abstract Stochastic gradient descent-ascent (SGDA) is one of the main workhorses for solving finite-sum minimax optimization problems. Most practical implementations of SGDA randomly reshuffle components and sequentially use them (i.e., without-replacement sampling); however, there are few theoretical results on this approach for minimax algorithms, especially outside the easier-to-analyze (strongly-)monotone setups....","categories": ["arXiv preprint"],
        "tags": ["minimax optimization","SGDA","without-replacement sampling","shuffling-based"],
        "url": "/publication/2022-10-12-sgda-with-shuffling",
        "teaser": null
      },]
