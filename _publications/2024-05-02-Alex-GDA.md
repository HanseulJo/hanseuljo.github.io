---
title: "Fundamental Benefit of Alternating Updates in Minimax Optimization"
collection: publications
permalink: /publication/Alex-GDA
date: 2024-05-02
authors:
    - JwL*
    - me*
    - CY
venue: <a href="https://icml.cc/Conferences/2024"><b>ICML 2024</b></a> (Short version at ICLR 2024 Workshop on <a href="https://sites.google.com/view/bgpt-iclr24">Bridging the Gap Between Practice and Theory in Deep Learning (BGPT)</a>)
award: <a href="https://icml.cc/virtual/2024/events/2024SpotlightPosters"><b>Spotlight</b> @ ICML 2024</a> (Top 3.5% among total submissions)
paperurl: https://proceedings.mlr.press/v235/lee24e.html
arxiv: https://arxiv.org/abs/2402.10475
code: https://github.com/HanseulJo/Alex-GDA
categories: 
    - ICLR Workshop
    - ICML
tags:
    - minimax optimization
    - Gradient Descent-Ascent (GDA)
    - Alex-GDA
---

![alex_gda_thumbnail](../assets/img/alex-gda/thumbnail_icml2024_hr.png)

## Poster

![alex_gda_poster](../assets/img/alex-gda/poster_icml2024.png)

## Abstract

The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax optimization problems, takes the descent and ascent steps either simultaneously (Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to converge faster, the performance gap between the two is not yet well understood theoretically, especially in terms of global convergence rates. To address this theory-practice gap, we present fine-grained convergence analyses of both algorithms for strongly-convex-strongly-concave and Lipschitz-gradient objectives. Our new iteration complexity upper bound of Alt-GDA is strictly smaller than the lower bound of Sim-GDA; i.e., Alt-GDA is provably faster. Moreover, we propose Alternating-Extrapolation GDA (Alex-GDA), a general algorithmic framework that subsumes Sim-GDA and Alt-GDA, for which the main idea is to alternately take gradients from extrapolations of the iterates. We show that Alex-GDA satisfies a smaller iteration complexity bound, identical to that of the Extra-gradient method, while requiring less gradient computations. We also prove that Alex-GDA enjoys linear convergence for bilinear problems, for which both Sim-GDA and Alt-GDA fail to converge at all.
