---
title: '[ë…¼ë¬¸ì½ê¸°] Are Transformers universal approximators of sequence-to-sequence functions?'
excerpt: 'ğŸ“Œ í•œ ì¤„ ìš”ì•½: **Transformerì˜ expressive powerë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë³´ì¸ ì²«ë²ˆì§¸ ë…¼ë¬¸**'
collections: posts
date: 2022-01-30
permalink: /posts/2022-01-30-are-transformer-universal-approximators-of-sequence-to-sequence-functions/
use_math: true
toc:      true
toc_sticky: true
categories:
    - paper review
tags:
    - transformer
    - sequence-to-sequence
    - approximation
    - expressive power
    - deep learning theory
---


[//]: <> ğŸ¤” ì°¸ê³ : ë…¸ì…˜ìœ¼ë¡œ ì‘ì„±í•œ [ì›ë¬¸](https://han-5eu1.notion.site/Are-Transformers-universal-approximators-of-sequence-to-sequence-functions-158eac79332a4d81b1b7cccff9b1b0ce)ì„ ì˜®ê²¨ì˜¨ ê²ƒì…ë‹ˆë‹¤.

* TOC
{:toc}

## Abstract

* **Transformer encoder**ëŠ” **â€˜permutation equivariant**â€™í•œ ì„±ì§ˆì„ ê°–ëŠ” **ì—°ì†**ì¸ â€˜**sequence-to-sequence**â€™ í•¨ìˆ˜(with compact support)ì— ëŒ€í•œ universal approximatorì„ì„ ë³´ì¸ë‹¤.
* Transformer encoderì—ë‹¤ **learnableí•œ positional encodingsë¥¼ ê°™ì´ ì“°ë©´** **ì„ì˜ì˜**(permutation equivariantí•˜ì§€ ì•Šì•„ë„) **ì—°ì†**ì¸ â€˜**sequence-to-sequence**â€™ í•¨ìˆ˜(with compact domain)ë¥¼ universally approximateí•¨ì„ ë³´ì¸ë‹¤.
* Contextual mappingì´ë¼ëŠ” ê²ƒì„ ìˆ˜ì‹ì ìœ¼ë¡œ ì •ì˜í–ˆìœ¼ë©°, Transformer Encoderì˜ **multi-head** **self-attention layerë“¤ì´ ì…ë ¥ sequenceì— ëŒ€í•œ contextual mappingì„ ì˜ ê³„ì‚°í•¨**ì„ ë³´ì¸ë‹¤.
* (ì‹¤í—˜ë„ ì§„í–‰í•˜ì˜€ìœ¼ë‚˜ ì—¬ê¸°ì„œëŠ” ìƒëµ)

## Keywords & Definitions

### 1. Sequence-to-sequence Function

$\mathbb{R}^{d\times n}$ì—ì„œ $\mathbb{R}^{d\times n}$ë¡œ ê°€ëŠ” í•¨ìˆ˜ë¥¼ **sequence-to-sequence** functionì´ë¼ê³  ë§í•œë‹¤. ì •í™•íˆëŠ” ì •ì˜ì—­ë„ ì¹˜ì—­ë„ ëª¨ë‘ subset of $\mathbb{R}^{d\times n}$ì¸ í•¨ìˆ˜ë¥¼ ë§í•œë‹¤. ($\mathbb{R}^{d\times n}$: the set of all $d\times n$  real matrices)

ì´ë•Œ $d$ì™€ $n$ì€ ê°ê°, [Transformer ë…¼ë¬¸](https://arxiv.org/abs/1706.03762)ì—ì„œ ì–¸ê¸‰í•˜ëŠ” embedding ì°¨ì›ê³¼ ì…ë ¥ sequence ê¸¸ì´ë¡œ ë¹„ìœ ëœë‹¤. ê¸°ì¡´ Transformer ë…¼ë¬¸ì—ì„œë„ ê±°ì˜ ê°™ì€ í‘œê¸°ë¥¼ ì‚¬ìš©í–ˆë‹¤($d_{\text{model}} = d$). í•œ ê°€ì§€ ì°¨ì´ê°€ ìˆë‹¤ë©´, Transformer ë…¼ë¬¸ì—ì„œëŠ” $n\times d$ í–‰ë ¬ì„ ì“°ëŠ” ë°˜ë©´, ì´ ë…¼ë¬¸ì—ì„œëŠ” ê·¸ ë°˜ëŒ€($d\times n$ í–‰ë ¬)ë¥¼ ì´ìš©í•˜ê¸° ë•Œë¬¸ì—, í–‰ë ¬ì˜ ê° ì—´(column)ì´ í•œ input word embedding(í˜¹ì€ token)ìœ¼ë¡œ ë¹„ìœ ëœë‹¤. ì•ˆê·¸ë˜ë„ ì´ ë…¼ë¬¸ì—ì„œ ê³„ì†í•´ì„œ $d\times n$ í–‰ë ¬ $X$ë¥¼ input sequenceë¼ê³  ì¹­í•œë‹¤.

* Sequence-to-sequence í•¨ìˆ˜ì˜ ì—°ì†ì„± ì •ì˜

    Sequence-to-sequence functionì´ í–‰ë ¬ì„ ë°›ì•„ í–‰ë ¬ì„ ë‚´ë±‰ëŠ” í•¨ìˆ˜ì´ë‹¤ ë³´ë‹ˆ ì—°ì†ì„±ë„ ì˜ ì •ì˜ë˜ì–´ì•¼ í•œë‹¤. ë…¼ë¬¸ì—ì„œëŠ” $\mathbb{R}^{d\times n}$ì— entry-wise $\ell^p$ norm($\|\cdot\|_p$)ê³¼ ê·¸ì— ëŒ€í•œ [norm topology](https://mathworld.wolfram.com/NormTopology.html)ë¥¼ ì£¼ê³  ê·¸ ìœ„ì—ì„œ ì—°ì†ì„±ì„ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ì´ë•Œ $p$ì˜ ê°’ì€ $1\le p<\infty$.

* í•¨ìˆ˜ ê°„ì˜ ê±°ë¦¬(function metric)

    í•¨ìˆ˜ë¼ë¦¬ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ ì§€ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ function ì‚¬ì´ì˜ distanceë¥¼ ì •ì˜í•œë‹¤. ì¦‰ sequence-to-sequence function spaceì˜ metric $d_p$ì„ ì“°ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

    ![metric-formula](/assets/img/papers/metric-formula.png)

    (Usualí•œ $L^p$ function normì„ ì´ìš©í•´ì„œ, ë…¼ë¬¸ì— ìˆëŠ” í‘œê¸°ì™€ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ ì ì–´ë³´ì•˜ë‹¤.)

  * Note: ë…¼ë¬¸ì—ì„œëŠ” ì–¸ì œë‚˜ compact domain, compact supportë¥¼ ê°€ì •í•˜ê¸° ë•Œë¬¸ì—, $N_p(f)$ê°€ ë¬´í•œëŒ€ë¡œ ë°œì‚°í•  ê±±ì •ì€ í•˜ì§€ ì•Šì•„ë„ ë  ê²ƒ ê°™ë‹¤.

### 2. Permutation Equivariant

* Permutation matrixë€

    Permutation matrixëŠ” ê° í–‰ê³¼ ê° ì—´ë§ˆë‹¤ 1ì´ ë”± í•˜ë‚˜ì”© ìˆëŠ” ì •ì‚¬ê°í–‰ë ¬ì´ë‹¤. ì–´ë–¤ í–‰ë ¬ $A\in \mathbb{R}^{m\times n}$ì— Permutation matrix $P$ë¥¼ ê³±í•˜ë©´ $A$ì˜ í–‰ ë˜ëŠ” ì—´ì˜ ìˆœì„œë¥¼ ë’¤ì£½ë°•ì£½ ì„ì–´ ë†“ì€ ê²ƒê³¼ ê°™ë‹¤. ì¢€ ë” ì •í™•íˆëŠ”, (1) $P\in \mathbb{R}^{n\times n}$ì´ë¼ë©´ $AP$ëŠ” $A$ì˜  ì—´ë“¤ì˜ ìˆœì„œë¥¼ ì„ì–´ë†“ì€ í–‰ë ¬ì´ ë˜ê³ , (2) $P\in \mathbb{R}^{m\times m}$ì´ë¼ë©´ $PA$ëŠ” $A$ì˜  í–‰ë“¤ì˜ ìˆœì„œë¥¼ ì„ì–´ë†“ì€ í–‰ë ¬ì´ ëœë‹¤. ì˜ˆë¥¼ ë“¤ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

    $$
    \begin{pmatrix} 1&2&3 \\ 4&5&6 \\ 7&8&9\end{pmatrix}\begin{pmatrix} 0&1&0 \\ 0&0&1 \\ 1&0&0\end{pmatrix} = \begin{pmatrix} 3&1&2 \\ 6&4&5 \\ 9&7&8\end{pmatrix}
    $$

    ì°¸ê³ ë¡œ ì´ëŸ¬í•œ permutation matrixëŠ” ì–¸ì œë‚˜ orthogonalí•˜ë‹¤: $P^TP=PP^T=I$. (Pê°€ í–‰/ì—´ì˜ ìˆœì„œë¥¼ ì–´ë–»ê²Œ ì„ëŠ”ì§€ ìƒê°í•´ë³´ì.)

ì„ì˜ì˜ $X\in \mathbb{R}^{m\times n}$ì™€ ì„ì˜ì˜ permutation matrix $P\in \mathbb{R}^{n\times n}$ì— ëŒ€í•´ì„œ, Sequence-to-sequence functionì¸ $f$ê°€ $f(XP)=f(X)P$ë¥¼ ë§Œì¡±í•˜ë©´ ì´ëŸ¬í•œ í•¨ìˆ˜ê°€ permutation equivariantí•˜ë‹¤ê³  ë§í•œë‹¤.

Sequenceì˜ ìˆœì„œë¥¼ ë’¤ì„ëŠ” ì¼ì„ í•¨ìˆ˜ì— ëŒ€ì…í•˜ê¸° ì „ì— í•˜ë‚˜ í›„ì— í•˜ë‚˜ ë‹¬ë¼ì§€ì§€ ì•ŠëŠ” í•¨ìˆ˜ë¥¼ ë§í•œë‹¤ê³  ë³´ë©´ ëœë‹¤.

ì°¸ê³ ë¡œ ë…¼ë¬¸ì—ì„œëŠ” ê°ê°ì˜ **transformer (encoder) blockì´ permutation equivariantí•œ sequence-to-sequence function**ì„ì„ ì¦ëª…í•œë‹¤. **(Claim 1)**

### 3. Universal Approximation

ë”¥ëŸ¬ë‹ ì´ë¡ ì˜ ì¶œë°œì ì´ë¼ê³  í•  ë§Œí•œ ì •ë¦¬ë¡œ, Neural networkì˜ expressive powerì— ëŒ€í•´ ì•Œë ¤ì£¼ëŠ” ì •ë¦¬ì¸ â€˜**universal approximation theorem**â€™ì´ ìˆë‹¤. ì´ê²ƒì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

> Hidden layerê°€ 1ê°œ ìˆëŠ” neural networkë§Œ ê°€ì§€ê³ ë„ ì•„ë¬´ëŸ° ì—°ì†í•¨ìˆ˜(with compact support)ë¥¼ **ì„ì˜ì˜ (ì•„ì£¼ ì‘ì€) ì˜¤ì°¨ë¡œ ê·¼ì‚¬**í•  ìˆ˜ ìˆë‹¤. (ë‹¨! networkì˜ widthì—ëŠ” ì œí•œì´ ì—†ìœ¼ë©°, ì¤‘ê°„ì— ìˆëŠ” activation functionì€ ë‹¤í•­í•¨ìˆ˜ê°€ ì•„ë‹˜.)

ì´ì²˜ëŸ¼, Universal ApproximatorëŠ” â€˜ì„ì˜ì˜ ì •í™•ë„ë¡œ ì—„ì²­ ë§ì€ í•¨ìˆ˜ë“¤ì„ ê·¼ì‚¬í•  ìˆ˜ ìˆâ€™ëŠ” ëª¨ë¸ì„ ë‘ê³  í•˜ëŠ” ë§ì´ë‹¤.  ì´í›„ë¡œë„ universal approximationì— ëŒ€í•œ ë‹¤ë°©ë©´ì˜ ì—°êµ¬ê°€ ì´ë£¨ì–´ì¡ŒëŠ”ë°, ì´ëŠ” ì—¬ê¸°ì„œ ì†Œê°œí•˜ëŠ” ë…¼ë¬¸ì˜ section 1.2 related works & notationì— ì˜ ì†Œê°œë˜ì–´ ìˆë‹¤.

### 4. Contextual Mapping

ë…¼ë¬¸ì— ë”°ë¥´ë©´, Transformerê°€ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ì´ìœ ê°€ ë³´í†µ â€˜contextual mappingâ€™ì„ ì˜ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì´ë¼ê³  í‰ê°€ëœë‹¤ê³  í•œë‹¤. ì¦‰, ê°ê°ì˜ ë¬¸ë§¥ì„ ì„œë¡œ ì˜ êµ¬ë¶„í•˜ëŠ” ëŠ¥ë ¥ì´ íƒì›”í•˜ë‹¤ê³  ë³´ëŠ” ê²ƒì´ë‹¤.

ë…¼ë¬¸ì—ì„œëŠ” Trasformerì˜ ì´ëŸ°ì €ëŸ° universal approximation ëŠ¥ë ¥ì„ ì¦ëª…í•˜ë ¤ í•˜ëŠ”ë°, ê·¸ ê³¼ì • ì¤‘ì— â€˜(multi-head) self-attention layersê°€ contextual mappingì„ ì˜ ê³„ì‚°í•œë‹¤â€™ëŠ” ê²ƒì„ ì¦ëª…í•˜ëŠ” ê²Œ ì •ë§ ì¤‘ìš”í•œ ì¤‘ê°„ ê³¼ì •ì´ë¼ê³  í•œë‹¤. ì´ë¥¼ ìœ„í•´ ë…¼ë¬¸ì—ì„œëŠ” contextual mappingì˜ ê°œë…ì„ ì•„ì˜ˆ ìˆ˜ì‹ì ìœ¼ë¡œ ì •ì˜í•´ë²„ë¦° ë’¤ì— ì´ë¥¼ ì¦ëª…ì— ì´ìš©í•œë‹¤. ë…¼ë¬¸ì—ì„œ ì£¼ì–´ì§„ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![transformer-formula.jpeg](/assets/img/papers/trasformer-formula.jpeg)

ì¦‰ contextual mappingì€ ê¸¸ì´ $n$ì¸ input sequenceë¥¼ ë°›ì•„ $n$ê°œì˜ ê°’ (í˜¹ì€ $n$ì°¨ì› ì—´ë²¡í„°)ë¥¼ ë‚´ë†“ëŠ” í•¨ìˆ˜ë¡œ ì •ì˜ëœë‹¤. ì´ë•Œ í•œ ë¬¸ì¥(sequence) ì•ˆì˜ ë‹¨ì–´ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ ì—­í• ì„ í•˜ë¯€ë¡œ ê°ê° ë‹¤ë¥¸ contextê°’(contextual mappingì˜ entry)ì´ ë§¤ê²¨ì§„ë‹¤(1ë²ˆ ì¡°ê±´). ê²Œë‹¤ê°€, ê°™ì€ ë‹¨ì–´ë¼ë„ ë‹¤ë¥¸ ë¬¸ì¥ì—ì„œëŠ” ë‹¤ë¥¸ ì˜ë¯¸ë¡œ í•´ì„ëœë‹¤ëŠ” ì˜ë¯¸ì—ì„œ, ì„œë¡œ ë‹¤ë¥¸ ë‘ input sequence(L, Lâ€™)ì— ëŒ€í•œ contextual mappingì— ìˆëŠ” ëª¨ë“  (ì´ 2nê°œì˜) entryë“¤ì€ ì „ë¶€ ë‹¤ë¥´ê²Œ ë§¤ê²¨ì§„ë‹¤(2ë²ˆ ì¡°ê±´).

* ì§‘í•© $\mathbb{L}$ì´ ìœ í•œì§‘í•©ìœ¼ë¡œ ì„¤ì •ëœ ì´ìœ ëŠ” (ë‚´ ìƒê°ì—ëŠ”)

    Vocabularyì˜ í¬ê¸°ë„ ìœ í•œí•˜ê³  sequence ê¸¸ì´ë„ ìœ í•œí•˜ë¯€ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” input sequenceì˜ ê°œìˆ˜ëŠ” ìœ í•œí•˜ë‹¤. Sequenceë“¤ì˜ ì§‘í•©ê³¼ ëŒ€ì‘ë˜ëŠ” ì§‘í•©ì´ $\mathbb{L}$ê³¼ ë¹„ìŠ·í•œ ê²ƒì´ë¼ë©´, $\mathbb{L}$ì„ ìœ í•œì§‘í•©ì´ë¼ê³  ë†“ì•„ë„ ê´œì°®ì„ ê²ƒì´ë‹¤. (ì´ ì¡°ê±´ì´ í•„ìˆ˜ì¸ì§€ëŠ” ì¦ëª…ì„ ë” ë“¤ì—¬ë‹¤ë´ì•¼..)

## Main Text

### 1. Universal Approximatorì„ì„ ë³´ì´ê¸° í˜ë“  ì´ìœ 

* ë„ˆë¬´ ë§ì•„ ë³´ì´ëŠ” Parameter sharing. Self-attention layerì™€ feed-forward layer ëª¨ë‘, tokenë¼ë¦¬ ê³µìœ í•˜ëŠ” parameterì˜ ìˆ˜ê°€ ë§¤ìš° ë§ë‹¤.
* ë„ˆë¬´ ì ì–´ ë³´ì´ëŠ” token-wise interaction. Self-attention layerì˜ íŠ¹ì„±ìƒ pairwise dot-productë¡œë§Œ token ì‚¬ì´ì˜ interactionì„ ì¡ì•„ë‚¸ë‹¤.

(ë‘˜ì§¸ ì´ìœ ëŠ” ê·¸ëŸ´ ë§Œí•˜ë‹¤ê³  ë³´ì´ëŠ”ë°, ì²«ì§¸ ì´ìœ ëŠ” ì•„ì§ ì˜ ì´í•´í•˜ì§€ ëª»í–ˆë‹¤.)

ë…¼ë¬¸ì—ì„œëŠ” ìœ„ì˜ ë‘ ì´ìœ ë¡œ ì¸í•´ transformer encoder ìì²´ê°€ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” sequence-to-sequence í•¨ìˆ˜ì˜ ì¢…ë¥˜ì— ì œí•œì´ ìˆë‹¤ê³  ë³´ë©°, ì´ë¥¼ trainableí•œ positional encodingìœ¼ë¡œ í•´ê²°í•œë‹¤.

â“ ì¼ë°˜ì ìœ¼ë¡œ, Parameter sharingì´ ë§ì„ìˆ˜ë¡ universal approximatorê°€ ë˜ê¸° ì–´ë ¤ìš´ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?

### 2. ë…¼ë¬¸ì—ì„œ ë³¸ Transformer

ì•„ë˜ëŠ” ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ transformer blockì— ëŒ€í•œ ì‹ì´ë‹¤.

![contextual-mapping.jpeg](/assets/img/papers/contextual-mapping.jpeg)

ì˜ ì•Œë ¤ì ¸ ìˆë“¯, transformer encoder blockì€ multi-head self-attention layer(â€™Attnâ€™)ì™€ token-wise feed-forward layer(â€™FFâ€™)ë¼ëŠ” ë‘ (sub-)layerë¡œ ë‚˜ë‰œë‹¤.

#### 2.1. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜ ê³µí†µì 

* ìˆ˜ì‹ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ residual connectionì€ ê·¸ëŒ€ë¡œ ì‚´ë ¤ë‘ì—ˆë‹¤.

#### 2.2. ê¸°ì¡´ Transformer ë…¼ë¬¸ê³¼ì˜ ì°¨ì´ì 

* í•´ì„ì„ ê°„ë‹¨íˆ í•˜ê¸° ìœ„í•´ layer normalizationì€ ëºë‹¤ê³  í•œë‹¤.
* Self-attention layer ì‹ì„ ë³´ë©´ ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ë³¼ ìˆ˜ ì—†ë˜ ì‹œê·¸ë§ˆ($\sum$) ê¸°í˜¸ê°€ ë³´ì¸ë‹¤. ì›ë˜ transformer ë…¼ë¬¸ì—ì„œëŠ” attention headë“¤ì„ concatenateí•˜ëŠ”ë°, ì´ëŸ¬í•œ concatenationì„ ìˆ˜ì‹ì ìœ¼ë¡œëŠ” ì €ë ‡ê²Œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì¦‰ ì˜ë¯¸ê°€ ë‹¤ë¥¸ ì‹ì´ ì•„ë‹ˆë‹¤.
* Self-attention layerì˜ ì†Œë¬¸ì ì‹œê·¸ë§ˆ í•¨ìˆ˜($\sigma(\cdot)$)ëŠ” (column-wise) softmaxë¥¼ ê°€ë¦¬í‚¨ë‹¤. ê·¸ëŸ°ë° ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” scaled dot-product attentionì„ ì‚¬ìš©í•˜ëŠ” ë°˜ë©´ ì—¬ê¸°ì„œëŠ” ê·¸ëƒ¥ dot-product attentionì„ ì“°ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì¸ë‹¤. ì‚¬ì‹¤ $\boldsymbol{W}_K$ë‚˜ $\boldsymbol{W}_Q$ê°™ì€ parameterë“¤ì´ ê·¸ scaling factor($\frac{1}{\sqrt{d_k}}$)ë¥¼ í•™ìŠµí•˜ë©´ ê·¸ë§Œì´ë‹¤.

â“ Layer normalizationì„ ë¹¼ë„ ê´œì°®ì€ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?

#### 2.3. Positional encoding

* Trainableí•œ positional encodingì´ ì—†ëŠ” ìˆœìˆ˜í•œ transformer blockì€ ì˜¤ì§ â€˜permutation equivariantâ€™í•œ ì¢…ë¥˜ì˜ í•¨ìˆ˜ë§Œì„ ì˜ ê·¼ì‚¬í•  ë¿ì´ë‹¤. ê·¸ëŸ¬ë‚˜ positional encodingì„ ë„ì…í•¨ìœ¼ë¡œì¨ ì´ëŸ¬í•œ í•¨ìˆ˜ ì¢…ë¥˜ì˜ ì œí•œ ì—†ì´ ì•„ë¬´ëŸ° sequence-to-sequence í•¨ìˆ˜(with compact domain)ì„ ì˜ ê·¼ì‚¬í•  ìˆ˜ ìˆê²Œ ëœë‹¤.
* Positional encoding $\boldsymbol{E}$ ì—­ì‹œ $d\times n$ í¬ê¸°ì˜ real matrixë¡œ ì •ì˜ëœë‹¤. Transformer blockì„ í•¨ìˆ˜ $g$ë¡œ ì“´ë‹¤ë©´, positional encodingì´ ë„ì…ëœ transformer blockì€ input sequence $\boldsymbol{X}$ì— ëŒ€í•´ $g(\boldsymbol{X}+\boldsymbol{E})$ë¼ê³  ì“¸ ìˆ˜ ìˆë‹¤.
* ë…¼ë¬¸ì—ì„œëŠ” ì´ $\boldsymbol{E}$ê°€ trainableí•˜ë‹¤ê³  ê°€ì •í•˜ë¯€ë¡œ ì•„ë¬´ë ‡ê²Œë‚˜ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œ, í•¨ìˆ˜ë“¤ì˜ domainì´ compactí•¨ì„ ê°€ì •í•´ì„œ input sequenceê°€ $\boldsymbol{X}\in [0,1]^{d\times n}$ ê°€ ë˜ë„ë¡ í•œ ë’¤, positional encodingì„ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬ì„ ì„ì˜ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤. (Appendix C ì°¸ê³ )

$$
\boldsymbol{E} = \begin{pmatrix} 0&1&2&\cdots&n-1\\0&1&2&\cdots&n-1\\\vdots&\vdots&\vdots&&\vdots\\0&1&2&\cdots&n-1\end{pmatrix}
$$

### 3. ì£¼ìš” ê²°ê³¼ (2ê°€ì§€)

ë…¼ë¬¸ì´ ì£¼ì¥í•˜ëŠ” ë‘ ê°€ì§€ ì¤‘ìš”í•œ ê²°ê³¼ëŠ” Abstractì—ì„œ ì†Œê°œí•œ ì²˜ìŒ ë‘ ì¤„ê³¼ ê°™ë‹¤. ì—¬ê¸°ì„œëŠ” ë” ìì„¸í•œ ì„œìˆ ì„ ì†Œê°œí•œë‹¤.

> **Theorem 2.** (ì„ì˜ì˜  $\epsilon>0$ì™€ $1\le p < \infty$ì— ëŒ€í•´) í•¨ìˆ˜ $f$ê°€ ë‹¤ìŒì˜ ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤ê³  í•˜ì.
>
> 1. $f$ëŠ” sequence-to-sequence í•¨ìˆ˜.
> 2. $f$ì˜ supportëŠ” compact.
> 3. $f$ëŠ” ì—°ì†(w.r.t. entry-wise $\ell^p$ norm).
> 4. $f$ëŠ” **permutation equivariant**.
>
> ê·¸ëŸ¬ë©´ ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” Transformer network $g$ê°€ ì¡´ì¬í•œë‹¤.
>
> 1. $g$ëŠ” $(h,m,r)=(2,1,4)$ë¥¼ ë§Œì¡±.
> 2. $d_p (f,g ) \le \epsilon$.

* ì°¸ê³ : Transformer networkë€, ê°™ì€ Transformer blockì„ ì—¬ëŸ¬ ê°œ ìŒ“ì€ ê²ƒì´ë‹¤. ë˜ ìœ„ì—ì„œ ì“°ì¸ h, m, rì€ ê°ê° ë‹¤ìŒê³¼ ê°™ì€ ê²ƒì„ ë‚˜íƒ€ë‚´ëŠ” ê¸°í˜¸ë‹¤.

| ë¬¸ì | ëœ» |
| :-: | :-: |
| $h$ | attention headì˜ ê°œìˆ˜ |
| $m$ | attention headì˜ í¬ê¸° |
| $r$ | feed-forward layerì˜ hidden ì°¨ì› (=$d_{ff}$)  |

> **Theorem 3.** (ì„ì˜ì˜  $\epsilon>0$ì™€ $1\le p < \infty$ì— ëŒ€í•´) í•¨ìˆ˜ $f$ê°€ ë‹¤ìŒì˜ ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤ê³  í•˜ì.
>
> 1. $f$ëŠ” sequence-to-sequence í•¨ìˆ˜.
> 2. $f$ì˜ domainì€ compact.
> 3. $f$ëŠ” ì—°ì†(w.r.t. entry-wise $\ell^p$ norm).
>
> ê·¸ëŸ¬ë©´ ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” **Transformer network $g$ with (trainable) positional encoding $\boldsymbol{E}$**ê°€ ì¡´ì¬í•œë‹¤.
>
> 1. $g$ëŠ” $(h,m,r)=(2,1,4)$ë¥¼ ë§Œì¡±.
> 2. $d_p (f,g ) \le \epsilon$.

ê±°ì˜ ëª¨ë“  ê²ƒì´ Theorem 2ì™€ ë™ì¼í•˜ì§€ë§Œ, Transformer networkì—ëŠ” positional encodingì´ ì¶”ê°€ëê³ , ëŒ€ì‹  ê·¼ì‚¬í•˜ë ¤ëŠ” sequence-to-sequence í•¨ìˆ˜ì˜ permutation equivariant ì¡°ê±´ì´ ì‚¬ë¼ì¡Œë‹¤.

* $(h,m,r)=(2,1,4)$ë¥¼ ì“°ëŠ” ì´ìœ ? (ë„ˆë¬´ ì‘ì€ block ì•„ë‹Œê°€?)

    Attention headê°€ 2ê°œë°–ì— ì—†ê³ , ê·¸ í¬ê¸°ë„ ê²¨ìš° 1ì´ê³ , ì‹¬ì§€ì–´ feed-forward layerì˜ hidden ì°¨ì›ì´ 4ë°–ì— ì•ˆ ë˜ëŠ” ì‘ì€ Transformer blockì€ ì‹¤ì§ˆì ìœ¼ë¡œ ì“°ì´ì§€ ì•ŠëŠ”ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ Transformer blockì„ ì´ìš©í•œ ì´ìœ ëŠ” ë‹¨ì§€ ë‹¨ìˆœí™”ê°€ ì¦ëª…ì„ ì‰½ê²Œ í•´ì£¼ê¸° ë•Œë¬¸ë§Œì€ ì•„ë‹ˆë‹¤.

    ë” í° ëª¨ë¸ì€ ìëª…í•˜ê²Œ expressive powerê°€ ë” í¬ê¸° ë•Œë¬¸ì´ë‹¤. ì‹¤ì§ˆì ìœ¼ë¡œ ì“°ì´ëŠ” transformer blockì€ í›¨ì”¬ ë” ë§ì€ parameterë¥¼ ì“¸ í…ë°, ê·¸ëŸ° modelì€ ë…¼ë¬¸ì—ì„œ ì“°ì´ëŠ” ë§¤ìš° ì‘ì€ transformer blockì— ë¹„í•˜ë©´ ë‹¹ì—°íˆ ë”ìš±ë” ë§ì€ í•¨ìˆ˜ë“¤ì„ í‘œí˜„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‹ˆ ì´ë ‡ê²Œ ì‘ì€ ìŠ¤ì¼€ì¼ë¡œ ë¬¸ì œë¥¼ ì¶•ì†Œì‹œì¼œì„œ ë¬¸ì œë¥¼ í’€ì–´ë„ ì¶©ë¶„í•˜ë‹¤.

â“ ìœ„ì˜ ë‘ ì •ë¦¬ëŠ” universal approximationì˜ ì¸¡ë©´ì—ì„œ ë§¤ìš° ìœ ì˜ë¯¸í•œ ê²°ê³¼ë¥¼ ë‚´ê³  ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ëª¨ë‘ ì¡´ì¬ì„± ì •ë¦¬ì¸ íƒ“ì—, í›ˆë ¨ ê³¼ì •ì—ì„œ transformerê°€ â€˜ìš°ë¦¬ê°€ ì›í•˜ëŠ” í•¨ìˆ˜â€™ë¥¼ ì‹¤ì œë¡œ ì˜ ê·¼ì‚¬í•  ìˆ˜ ìˆëŠ”ì§€ëŠ” ë§í•´ì£¼ì§€ ì•ŠëŠ” ê²Œ ë¶„ëª…í•˜ë‹¤. ì´ê²ƒì´ ê°€ëŠ¥í•œì§€ëŠ” ì–´ë–»ê²Œ ì—°êµ¬í•´ì•¼ í• ê¹Œ?/ ì–´ë–»ê²Œ ì—°êµ¬ë˜ê³  ìˆì„ê¹Œ?

### 4. ì–´ë–»ê²Œ ì¦ëª…í•˜ë‚˜?

Theorem 2ì™€ Theorem 3ì˜ ì¦ëª…ì€ ë§¤ìš° ìœ ì‚¬í•˜ë©°, ë³¸ë¬¸ì—ì„œëŠ” Theorem 2ì˜ ì¦ëª…ê³¼ì •ì„ ìš”ì•½í•˜ì—¬ ì„¤ëª…í•œë‹¤. ì„¸ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ì„ì˜ì˜ continuous, permutation equivariant, sequence-to-sequence function $f$ with compact supportë¥¼ ì ì ˆí•œ Transformer networkë¡œ ê·¼ì‚¬í•œë‹¤. ê·¸ ë¡œë“œë§µì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

**4.1) $f$ë¥¼ piece-wise ìƒìˆ˜í•¨ìˆ˜ë¡œ ê·¼ì‚¬í•˜ê¸°**

ìƒìˆ˜í•¨ìˆ˜ë¼ê³  í•´ì„œ fê°€ ê°‘ìê¸° real-valuedê°€ ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤. ì—¬ê¸°ì„œì˜ ìƒìˆ˜í•¨ìˆ˜ ì—­ì‹œ í–‰ë ¬ì„ ë°›ì•„ í–‰ë ¬ì„ ë‚´ë±‰ëŠ” í•¨ìˆ˜ì¸ë°, í•¨ìˆ«ê°’ìœ¼ë¡œì„œì˜ í–‰ë ¬ì´ ê³ ì •ë˜ì–´ ìˆìœ¼ë©´ ìƒìˆ˜í•¨ìˆ˜ì¸ ê²ƒì´ë‹¤.

**4.2) Piece-wise ìƒìˆ˜í•¨ìˆ˜ë¥¼ â€˜modifiedâ€™ Transformer networkë¡œ ê·¼ì‚¬í•˜ê¸°**

â€˜Modifiedâ€™ Transformerë€, ê¸°ì¡´ì˜ Transformerì—ì„œ ì“°ì´ë˜ (column-wise) softmax í•¨ìˆ˜($\sigma$)ëŠ” column-wise hardmax($\sigma_H$)ë¡œ ëŒ€ì²´í•˜ê³ , FFì˜ activation functionìœ¼ë¡œ ì“°ì´ë˜ ReLUëŠ” ë˜ë‹¤ë¥¸ íŠ¹ì´í•œ í•¨ìˆ˜($\phi \in \Phi$, ìì„¸í•œ ì •ì˜ëŠ” ì•„ë˜ì—)ë¡œ ëŒ€ì²´í•œ ê²ƒì´ë‹¤.

* $\Phi$ì˜ ì •ì˜:  
The set of all piece-wise linear functions with at most three pieces, where at least one piece is constant. (p.9)

ì´ ë¶€ë¶„ì„ ì¦ëª…í•˜ê¸° ìœ„í•´, ë…¼ë¬¸ì—ì„œëŠ” modified Transformerì˜ layer ìˆœì„œë¥¼ ëœ¯ì–´ê³ ì¹˜ëŠ” ì¼ì„ í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. Residual connectionì„ ì´ìš©í•˜ë©´, self-attentionê³¼ feed-forward layerë¥¼ ë²ˆê°ˆì•„ ì ìš©í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ self-attentionë§Œ ì­‰, í˜¹ì€ feed-forward layerë§Œ ì­‰ ì´ì–´ í•©ì„±í•œ ê²ƒì„ í™œìš©í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.

> (....) we note that even though a Transformer network stacks self-attention and feed-forward layers in an alternate manner, **the skip connections enable these networks to employ a composition of multiple self-attention or feed-forward layers.** (ì¤‘ëµ) self-attention and feed-forward layers play in realizing the ability to universally approximate sequence-to-sequence functions: 1) self-attention layers compute precise contextual maps; and 2) feed-forward layers then assign the results of these contextual maps to the desired output values. (p.6)

â“ Modified Transformer networkì˜ layer ìˆœì„œë¥¼ ë’¤ë°”ê¾¸ì–´ ê°™ì€ ì¢…ë¥˜ì˜ layerë§Œ ì´ì–´ë¶™ì¼ ìˆ˜ ìˆëŠ” ì´ìœ ê°€ êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì¼ê¹Œ? ì—¬ê¸°ì— skip connectionì€ ì–´ë–¤ ì—­í• ì„ í• ê¹Œ?

**4.3) Modified Transformer networkë¥¼ Transformer networkë¡œ ê·¼ì‚¬í•˜ê¸°**

ì•ì—ì„œ ëŒ€ì²´í–ˆë˜ softmaxì™€ ReLUë¥¼ ì›ë˜ëŒ€ë¡œ ëŒë ¤ë†“ëŠ” ì‘ì—…ì´ë¼ê³  ë³´ë©´ ë  ê²ƒ ê°™ë‹¤.

### 5. ëª‡ ê°œì˜ blockì„ ìŒ“ì•„ì•¼ í•˜ë‚˜?

Theorem 2ëŠ” ê²°ê³¼ì ìœ¼ë¡œ ëª‡ê°œì˜ Transformer blockì„ ìŒ“ì•„ì•¼ í•˜ëŠ”ì§€ ë³´ì—¬ì¤€ë‹¤. ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ”, permutation equivariant í•¨ìˆ˜ë¥¼ ì˜ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ í•„ìš”í•œ (h,m,r)=(2,1,4) Transformer blockì€ ì´ $O(n(1/\delta)^{dn}/n!)$ê°œë‹¤. ë˜í•œ, positional encodingê¹Œì§€ ë”í•´ ì¢€ ë” ê´‘ë²”ìœ„í•œ sequence-to-sequence í•¨ìˆ˜ë¥¼ ì˜ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ì„œ í•„ìš”í•œ blockì€ $O(n(1/\delta)^{dn})$ê°œë‹¤.

ì´ë•Œ $\delta$ëŠ” Theorem 2/3ì˜ ì¦ëª… 1~2ë‹¨ê³„ì—ì„œ ì“°ì¸ piecewise constant functionì˜ domainì„ êµ¬ë¶„í•˜ëŠ” gridë¥¼ ì´ë£¨ëŠ” (hyper-)cubeì˜ í•œ ë³€ì˜ ê¸¸ì´ì´ë©°, ì¶©ë¶„íˆ ì‘ìŒì„ ê°€ì •í•´ì•¼ í•œë‹¤. (ì¦ëª…ê³¼ì •ì— ë”°ë¥´ë©´, $O(\delta^{d/p} ) \le \epsilon/3)$

â“ ë…¼ë¬¸ì—ì„œëŠ” ì¦ëª…ì„ ìœ„í•´ ì•„ì£¼ ì‘ì€ transformer blockì„ ì´ìš©í•˜ê³  ìˆë‹¤. ë§Œì•½ ì´ transformer blockì˜ í¬ê¸°ë¥¼ í‚¤ìš´ë‹¤ë©´ í•„ìš”í•œ blockì˜ ìˆ˜ëŠ” ì¤„ì–´ë“¤ê¹Œ? (ì•„ë§ˆ $d$ì™€ $n$ì— ë”°ë¥¸ complexityì—ëŠ” í¬ê²Œ ì°¨ì´ê°€ ìˆì§€ ì•Šì„ ê²ƒ ê°™ë‹¤. $h$, $m$, $r$ ë“±ì˜ ê°’ì€ $d$ë‚˜ $n$ì˜ ê°’ê³¼ëŠ” ê´€ë ¨ì´ ì—†ìœ¼ë¯€ë¡œ.)

## My Comments & Questions

* ì„ í˜•ëŒ€ìˆ˜í•™ì„ ê½¤ë‚˜ ì“°ëŠ” ë…¼ë¬¸ì´ì§€ë§Œ ì‹¤ìƒì€ ì—„ì²­ë‚˜ê²Œ í•´ì„í•™ìŠ¤ëŸ¬ìš´ ë…¼ë¬¸ì´ì—ˆë‹¤. í•´ì„í•™1ë•Œ Weierstrass Approximation Theorem(compact domainì—ì„œ ì—°ì†í•¨ìˆ˜ë¥¼ ë‹¤í•­ì‹ìœ¼ë¡œ ì„ì˜ì˜ ì •í™•ë„ë¡œ ê·¼ì‚¬í•˜ê¸°) ë°°ì› ë˜ ê²ƒì´ ìƒˆë¡ìƒˆë¡...
* ìœ„ì—ì„œ ë˜ì¡Œë˜ ì§ˆë¬¸ë“¤ì€ ë‚´ê°€ ë…¼ë¬¸ì„ ì½ìœ¼ë©´ì„œë„ ëê¹Œì§€ ì´í•´í•˜ì§€ ëª»í–ˆë˜, í˜¹ì€ ìŠ¤ìŠ¤ë¡œ 100% ë§Œì¡±ìŠ¤ëŸ½ê²Œ ëŒ€ë‹µí•˜ì§€ëŠ” ëª»í–ˆë˜ ì§ˆë¬¸ë“¤ì´ë‹¤. ë‹¤ì‹œ ëª¨ì•„ë³´ì.

â“ ì¼ë°˜ì ìœ¼ë¡œ, Parameter sharingì´ ë§ì„ìˆ˜ë¡ universal approximatorê°€ ë˜ê¸° ì–´ë ¤ìš´ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?

â“ Layer normalizationì„ ë¹¼ë„ ê´œì°®ì€ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ?

â“ (Paraphrased:) í›ˆë ¨ ê³¼ì •ì—ì„œ transformerê°€ â€˜ìš°ë¦¬ê°€ ì›í•˜ëŠ” í•¨ìˆ˜â€™ë¥¼ ì‹¤ì œë¡œ ì˜ ê·¼ì‚¬í•  ìˆ˜ ìˆëŠ”ì§€ëŠ” ì–´ë–»ê²Œ ì•Œ ìˆ˜ ìˆì„ê¹Œ?

â“ Modified Transformer networkì˜ layer ìˆœì„œë¥¼ ë’¤ë°”ê¾¸ì–´ ê°™ì€ ì¢…ë¥˜ì˜ layerë§Œ ì´ì–´ë¶™ì¼ ìˆ˜ ìˆëŠ” ì´ìœ ê°€ êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì¼ê¹Œ? ì—¬ê¸°ì— skip connectionì€ ì–´ë–¤ ì—­í• ì„ í• ê¹Œ?

â“ ë…¼ë¬¸ì—ì„œëŠ” ì¦ëª…ì„ ìœ„í•´ ì•„ì£¼ ì‘ì€ transformer blockì„ ì´ìš©í•˜ê³  ìˆë‹¤. ë§Œì•½ ì´ transformer blockì˜ í¬ê¸°ë¥¼ í‚¤ìš´ë‹¤ë©´ í•„ìš”í•œ blockì˜ ìˆ˜ëŠ” ì¤„ì–´ë“¤ê¹Œ?
