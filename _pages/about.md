---
permalink: /
title: "Hanseul Cho (ì¡°í•œìŠ¬) ğŸ‘‹"
excerpt: "About me"
author_profile: true
toc: true
toc_sticky: true
redirect_from: 
  - /about/
  - /about.html
---

<style>
gray { color: gray; font-size: 75%;}
red { color: #DC3522}
.highlighted { 
    background-color: #FFFD90; 
}
.nobull {
  margin:0px; padding:0px;
  list-style: none;
  padding-left: 2rem;
  text-indent: -1.6rem;
}
.nobull2 {
  line-height:1em;
  padding-left: 1rem;
  text-indent: 0rem;
}
</style>

I am a Ph.D. student at [Kim Jaechul Graduate School of AI](https://gsai.kaist.ac.kr), [Korea Advanced Institute of Science and Technology](https://www.kaist.ac.kr/en/) (**KAIST AI**) (08/2023--08/2027 (expected)). I am fortunate to be advised by [Prof. **Chulhee "Charlie" Yun**](https://chulheeyun.github.io) of the [Optimization & Machine Learning (**OptiML**) Laboratory](https://chulheeyun.github.io), KAIST AI.
Previously, I completed my M.Sc. (in AI, 02/2022--08/2023) and B.Sc. (in Math, minor in CS, *Summa Cum Laude*) at KAIST.

Currently, I am a Student Researcher at [**Google**](https://research.google) in New York CityğŸ‡ºğŸ‡¸ (05/05/2025--07/25/2025), working for [Srinadh Bhojanapalli](https://bsrinadh.github.io). Also, I'll be in NYC from 05/02/2025 to 08/21/2025, exploiting the grace periods before & after my J-1 visa program.

Please don't hesitate to reach out for questions, discussions, and collaborations! ğŸ¤—

ğŸ“‹ **Curriculum Vitae (CV)**: [[PDF]](/files/Curriculum_Vitae__Hanseul_Cho.pdf) | [[Overleaf-ReadOnly]](https://www.overleaf.com/read/pmkhfyywpjnt#3ad9b9)  
ğŸ“§ **Primary E-mail**: jhs4015 at kaist dot ac dot kr  
ğŸ“§ **Googler E-mail**: {firstname}{lastname} at google dot com

## ğŸ”¬ Research Interests ğŸ”­

My primary research interests lie in **optimization, machine learning (ML), and deep learning (DL)**, especially focusing on both mathematical/theoretical analysis and empirical improvements (usually based on theoretical understanding).

During my journey to a Ph.D.ğŸ‘¨ğŸ»â€ğŸ“, my ultimate research goal is to **rigorously understand and practically overcome** the following **three critical <red>challenges</red> in ML/DL** (see my [Thesis ProposalğŸ”—ğŸ“°](/posts/phd-thesis-proposal/) if interested):

* <red><i class="fa fa-sitemap"></i> <b>[Generalizability]</b></red> **Out-of-distribution generalization of (large) language models** (e.g., length generalization and compositional generalization of Transformers)
* <red><i class="fa fa-flask"></i> <b>[Adaptability]</b></red> **Training adaptable models under evolving environments** (e.g., continual learning, maintaining the plasticity of neural networks, sample-efficient reinforcement learning)
* <red><i class="fa fa-cubes"></i> <b>[Multifacetedness]</b></red> **Learning with multiple (possibly conflicting and/or orthogonal) goals** (e.g., minimax optimization, biâ€‘level optimization, fairness in ML)

## â€¼ï¸Newsâ€¼ï¸

<ul class="nobull">
  <li>ğŸ—ï¸ [Jun. '25] I was selected as one of the <a href="https://icml.cc/Conferences/2025/ProgramCommittee#top-reviewer">Top Reviewers (top 1.88%: 206 of 10,943 reviewers)</a> at ICML 2025! </li>
  <li>ğŸ—ï¸ [May '25] <red><b>(NEW)</b></red> I visit NYCğŸ‡ºğŸ‡¸ from 2025-05-02 to 2025-08-21 (see the item below). Let's grab a coffee and have a chat if you are in NYC! </li>
  <li>ğŸ—ï¸ [Feb. '25] I'll work as a <b>Student Researcher</b> at <b>Google</b> in New York CityğŸ‡ºğŸ‡¸! (05/05/2025&ndash;07/25/2025, Host: <a href="https://bsrinadh.github.io">Srinadh Bhojanapalli</a>) </li>
  <li>ğŸ—ï¸ [Jan. '25] Invited as a reviewer of  <a href="https://jmlr.org/tmlr/index.html">Transactions on Machine Learning Research (TMLR)</a>.</li>
  <li>ğŸ—ï¸ [Jan. '25] Two papers got accepted to <b>ICLR 2025</b>! ğŸ‰ <a href="/publication/Position-Coupling-Scratchpad">One</a> is the sequel of our <a href="/publication/Position-Coupling">Position Coupling</a> paper; <a href="/publication/Continual-Linear-Classfication-GD">another</a> is about a theoretical analysis of continual learning algorithm. See you in SingaporeğŸ‡¸ğŸ‡¬!</li>
  <li>ğŸ—ï¸ [Nov. '24] An early version of our <a href="/publication/Continual-Linear-Classfication-GD">paper</a> on theoretical analysis of continual learning is accepted to <a href="http://aiassociation.kr">JKAIA 2024</a> and won the <b>Best Paper Award</b> (top 3 papers)! ğŸ‰</li>
  <li>ğŸ—ï¸ [Nov. '24] I was selected as one of the <a href="https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers">Top Reviewers (top 8.60%: 1,304 of 15,160 reviewers)</a> at NeurIPS 2024! (+ Free registration! ğŸ˜) </li>
  <li>ğŸ—ï¸ [Sep. '24] Two papers got accepted to <b>NeurIPS 2024</b>! ğŸ‰ <a href="/publication/Position-Coupling">One</a> is about length generalization of arithmetic Transfomers, and <a href="/publication/DASH-Direction-Aware-SHrinking">another</a> is about mitigating loss of plasticity in incremental neural net training. See you in Vancouver, CanadağŸ‡¨ğŸ‡¦!</li>
  <li>ğŸ—ï¸ [Jun. '24] An early version of our <a href="/publication/Position-Coupling">paper</a> on length generalization of Transformers got accepted to the ICML 2024 Workshop on <a href="https://longcontextfm.github.io/">Long-Context Foundation Models</a>!</li>
  <li>ğŸ—ï¸ [May '24] A <a href="/publication/Alex-GDA">paper</a> got accepted to <b>ICML 2024</b> as a <b>spotlight paper</b> (top 3.5% among all submissions)! ğŸ‰ We show global convergence of Alt-GDA (which is <i>strictly</i> faster than Sim-GDA) and propose an enhanced algorithm called Alex-GDA for minimax optimization. See you in Vienna, AustriağŸ‡¦ğŸ‡¹!</li>
  <li>ğŸ—ï¸ [Sep. '23] Two papers are accepted to <b>NeurIPS 2023</b>! ğŸ‰ One is about <a href="/publication/fair-streaming-pca">Fair Streaming PCA</a> and another is about <a href="/publication/PLASTIC">enhancing plasticity in RL</a>. See you in New Orleans, USAğŸ‡ºğŸ‡¸!</li>
  <li>ğŸ—ï¸ [Jan. '23] Our <a href="/publication/sgda-with-shuffling">paper</a> about shuffling-based stochastic gradient descent-ascent got accepted to <b>ICLR 2023</b>! </li>
  <li>ğŸ—ï¸ [Nov. '22] An early version of our <a href="/publication/sgda-with-shuffling">paper</a> about shuffling-based stochastic gradient descent-ascent is accepted to 2022 <a href="http://aiassociation.kr">Korea AI Association</a> + <a href="https://www.navercorp.com/en">NAVER</a> Autumnal Joint Conference (JKAIA 2022) and selected as the <b>NAVER Outstanding Theory Paper</b> (top 3 papers)! </li>
  <li>ğŸ—ï¸ [Oct. '22] I am happy to announce that our very first <a href="https://arxiv.org/abs/2210.05995">preprint</a> is now on arXiv!  It is about convergence analysis of shuffling-based stochastic gradient descent-ascent. </li>
  <li>ğŸ—ï¸ [Feb. '22] Now I am part of <a href="https://chulheeyun.github.io">OptiML Lab</a> of KAIST AI. </li>
</ul>

<!-- ## Education

<ul class="nobull">
  <li>ğŸ« Ph.D. in Artificial Intelligence  <gray>KAIST, Sept. 2023 â€“ Current</gray></li>
  <li>ğŸ« M.Sc. in Artificial Intelligence  <gray>KAIST, Mar. 2022 â€“ Aug. 2023</gray></li>
  <li>ğŸ« B.Sc. in Mathematical Sciences  <gray>KAIST, Mar. 2017 â€“ Feb. 2022</gray></li>
    <ul class="nobull2" style="color:gray">
    <li>Minor in Computing Sciences / <b>Summa Cum Laude</b></li>
    </ul>
</ul> -->

