var store = [{
        "title": "[Paper Reading] [KR only] Are Transformers universal approximators of sequence-to-sequence functions?",
        "excerpt":"Main References Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are Transformers universal approximators of sequence-to-sequence functions? ICLR 2020. Abstract Transformer encoder는 ‘permutation equivariant’한 성질을 갖는 연속인 ‘sequence-to-sequence’ 함수(with compact support)에 대한 universal approximator임을 보인다. Transformer encoder에다 learnable한 positional encodings를 같이 쓰면 임의의(permutation equivariant하지 않아도) 연속인...","categories": ["paper review"],
        "tags": ["transformer","sequence-to-sequence","approximation","expressive power","deep learning theory"],
        "url": "/posts/are-transformer-universal-approximators-of-sequence-to-sequence-functions/",
        "teaser": null
      },{
        "title": "[Paper Reading] WGAN with an Inﬁnitely Wide Generator Has No Spurious Stationary Points",
        "excerpt":"Please visit the link to open the slide:  View PDF   Main References      Albert No, TaeHo Yoon, Sehyun Kwon, and Ernest K. Ryu. WGAN with an Infinitely Wide Generator Has No Spurious Stationary Points. ICML 2021.   ","categories": ["paper review","group meeting"],
        "tags": ["wasserstein gan","gan","no spurious stationarity"],
        "url": "/posts/wgan-no-spurious-stationarity/",
        "teaser": null
      },{
        "title": "[Paper Reading] Offline Reinforcement Learning with Implicit Q Learning",
        "excerpt":"Please visit the link to open the slide:  View PDF   Main References      Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline Reinforcement Learning with Implicit Q Learning. ICLR 2022.   ","categories": ["paper review","coursework"],
        "tags": ["offline RL","implicit q learning"],
        "url": "/posts/implicit-q-learning/",
        "teaser": null
      },{
        "title": "[Paper Reading] On Learning Fairness and Accuracy on Multiple Subgroups",
        "excerpt":"Please visit the link to open the slide:  View PDF   Main References      Changjian Shui, Gezheng Xu, Qi CHEN, Jiaqi Li, Charles Ling, Tal Arbel, Boyu Wang, and Christian Gagné. On Learning Fairness and Accuracy on Multiple Subgroups. NeurIPS 2022.   ","categories": ["paper review","group meeting"],
        "tags": ["fairness","multiple subgroup fairness"],
        "url": "/posts/multi-group-fairness/",
        "teaser": null
      },{
        "title": "[Paper Reading] Adversarial training descends without descent: Finding actual descent directions based on Danskin's Theorem",
        "excerpt":"Please visit the link to open the slide: View PDF Main References Fabian Latorre, Igor Krawczuk, Leello Tadesse Dadi, Thomas Pethick, and Volkan Cevher. Finding Actual Descent Directions for Adversarial Training. ICLR 2023. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu Towards Deep Learning Models Resistant to...","categories": ["paper review","group meeting"],
        "tags": ["adversarial training","danskin's lemma"],
        "url": "/posts/adversarial-training-danskin/",
        "teaser": null
      },{
        "title": "[Paper Reading] Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions",
        "excerpt":"Please visit the link to open the slide:  View PDF   Main References      Arthur Jacot. Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions. ICLR 2023.   ","categories": ["paper review","group meeting"],
        "tags": ["implicit bias","low rank"],
        "url": "/posts/implicit-bias-large-depth/",
        "teaser": null
      },{
        "title": "[Coursework Report] An Overview on Optimal Transport and its Application to Model Fusion",
        "excerpt":"Please visit the link to open the report: View PDF Abstract In this report, we briefly overview the theory of optimal transport (OT),entropically regularized OT and Sinkhorn algorithm, and their application to deep learning model fusion technique. For the theory of OT, we delve into the derivation of dual OT...","categories": ["coursework","report"],
        "tags": ["optimal transport","duality","entropic regularization","model fusion"],
        "url": "/posts/optimal-transport-model-fusion/",
        "teaser": null
      },{
        "title": "[Paper Reading] A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning",
        "excerpt":"Please visit the link to open the slide: View PDF Main References Alicia Curth, Alan Jeffares, and Mihaela van der Schaar. A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning. NeurIPS 2023. Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical...","categories": ["paper review","group meeting"],
        "tags": ["double descent","machine learning"],
        "url": "/posts/u-turn-double-descent/",
        "teaser": null
      },{
        "title": "[Paper Reading] Understanding Gradient Descent on Edge of Stability in Deep Learning",
        "excerpt":"Please visit the link to open the slide:  View PDF   Main References      Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding Gradient Descent on Edge of Stability in Deep Learning. ICML 2022.  ","categories": ["paper review","group meeting"],
        "tags": ["edge of stability","normalized gradient descent"],
        "url": "/posts/edge-of-stability-gd/",
        "teaser": null
      },{
        "title": "[Paper Reading] Convex and Non-convex Optimization under Generalized Smoothness",
        "excerpt":"Please visit the link to open the slide:  View PDF   Main References      Haochuan Li, Jian Qian, Yi Tian, Alexander Rakhlin, and Ali Jadbabaie. Convex and Non-convex Optimization under Generalized Smoothness. NeurIPS 2023.  ","categories": ["paper review","group meeting"],
        "tags": ["generalized smoothness","optimization"],
        "url": "/posts/generalized-smoothness/",
        "teaser": null
      },{
        "title": "[Paper Reading] StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization",
        "excerpt":"Please visit the link to open the slide:  View PDF   Main References      Shida Wang and Qianxiao Li. StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization. ICML 2024.   ","categories": ["paper review","group meeting"],
        "tags": ["state space model","curse of memory","stable reparameterization"],
        "url": "/posts/stable-ssm/",
        "teaser": null
      },{
        "title": "[Paper Reading] Viewing Log-Depth Transformers via the Lens of Distributed Computing",
        "excerpt":"Please visit the link to open the slide: View PDF Main References Clayton Sanford, Danial Hsu, and Matus Telgarsky. Transformers, Parallel Computation, and Logarithmic Depth. ICML 2024. Clayton Sanford, Bahare Fatemi, Ethan Hall, Anton Tsitsulin, Mehran Kazemi, Jonathan Halcrow, Bryan Perozzi, and Vahab Mirrokni. Understanding Transformer Reasoning Capabilities via Graph...","categories": ["paper review","group meeting"],
        "tags": ["transformers","parallel computation","graph algorithms","distributed computing"],
        "url": "/posts/log-depth-transformer/",
        "teaser": null
      },{
        "title": "SGDA with shuffling: faster convergence for nonconvex-PŁ minimax optimization",
        "excerpt":"Poster Abstract Stochastic gradient descent-ascent (SGDA) is one of the main workhorses for solving finite-sum minimax optimization problems. Most practical implementations of SGDA randomly reshuffle components and sequentially use them (i.e., without-replacement sampling); however, there are few theoretical results on this approach for minimax algorithms, especially outside the easier-to-analyze (strongly-)monotone...","categories": ["ICLR","KAIA"],
        "tags": ["minimax optimization","SGDA","without-replacement sampling","shuffling-based"],
        "url": "/publication/sgda-with-shuffling",
        "teaser": null
      },{
        "title": "PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning",
        "excerpt":"Main Figures Abstract In Reinforcement Learning (RL), enhancing sample efficiency is crucial, particularly in scenarios when data acquisition is costly and risky. In principle, off-policy RL algorithms can improve sample efficiency by allowing multiple updates per environment interaction. However, these multiple updates often lead the model to overfit to earlier...","categories": ["NeurIPS"],
        "tags": ["Reinforcement Learning","Plasticity","Sharpness-aware Minimization","Reset Mechanism"],
        "url": "/publication/PLASTIC",
        "teaser": null
      },{
        "title": "Fair Streaming Principal Component Analysis: Statistical and Algorithmic Viewpoint",
        "excerpt":"Main Figures Poster Abstract Fair Principal Component Analysis (PCA) is a problem setting where we aim to perform PCA while making the resulting representation fair in that the projected distributions, conditional on the sensitive attributes, match one another. However, existing approaches to fair PCA have two main problems: theoretically, there...","categories": ["NeurIPS"],
        "tags": ["Fairness","Streaming","Principal Component Analysis (PCA)","Unsupervised Learnability"],
        "url": "/publication/fair-streaming-pca",
        "teaser": null
      },{
        "title": "Fundamental Benefit of Alternating Updates in Minimax Optimization",
        "excerpt":"Poster Abstract The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax optimization problems, takes the descent and ascent steps either simultaneously (Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to converge faster, the performance gap between the two is not yet well understood theoretically, especially in terms of global...","categories": ["ICLR Workshop","ICML"],
        "tags": ["minimax optimization","Gradient Descent-Ascent (GDA)","Alex-GDA"],
        "url": "/publication/Alex-GDA",
        "teaser": null
      },{
        "title": "DASH: Warm-Starting Neural Network Training in Stationary Settings without Loss of Plasticity",
        "excerpt":"Main Figures Abstract Warm-starting neural network training by initializing networks with previously learned weights is appealing, as practical neural networks are often deployed under a continuous influx of new data. However, it often leads to loss of plasticity, where the network loses its ability to learn new information, resulting in...","categories": ["NeurIPS","ICML Workshop"],
        "tags": ["Loss of Plasticity","Warm-Starting","Incremental Learning","Generalization","Direction-Aware SHrinking","DASH"],
        "url": "/publication/DASH",
        "teaser": null
      },{
        "title": "Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure",
        "excerpt":"Main Figures Abstract Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional...","categories": ["NeurIPS","ICML Workshop"],
        "tags": ["Transformers","Length Generalization","Position Coupling","Positional Encoding","Out-of-distribution Generalization","Arithmetic Tasks","Algorithmic Tasks"],
        "url": "/publication/Position-Coupling",
        "teaser": null
      },{
        "title": "Arithmetic Transformers Can Length-Generalize in Both Operand Length and Count",
        "excerpt":"Main Figures Abstract Transformers often struggle with length generalization, meaning they fail to generalize to sequences longer than those encountered during training. While arithmetic tasks are commonly used to study length generalization, certain tasks are considered notoriously difficult, e.g., multi-operand addition (requiring generalization over both the number of operands and...","categories": ["Arxiv"],
        "tags": ["Transformers","Length Generalization","Position Coupling","Scratchpad","Positional Encoding","Out-of-distribution Generalization","Arithmetic Tasks"],
        "url": "/publication/Position-Coupling-Scratchpad",
        "teaser": null
      },{
        "title": "Convergence and Implicit Bias of Gradient Descent on Continual Linear Classification",
        "excerpt":"Abstract We study continual learning on multiple linear classification tasks by sequentially running gradient descent (GD) for a fixed budget of iterations per each given task. When all tasks are jointly linearly separable and are presented in a cyclic/random order, we show the directional convergence of the trained linear classifier...","categories": ["KAIA"],
        "tags": ["Continual Learning","Sequential Learning","Gradient Descent","Linear Classification","Convergence","Implicit Bias"],
        "url": "/publication/Continual-Linear-Classfication-GD",
        "teaser": null
      },]
